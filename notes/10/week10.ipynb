{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1024,\n",
    "        'height': 768,\n",
    "        'scroll': True,\n",
    "})\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "\n",
    "import george\n",
    "from george import kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 10, ASTR 596: Fundamentals of Data Science\n",
    "\n",
    "\n",
    "## Hierarchical Bayes and Modeling Populations \n",
    "\n",
    "### Gautham Narayan \n",
    "##### <gsn@illinois.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "Last week, we saw how:\n",
    "\n",
    "1. A Gaussian process is specified by a choice for mean and covariance kernel\n",
    "2. For a fixed choice of mean and covariance kernel, but no data, you have a Gaussian process prior - an infinite number of functions can be drawn from this prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a kernel \n",
    "k = 3*kernels.ExpSquaredKernel(metric=2.)\n",
    "\n",
    "# Define a Gaussian Process - you need a mean and a kernel \n",
    "gp = george.GP(mean=0, kernel=k)\n",
    "\n",
    "# Evaluate the kernel on the abscissa values/exogenous variable/x/t\n",
    "t = np.arange(0, 100)\n",
    "gp.compute(t)\n",
    "\n",
    "# draw some random samples from the Gaussian process prior\n",
    "samples = gp.sample(t, size=5)\n",
    "\n",
    "# and plot them\n",
    "plt.figure()\n",
    "plt.plot(t, samples.T)\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "3. This framework is really flexible, and you can use it to make predictions for physical processes even when you can't write down an explicit model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define some function of the exogenous variable\n",
    "def func(t):\n",
    "    ''' \n",
    "    some completely whacky function that we might never imagine to write down as a model\n",
    "    '''\n",
    "    np.random.seed(42)\n",
    "    y = - 3*((t/20)-30)**2 - 0.001*(t-20)**3 + 20*np.random.randn(len(t))\n",
    "    ind = ((t/18)%2)<=1\n",
    "    y[ind] += 90*np.sin(np.pi/4*t[ind] )\n",
    "    return y\n",
    "\n",
    "# evaluate the function at abscissa values\n",
    "y = func(t)\n",
    "\n",
    "\n",
    "# and plot them\n",
    "plt.figure()\n",
    "plt.scatter(t, y, marker='o', label='some wonky function')\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "4. You have to condition the model on the data (i.e. evaluate the posterior) if you want to make predictions (either interpolating or extrapolating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define a GP with the same kernel as before, but a mean specified by the mean of the data\n",
    "gp = george.GP(mean=np.mean(y), kernel=k)\n",
    "\n",
    "\n",
    "# again, evaluate the kernel at the abscissa values - we'll only use every other value\n",
    "gp.compute(t[::2])\n",
    "\n",
    "# setup a new array of times for prediction\n",
    "t_new = np.arange(-30, 131, 0.1)\n",
    "\n",
    "plt.figure()\n",
    "# now *CONDITION* the GP on the observed values y, and predict on the new values t_new\n",
    "ypred1 = gp.predict(y[::2], t_new, return_cov=False)\n",
    "\n",
    "# and plot the original data + the GP prediction with the constant mean\n",
    "plt.scatter(t, y, color='C0', marker='o', label='some wonky function')\n",
    "plt.scatter(t[::2], y[::2], color='C3', marker='o', label='The data we used to condition the GP')\n",
    "\n",
    "plt.plot(t_new, ypred1, 'C1', ls='--', label='conditioned, unoptimized GP\\nw/ constant mean', lw=3)\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "5. If you want your predictions to be reasonable, you should think about how to specify the mean function well.\n",
    "    \n",
    "Advice: Use the mean function to specify the things that can you model reasonably, and use the Gaussian process to describe the bit you cannot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# maybe we can't write down a model, but we can say looking a the data that there is some polynomial trend\n",
    "coeffs = np.polyfit(t, y, 3)\n",
    "plt.figure()\n",
    "# plot the data with the fitted polynomial\n",
    "plt.scatter(t, y, color='C0', marker='o', label='some wonky function')\n",
    "plt.plot(t, np.polyval(coeffs, t), color='C2', ls='--', label='Some 3rd order polynomial\\nto describe the mean', lw=3)\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as op\n",
    "from george.modeling import Model\n",
    "\n",
    "# define a mean model defined by the polynomial we just found\n",
    "class PolynomialModel(Model):\n",
    "    parameter_names = ()\n",
    "    global coeffs\n",
    "    def get_value(self, t):\n",
    "        t = t.flatten()\n",
    "        return (np.polyval(coeffs, t))\n",
    "\n",
    "    \n",
    "# now use the polynomial as the mean model - but we're not fitting those values again - just the GP parameters\n",
    "gp = george.GP(mean=PolynomialModel(), kernel=k)\n",
    "\n",
    "\n",
    "# Define the objective function (negative log-likelihood in this case).\n",
    "def nll(p):\n",
    "    gp.set_parameter_vector(p)\n",
    "    ll = gp.log_likelihood(y[::2], quiet=True)\n",
    "    return -ll if np.isfinite(ll) else 1e25\n",
    "\n",
    "# And the gradient of the objective function.\n",
    "def grad_nll(p):\n",
    "    gp.set_parameter_vector(p)\n",
    "    return -gp.grad_log_likelihood(y[::2], quiet=True)\n",
    "\n",
    "# You need to compute the GP once before starting the optimization.\n",
    "gp.compute(t[::2])\n",
    "\n",
    "# Run the optimization routine.\n",
    "p0 = gp.get_parameter_vector()\n",
    "results = op.minimize(nll, p0, jac=grad_nll, method=\"L-BFGS-B\")\n",
    "\n",
    "# Update the kernel and print the final log-likelihood.\n",
    "gp.set_parameter_vector(results.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# now make a new prediction with our new mean model\n",
    "ypred2, cov = gp.predict(y[::2], t_new, return_cov=True)\n",
    "std = np.sqrt(np.diag(cov))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# plot the data\n",
    "plt.scatter(t, y, color='C0', marker='o', label='some wonky function')\n",
    "plt.scatter(t[::2], y[::2], color='C3', marker='o', label='The data we used to condition the GP')\n",
    "\n",
    "# and plot the truth\n",
    "plt.plot(t_new, func(t_new), 'C4', ls='-', lw=0.5, label='Truth', alpha=0.5)\n",
    "\n",
    "# plot our initial prediction with a constant mean\n",
    "plt.plot(t_new, ypred1, 'C1', ls='--', label='conditioned, unoptimized GP\\nw/ constant mean', lw=3)\n",
    "\n",
    "# plot our new prediction with the polynomial mean \n",
    "plt.plot(t_new, ypred2, 'C2', ls='-.', label='conditioned, optimized GP\\nw/ poly3 mean', lw=3)\n",
    "\n",
    "# plot the scatter about the posterior predictive mean from the diagonal of the covariance matrix \n",
    "plt.fill_between(t_new, ypred2-std, ypred2+std, color='lightgrey')\n",
    "\n",
    "\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "\n",
    "\n",
    "The \"ExpSquared\" (Squared Exponential) kernel we used above is:\n",
    "\n",
    "\\begin{equation}\n",
    "k_{1}\\left(t, t^{\\prime}\\right)=\\theta_{1}^{2} \\exp \\left(-\\frac{\\left(t-t^{\\prime}\\right)^{2}}{2 \\theta_{2}^{2}}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "If, for a specific set of values for $\\theta_{1},\\theta_{2}$ in your chosen kernel, your Gaussian process prior corresponds to a family of functions, and you are not sure of the values of $\\theta_{1},\\theta_{2}$ itself, then you can imagine $\\theta_{1},\\theta_{2}$ to be parameters themselves.\n",
    "\n",
    "### These are parameters of a prior distribution - we call these **hyperparameters** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How we pick priors:\n",
    "\n",
    "1. If we have a previous/independent measurement/inference of the parameter etc., use it with its error bars as $p(\\theta)$. (You've done this)\n",
    "\n",
    "2. Choose wide, uninformative distributions for all the parameters we don’t know well. (You've done this)\n",
    "\n",
    "3. **Use distributions in nature from previous observations of similar objects i.e. the population.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Option 3: Use distributions in nature from previous observations of similar objects.\n",
    "\n",
    "\n",
    "Histograms of population properties, when normalized, can be interpreted as probability distributions for individual parameters:\n",
    "\n",
    "\n",
    "## $$\n",
    "\\mathbf{p}(\\theta)=\\mathbf{n}(\\theta | \\mathbf{\\alpha}) / \\int \\mathbf{n}(\\theta | \\mathbf{\\alpha}) \\mathbf{d} \\mathbf{\\theta}=\\mathbf{p}(\\theta | \\mathbf{\\alpha})\n",
    "$$\n",
    "\n",
    "\n",
    "where $n(\\theta|\\alpha)$ is the function with parameters $\\alpha$ that was fit to the histogram\n",
    "\n",
    "$\\theta$ = parameter\n",
    "\n",
    "$\\alpha$ = hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Going Hierarchical:\n",
    "\n",
    "Bayes theorem:\n",
    "\n",
    "## $$ p(\\theta|x) = p(x|\\theta)\\cdot p(\\theta) $$\n",
    "\n",
    "Abstracting:\n",
    "## $$\\Bigg\\downarrow$$\n",
    "\n",
    "## $$ p(\\theta|x) = p(x|\\theta)\\cdot p(\\theta | \\alpha) $$\n",
    "\n",
    "i.e. the population helps make inference on individual ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is this hierarchical Bayesian framework good for?\n",
    "\n",
    "### If you have multiple sets of measurements, each of which are related, we may want to learn something not just about each individual measurement, *but also the population*\n",
    "\n",
    "In quantitative psychology for example you test multiple subjects on the same task. We then want to estimate a computational/mathematical model that describes the behavior on the task by a set of parameters. \n",
    "\n",
    "- We could thus fit a model to each subject individually, assuming they share no similarities; \n",
    "- or, pool all the data and estimate one model assuming all subjects are identical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"tacos.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Hierarchical modeling** allows the best of both worlds by modeling subjects’ similarities but also allowing estimiation of individual parameters.\n",
    "\n",
    "In elections, everyone is looking at the same ballot nationally, and we care about the overall population, but maybe we also care about how states/counties differ from each other.\n",
    "\n",
    "The same is true of countries during a global pandemic..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "E.g. your midterm - you have multiple galaxies, each with a potentially indpendent period-luminosity relation (indeed many of you treated it as such) because they have different ages/star formation history/whatever.\n",
    "\n",
    "Maybe you are interested in the P-L relation for just 1 galaxy, but maybe you also care about the population of all of them so you can derive the Hubble constant.\n",
    "\n",
    "I didnd't have you do that for the slopes, but I did for the intercepts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Going Hierarchical: Inferring properties of a population\n",
    "\n",
    "... but what if we want to use the individuals to infer things (the $\\alpha$’s) about the population?\n",
    "\n",
    "i.e., **$p(\\theta|\\alpha)$ contains interesting physics and getting values for $\\alpha$ given the data (individuals) can help us understand the population**\n",
    "\n",
    "## $$ p(\\theta|x) = p(x|\\theta)\\cdot p(\\theta | \\alpha) $$\n",
    "\n",
    "## $$\\Bigg\\downarrow$$\n",
    "\n",
    "## $$ p(\\alpha, \\theta|x) = p(x|\\theta)\\cdot p(\\theta | \\alpha) \\cdot p(\\alpha)$$\n",
    "\n",
    "For your midterm, where we could have fit individual slopes $\\theta_i$ we did the extreme thing of $p(\\theta_i | \\alpha) = \\delta(\\theta_i - \\alpha)$ and fit just one global slope $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"hierarchical1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"hierarchical2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Build up complexity by layering conditional probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## And if you really don't care the parameters for individual measurements, just marginalize over them\n",
    "\n",
    "## $$\n",
    "p(\\alpha |x) \\propto\\left[\\int p(x | \\theta, \\alpha) p(\\theta | \\alpha) d\\theta\\right] p(\\alpha)=p(x | \\alpha) p(\\alpha)\n",
    "$$\n",
    "\n",
    "(This is what happens with your Gaussian Process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Hierarchical modeling is a statistically rigorous way  to make scientific inferences about a population (or specific object) based on many individuals (or observations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"hbm1.png\">\n",
    "\n",
    "courtesy: Tom Loredo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"hbm2.png\">\n",
    "\n",
    "courtesy: Tom Loredo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# WHY does this work??\n",
    "\n",
    "- All 18 players are humans playing baseball—they are members of a population, not arbitrary, unrelated binomial random number generators!\n",
    "\n",
    "- In the absence of data about player $i$, we may use the performance of the other players to guide a guess about that player’s performance—they provide indirect evidence about player $i$\n",
    "\n",
    "- But information that is relevant in the absence of data for $i$ remains relevant when we additionally obtain that data; shrinkage estimators account for this\n",
    "\n",
    "- There is “mustering and borrowing of strength” (Tukey) across the population\n",
    "\n",
    "- Hierarchical Bayesian modeling is the most flexible framework for generalizing this lesson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## There's many examples of situations where we care about populations:\n",
    "\n",
    "<img src=\"kepler_mass_radius.png\">\n",
    "\n",
    "[Kepler Planets from Lissauer, Dawson and Tremaine, 2014](https://ui.adsabs.harvard.edu/abs/2014Natur.513..336L/abstract)\n",
    "\n",
    "\n",
    "### Once we discover an object, we look for more to characterize their properties and understand their origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Or maybe we use many noisy observations of a single object to understand it's physics.\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"SN1987A_R.jpg\"></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><img src=\"SN1987A_B.jpg\"></th>\n",
    "  </tr>\n",
    "</table> \n",
    "\n",
    "[Larrson et al., 2019](https://ui.adsabs.harvard.edu/abs/2019ApJ...886..147L/abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"hierarchical3.png\">\n",
    "Courtesy: Angie Wolfgang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"hierarchical4.png\">\n",
    "Courtesy: Angie Wolfgang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"hierarchical5.png\">\n",
    "Courtesy: Angie Wolfgang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"hierarchical6.png\">\n",
    "Courtesy: Angie Wolfgang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"hierarchical7.png\">\n",
    "\n",
    "Courtesy: Angie Wolfgang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So let's look at a few different approaches to the midterm:\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "    <col width=\"300\">\n",
    "    <col width=\"300\">\n",
    "    <col width=\"300\">\n",
    "    <tr>\n",
    "        <td>\n",
    "            One Observation of one Cepehid (Use PSF fitting to get photometry):\n",
    "            <img src=\"HubbleVar.png\">\n",
    "        </td>\n",
    "        <td>\n",
    "            One Cepheid (Use Periodogram/GP to get period):\n",
    "            <img src=\"Cepheid-variabledb92-600x461.jpg\">\n",
    "        </td>\n",
    "        <td>\n",
    "            Many Cepheids in one galaxy (Use MLE to get slope and intercept of PL relation):\n",
    "            <img src=\"cepheid_data.png\">\n",
    "        </td>\n",
    "    </tr>  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: PGM for Many Cepehids in One Galaxy\n",
    "\n",
    "Many Cepheids in one galaxy.\n",
    "\n",
    "\n",
    "<img src=\"pgms_cepheids.png\">\n",
    "\n",
    "\n",
    "**This is a multi-level model**\n",
    "\n",
    "When we first saw this back in week 3, we weren't even Bayesian and didn't worry about things like priors, let alone hyper priors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: PGM for many Cepheids in many galaxies\n",
    "\n",
    "This is what the midterm question prompted:\n",
    "\n",
    "<img src=\"keygals.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Daft](https://docs.daft-pgm.org/en/latest/) is a package to draft probabilistic graphical models\n",
    "\n",
    "```\n",
    "pip install daft\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import daft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pgm = daft.PGM(observed_style=\"inner\", dpi=300)\n",
    "pgm.add_node(\"P\", r'$\\log_{10} P_{k,g}$', x=0, y=0, fixed=True)\n",
    "pgm.add_node(\"m\", r'$ m_{k,g}$', x=1, y=0, fixed=True, offset=(0, -16))\n",
    "pgm.add_node(\"sig\", r'$\\sigma_{k,g}$', x=1.5, y=-0.5, fixed=True, offset=(-8,0))\n",
    "pgm.add_node(\"dmu\", r'$\\Delta \\mu_g$', x=1.5, y=1.4)\n",
    "pgm.add_node('mobs', r'$m_{k,g}^{\\mathrm{obs}}$', x=2, y=0, observed=True)\n",
    "pgm.add_node(\"sigint\", r'$\\sigma_{\\mathrm{int}}$', x=2, y=2.2)\n",
    "pgm.add_node(\"Z\", r'$Z_{4258}$', x=1, y=2.2)\n",
    "pgm.add_node(\"a\", r'$a$', x=0.35, y=2.2)\n",
    "\n",
    "pgm.add_edge(\"P\", \"m\")\n",
    "pgm.add_edge(\"m\", \"mobs\")\n",
    "pgm.add_edge(\"sig\", \"mobs\")\n",
    "pgm.add_edge(\"dmu\", \"m\")\n",
    "pgm.add_edge('Z',\"m\")\n",
    "pgm.add_edge(\"a\", \"m\")\n",
    "pgm.add_edge(\"sigint\", \"mobs\" )\n",
    "\n",
    "pgm.add_plate([-0.5, -1, 3, 2], \"cepheids $k$\", )\n",
    "pgm.add_plate([-0.78, -1.35, 3.4, 3.2], \"galaxies $g$\", )\n",
    "pgm.render();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This model subtly had a population parameter and individual parameters $Z_{4258}$ and $\\Delta\\mu_g$\n",
    "\n",
    "## This choice of parameterization was good \n",
    "\n",
    "## We only cared about the population parameter $Z_{4258}$ and could regard the $\\Delta\\mu$'s as nuisance parameters\n",
    "\n",
    "\n",
    "# $$\n",
    "\\log H_{0}= \\sum_{g} \\frac{\\left(m_{v, g}^{0}+5 a_{v}\\right)-\\left(\\Delta \\mu_{g}+Z_{4258}\\right)+25}{5}\n",
    "$$\n",
    "\n",
    "(That's what the sum over $g$ is - marginalizing over all the galaxies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## But we also forced all the cepheids in all the galaxies to obey the exact same P-L relation\n",
    "\n",
    "\n",
    "## That's a modeling choice, but it may not be a very good one\n",
    "\n",
    "<img src=\"all_gals.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Yet, visibly, the cepehids do not all have vastly different slopes...\n",
    "\n",
    "\n",
    "## So what we if we treated the P-L relation as a population parameter, but allowed each galaxy to vary a little?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pgm = daft.PGM(observed_style=\"inner\", dpi=300)\n",
    "pgm.add_node(\"P\", r'$\\log_{10} P_{k,g}$', x=0, y=0, fixed=True)\n",
    "pgm.add_node(\"m\", r'$ m_{k,g}$', x=1, y=0, fixed=True, offset=(0, -16))\n",
    "pgm.add_node(\"sig\", r'$\\sigma_{k,g}$', x=1.5, y=-0.5, fixed=True, offset=(-8,0))\n",
    "pgm.add_node(\"dmu\", r'$\\Delta \\mu_g$', x=1.5, y=1.4)\n",
    "pgm.add_node('mobs', r'$m_{k,g}^{\\mathrm{obs}}$', x=2, y=0, observed=True)\n",
    "pgm.add_node(\"sigint\", r'$\\sigma_{\\mathrm{int}}$', x=2, y=2.2)\n",
    "pgm.add_node(\"Z\", r'$Z_{4258}$', x=1, y=2.2)\n",
    "pgm.add_node(\"a\", r'$a$', x=0.35, y=2.2)\n",
    "pgm.add_node(\"a_g\", r'$a_g$', x=0.35, y=1.4)\n",
    "\n",
    "pgm.add_edge(\"P\", \"m\")\n",
    "pgm.add_edge(\"m\", \"mobs\")\n",
    "pgm.add_edge(\"sig\", \"mobs\")\n",
    "pgm.add_edge(\"dmu\", \"m\")\n",
    "pgm.add_edge('Z',\"m\")\n",
    "pgm.add_edge(\"a\", \"a_g\")\n",
    "pgm.add_edge(\"a_g\", \"m\")\n",
    "pgm.add_edge(\"sigint\", \"mobs\" )\n",
    "\n",
    "pgm.add_plate([-0.5, -1, 3, 2], \"cepheids $k$\", )\n",
    "pgm.add_plate([-0.78, -1.35, 3.4, 3.2], \"galaxies $g$\", )\n",
    "pgm.render();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from matplotlib import pyplot as plt\n",
    "import pymc as pm\n",
    "\n",
    "# same code as in midterm to load the data\n",
    "exec(open('cepheids.py').read())\n",
    "ceph = Cepheids('R11ceph.dat')\n",
    "hosts = ceph.list_hosts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lets implement the case of each galaxy treated independently of each other \n",
    "\n",
    "i.e. $2 N_{\\text{host}}$ parameters - slope and intercept for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# define a likelihood for each Cepehid galaxy\n",
    "def neg_log_like(theta, i):\n",
    "    ceph.select(hosts[i])\n",
    "    mobs = ceph.mobs\n",
    "    logP = ceph.logP\n",
    "    sigma_obs = ceph.sigma\n",
    "    b, mu = theta\n",
    "    return np.sum((mobs - b*logP - mu)**2./(sigma_obs**2.))\n",
    "\n",
    "results = {}\n",
    "\n",
    "\n",
    "for i, ID in enumerate(hosts):\n",
    "    ceph.select(ID)\n",
    "    mobs = ceph.mobs\n",
    "    logP = ceph.logP\n",
    "    sigma_obs = ceph.sigma\n",
    "  \n",
    "    # fit each of the Cepehid galaxies\n",
    "    res = minimize(neg_log_like, (-2., np.median(mobs)), args=(i,))\n",
    "    print(res.x)\n",
    "    results[ID] = res.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# just create an integer mapping for each host so we can use as an index\n",
    "nhosts = len(hosts)\n",
    "host_map = dict(zip(hosts, np.arange(len(hosts))))\n",
    "host_idx = [host_map[str(int(x))] for x in ceph.data[:,1]]\n",
    "host_idx = np.array(host_idx)\n",
    "print(host_map)\n",
    "\n",
    "mobs = ceph.data[:,2]\n",
    "err  = ceph.data[:,3]\n",
    "logP = np.log10(ceph.data[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as hierarchical_model: \n",
    "\n",
    "    # Hyperpriors for group nodes\n",
    "    mu_a = pm.Normal('slope', mu=-3, sigma=0.5)\n",
    "    sigma_a = pm.HalfNormal('slopescatter', 0.5)\n",
    "    sigma_b = pm.HalfNormal('dmuscatter         ', 5.)\n",
    "    Z = pm.Normal('Z', mu=25, sigma=5.)\n",
    "    \n",
    "    a = pm.Normal('a', mu=mu_a, sigma=sigma_a, shape=nhosts)\n",
    "    dmu = pm.Normal('dmu', mu=0, sigma=sigma_b, shape=nhosts)\n",
    "    \n",
    "    sig_int = 0.1\n",
    "    \n",
    "    mpred = a[host_idx]*logP + Z + dmu[host_idx]\n",
    "    like = pm.Normal('likelihood', mu=mpred, sigma =np.sqrt(sig_int**2. + err**2), observed=mobs)\n",
    "    \n",
    "    hm = pm.sample(2000, tune=500, target_accept=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "res_hm = pm.summary(hm)\n",
    "res_hm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_trace(hm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "for i, host in enumerate(hosts):\n",
    "    old_slope, old_intercept = results[host]\n",
    "    new_slope = res_hm.loc[f'a[{i}]']['mean']\n",
    "    new_intercept = res_hm.loc[f'dmu[{i}]']['mean'] + res_hm.loc['Z']['mean']\n",
    "    ax.scatter(old_intercept, old_slope, color='r', marker='o')\n",
    "    ax.scatter(new_intercept, new_slope, color='b', marker='s')\n",
    "    ax.plot([old_intercept, new_intercept], [old_slope, new_slope], 'k--')\n",
    "    \n",
    "ax.set_xlabel('Intercept')\n",
    "ax.set_ylabel('Slope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
