{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1024,\n",
    "        'height': 768,\n",
    "        'scroll': True,\n",
    "})\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [7, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 14, ASTR 596: Fundamentals of Data Science\n",
    "\n",
    "\n",
    "## Deep Learning\n",
    "#### Many pretty pictures are borrowed from Federica Bianco and Kostya Malanchev\n",
    "\n",
    "### Gautham Narayan \n",
    "\n",
    "##### <gsn@illinois.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th>Method</th>\n",
    "    <th>Unsupervised</th>\n",
    "    <th>Supervised</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>Centroid-based</td>\n",
    "    <td>k-Means</td>\n",
    "    <td>kNN w/ k=1 (aka Nearest Centroid - we've not covered this but I'll give you an example ntbk) </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Linkage-based Model</td>\n",
    "    <td>Hierarchical Clustering</td>\n",
    "    <td>Decision Trees/Random Forests</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Parametric Density Model</td>\n",
    "    <td>GMMs, Extreme Deconvolution</td>\n",
    "    <td>Gaussian Naive Bayes, LDA, QDA</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Nonparametric Density/Locally Weighted*</td>\n",
    "    <td>DBSCAN/Optics</td>\n",
    "      <td>kNN with k>1, <strong>SVM, MLPs, DNN</strong></td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "* = KDEs are perfectly useable in either column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## And now for biology!!\n",
    "\n",
    "There's a long history of design being influenced by biology\n",
    "<img src=\"rsta20160192f19.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## And this includes algorithms\n",
    "<img src=\"msb201178-fig-0002-m.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"bioneuron.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Signals are responses to stimuli caused by changing the balanace of Na+ and K+ ions within the Axon (which in turn changes the bias voltage between the terminal and dendrite) - allows a signal to be transmitted down the axon.\n",
    "\n",
    "<img src=\"neuronal_action_potential.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Action_Potential_Propagation_Along_Axon_Animation.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"myelinated-axon.gif\">\n",
    "\n",
    "If this is interesting, you can look at a more detailed explanation here: https://www.getbodysmart.com/nervous-system/action-potential-events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"McCulloughPitts1943.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"bioneuron2.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"mp_neuron.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"perceptron1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"perceptron2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"perceptron3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## OK, but how do we get the weights when this is linear regression, but the output is binary (0, 1)\n",
    "\n",
    "### Recall the discriminant function from Tuesday:\n",
    "\n",
    "\n",
    "## $$\\begin{eqnarray} \\widehat{y} & = & \\left\\{ \\begin{array}{cl} 1 & \\mbox{if $g(x) > \\theta$}, \\\\ 0 & \\mbox{otherwise,} \\end{array} \\right. \\end{eqnarray}$$\n",
    "\n",
    "Where $\\theta$ is the **threshold**\n",
    "\n",
    "We compare the output vector $y$ to the target/truth $t$ and we want to minimize the error\n",
    "\n",
    "## $$ E = 1/2 \\cdot (t-y)^2$$\n",
    "\n",
    "To minimize the error, we need to change the weights of the inputs to get $t$ close to $y$.\n",
    "\n",
    "Let's do this with our old friend, **gradient descent**\n",
    "\n",
    "## $$w_i = w'_i - \\eta \\cdot \\frac{dE}{dw_i} $$\n",
    "\n",
    "Where $\\eta$ is some constant that we'll call the **learning rate**\n",
    "\n",
    "## $$w_i = w'_i + \\eta \\cdot (t-y) \\cdot\\frac{dy}{dw_i} $$\n",
    "\n",
    "## $$\\frac{dy}{dw_i} = x_i $$\n",
    "\n",
    "so..\n",
    "\n",
    "## $$w_i = w'_i + \\eta \\cdot (t-y) \\cdot x_i $$\n",
    "\n",
    "\n",
    "If $y != t$ (and remember the t is just a 0 or 1):\n",
    "## $$ w_{\\text{new}}  = w_{\\text{old}} + \\eta \\cdot t \\cdot x$$\n",
    "\n",
    "and $y == t$, don't update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"mk1_perceptron.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"1960_preceptron.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Mark_I_perceptron.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The first step to neural networks: Linear Regression\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "f(x) = \\sum_j w_j x_j + c\n",
    "\\end{equation}\n",
    "\n",
    "### MSE loss makes it to be the ordinal linear least sqaures problem\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "\\min_{\\theta=\\{w_j,c\\}} \\sum_i \\left[y_i - \\left(\\sum_j w_j x_{ij} + c\\right)\\right]^2\n",
    "\\end{equation}\n",
    "\n",
    "Do $\\partial / \\partial w_i = 0, \\partial / \\partial c = 0$ and you have the (analytical) solution!\n",
    "\n",
    "### Regression\n",
    "\n",
    "![](tf_linear_regression_good.png)\n",
    "\n",
    "[TF playground](http://playground.tensorflow.org/#activation=sigmoid&batchSize=5&dataset=gauss&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=&seed=0.65532&showTestData=false&discretize=true&percTrainData=40&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=regression&initZero=false&hideText=false)\n",
    "\n",
    "### Classification\n",
    "\n",
    "Actualy it is also regression, but the real output value is converted to a binary label.\n",
    "However, if we'd like to have outputs to be between 0 and 1 we need some normalisation, for example logistic function $\\sigma(z) = 1 / (1 + \\exp{-z})$.\n",
    "In this case the model is called logistic regression.\n",
    "\n",
    "![](tf_linear_regression_clf_good.png)\n",
    "\n",
    "[TF playground](http://playground.tensorflow.org/#activation=sigmoid&batchSize=5&dataset=gauss&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=&seed=0.73566&showTestData=false&discretize=true&percTrainData=40&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
    "\n",
    "### But what if the problem is not linear...\n",
    "\n",
    "![](tf_linear_regression_clf_bad.png)\n",
    "\n",
    "[TF playground](http://playground.tensorflow.org/#activation=sigmoid&batchSize=5&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=&seed=0.12820&showTestData=false&discretize=true&percTrainData=40&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
    "\n",
    "### Sometimes we can solve it with a bit of feature engineering\n",
    "\n",
    "![](tf_linear_regression_engineering.png)\n",
    "\n",
    "[TF playground](http://playground.tensorflow.org/#activation=sigmoid&batchSize=5&dataset=circle&regDataset=reg-gauss&learningRate=0.03&regularizationRate=0&noise=0&networkShape=2&seed=0.17882&showTestData=false&discretize=true&percTrainData=40&x=false&y=false&xTimesY=false&xSquared=true&ySquared=true&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&numHiddenLayers_hide=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Making the model non-linear\n",
    "\n",
    "\\begin{eqnarray}\n",
    "&\\large h_1(x) = g\\left(\\sum_j w^{\\{1\\}}_j x + c^{\\{1\\}}\\right), \\\\\n",
    "&\\large h_2(x) = g\\left(\\sum_j w^{\\{2\\}}_j x + c^{\\{2\\}}\\right), \\\\\n",
    "&\\large h_3(x) = g\\left(\\sum_j w^{\\{3\\}}_j x + c^{\\{3\\}}\\right), \\\\\n",
    "&\\large f(x) = w_{h1} h_1(x) + w_{h2} h_2(x) + w_{h3} h_3(x) + c_{h},\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $g$ is some (non-linear) scalar function, same for all transformations. Note that \"weights\" $w$ and \"biases\" (or, if we'd write minus instead of plus, \"thresholds\") $c$ are different for each transformation.\n",
    "\n",
    "![](tf_slp3.png)\n",
    "\n",
    "[TF playground](http://playground.tensorflow.org/#activation=sigmoid&batchSize=5&dataset=circle&regDataset=reg-gauss&learningRate=0.03&regularizationRate=0&noise=0&networkShape=3&seed=0.17882&showTestData=false&discretize=true&percTrainData=40&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&numHiddenLayers_hide=false)\n",
    "\n",
    "### Can we make it better? Easy!\n",
    "\n",
    "It has $2 \\times 8 + 8 \\times 1 = 24$ weights.\n",
    "\n",
    "![](tf_slp8.png)\n",
    "\n",
    "[TF playground](http://playground.tensorflow.org/#activation=sigmoid&batchSize=5&dataset=circle&regDataset=reg-gauss&learningRate=0.03&regularizationRate=0&noise=0&networkShape=8&seed=0.17882&showTestData=false&discretize=true&percTrainData=40&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&numHiddenLayers_hide=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"perceptron4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"perceptron5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"perceptron6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## \n",
    "\n",
    "- The Rosenblatt Perceptron uses the class labels to learn model coefficients\n",
    "\n",
    "- The Adaline Neuron uses continuous predicted values (from the net input) to learn the model coefficients, which is more “powerful” since it tells us by “how much” we were right or wrong\n",
    "    - the weights are how sensitive a neuron is \n",
    "    - the activation function turns neurons on/off \n",
    "    - The loss function of the network is updated to penalize models in proportion to the magnitude of their activation\n",
    "    - prevents updating weights when y gets close to t - i.e. prevents overfitting!\n",
    "\n",
    "    \n",
    "## $$w_i = w'_i + \\eta \\cdot (t-y) \\cdot\\frac{df(y)}{dw_i} $$\n",
    "\n",
    "<img src=\"adaline_vs_perceptron.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"activation_functions.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-not actually class-class exercise:\n",
    "\n",
    "https://jalammar.github.io/feedforward-neural-networks-visual-interactive/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer perceptron = dense neural network\n",
    "\n",
    "In practice, single layer is not sufficient, we need huge neuron to solve real world problems.\n",
    "\n",
    "We can easialy add layers feeding the next perceptron to the output of the previous one.\n",
    "\n",
    "![](tf_mlp_relu.png)\n",
    "[TF Playground](http://playground.tensorflow.org/#activation=relu&regularization=L2&batchSize=5&dataset=circle&regDataset=reg-gauss&learningRate=0.001&regularizationRate=0.001&noise=0&networkShape=3,3,3,3,3,3&seed=0.95354&showTestData=false&discretize=true&percTrainData=40&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&numHiddenLayers_hide=false)\n",
    "\n",
    "Multi-layer neural network is covered by \"arbitrary depth\" universal approximation theorem, which is proven for some specific cases.\n",
    "\n",
    "### Fit spiral dataset yourself: [TF playground](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=&seed=0.04277&showTestData=false&discretize=false&percTrainData=70&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
    "\n",
    "<!--\n",
    "![](tf_mlp_relu_spiral.png)\n",
    "[TF playground](http://playground.tensorflow.org/#activation=relu&regularization=L2&batchSize=5&dataset=spiral&regDataset=reg-gauss&learningRate=0.001&regularizationRate=0.001&noise=0&networkShape=4,4,4,4,4,4&seed=0.75252&showTestData=false&discretize=true&percTrainData=70&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&numHiddenLayers_hide=false)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"mlp1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mlp2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-class exercise: Implementing a simple MLP Classifier with `sklearn`\n",
    "\n",
    "Our neural network will take an $\\mathbf{x} = (x_1, x_2)$ vector as input and output a $K$-dimensional vector $\\mathbf{p}=(p_1,\\dots,p_K)$ of class probabilities. For simplicity we'll focus on a single choice of activation function, the ReLU function $f(x) = \\max(x, 0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports / plotting configuration\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "plt.rcParams['image.interpolation'] = 'nearest'  # hard classification boundaries\n",
    "plt.rcParams['image.cmap'] = 'viridis'\n",
    "np.random.seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate spiral sample data\n",
    "# N = num points\n",
    "# K = num spiral arms\n",
    "# sigma = some noise\n",
    "def spiral_data(N, K=3, sigma=0.1):\n",
    "    X = np.zeros((N * K, 2))\n",
    "    y = np.zeros(N * K, dtype='int')\n",
    "\n",
    "    for j in range(K):\n",
    "        ix = range(N * j, N * (j + 1))\n",
    "        r = np.linspace(0.0, 1, N)  # radius\n",
    "        theta = 2 * np.pi * j / K + np.linspace(0, 3 * np.pi, N) + np.random.randn(N) * sigma\n",
    "        X[ix] = np.c_[r * np.sin(theta), r * np.cos(theta)]\n",
    "        y[ix] = j\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "K = 3\n",
    "X, y = spiral_data(N, K, 0.1)\n",
    "\n",
    "# Visualize the generated data\n",
    "fig = plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap='viridis')\n",
    "plt.xlim([-1, 1])\n",
    "plt.ylim([-1, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class SingleLayerReLU(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Skeleton code for single-layer multi-class neural network classifier w/ ReLU activation.\n",
    "    NOTE: Whenever you change the code below, you need to re-run this cell AND re-initialize\n",
    "    your model (`model = SingleLayerNet(...)`) in order to update your specific `model` object.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, num_classes, sigma_init=0.01):\n",
    "        \"\"\"Initialize weights with Gaussian noise scaled by `sigma_init` and\n",
    "        biases with zeros.\n",
    "        \"\"\"\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.W1 = sigma_init * np.random.randn(hidden_size, 2)\n",
    "        self.W2 = sigma_init * np.random.randn(num_classes, hidden_size)\n",
    "        self.b1 = np.zeros(hidden_size)\n",
    "        self.b2 = np.zeros(num_classes)\n",
    "    \n",
    "    def loss(self, y, P):\n",
    "        \"\"\"Compute total softmax loss.\n",
    "        Inputs:  y -> (N,) array of true (integer) labels\n",
    "                 p -> (N, K) array of predicted probabilities\n",
    "        Outputs: L -> total loss value       \n",
    "        \"\"\"\n",
    "        return -np.sum(np.log(P[range(len(P)), y]))\n",
    "        \n",
    "    def dloss(self, X, y):\n",
    "        \"\"\"Compute gradient of softmax loss with respect to network weights.\n",
    "        Inputs:  X -> (N, 2) array of network inputs\n",
    "                 y -> (N,) array of true labels\n",
    "        Outputs: dW1 -> (hidden_size, 2) array of weight derivatives\n",
    "                 dW2 -> (hidden_size, 2) array of weight derivatives\n",
    "                 db1 -> (hidden_size,) array of bias derivatives\n",
    "                 db2 -> (hidden_size,) array of bias derivatives\n",
    "        \"\"\"\n",
    "        H = np.maximum(0, X @ self.W1.T + self.b1)  # ReLU activation\n",
    "        Z = H @ self.W2.T + self.b2\n",
    "        P = np.exp(Z) / np.sum(np.exp(Z), axis=1, keepdims=True)\n",
    "\n",
    "        dZ = P\n",
    "        dZ[range(len(X)), y] -= 1\n",
    "\n",
    "        dW2 = (H.T @ dZ).T\n",
    "        db2 = np.sum(dZ, axis=0)\n",
    "\n",
    "        dH = dZ @ self.W2\n",
    "        dH[H <= 0] = 0  # backprop ReLU  activation\n",
    "\n",
    "        dW1 = (X.T @ dH).T\n",
    "        db1 = np.sum(dH, axis=0)\n",
    "        \n",
    "        return (dW1, dW2, db1, db2)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Compute forward pass for all input values.\n",
    "        Inputs:  X -> (N, 2) array of network inputs\n",
    "        Outputs: P -> (N, K) array of class probabilities\n",
    "        \"\"\"\n",
    "        H = np.maximum(0, X @ self.W1.T + self.b1)  # ReLU activation\n",
    "        Z = H @ self.W2.T + self.b2\n",
    "        P = np.exp(Z) / np.sum(np.exp(Z), axis=1, keepdims=True)\n",
    "        \n",
    "        return P\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Compute most likely class labels for all input values.\n",
    "        Inputs:  X -> (N, 2) array of network inputs\n",
    "        Outputs: P -> (N, K) array of class probabilities\n",
    "        \"\"\"\n",
    "        P = self.predict_proba(X)\n",
    "        return np.argmax(P, 1)\n",
    "        \n",
    "    def fit(self, X, y, step_size=3e-3, n_iter=10000):\n",
    "        \"\"\"Optimize model parameters W1, W2, b1, b2 via gradient descent.\n",
    "        Inputs:  X -> (N, 2) array of network inputs\n",
    "                 y -> (N,) array of true labels\n",
    "                 step_size -> gradient descent step size\n",
    "                 n_iter -> number of gradient descent steps to perform\n",
    "        Outputs: losses -> (n_iter,) array of loss values after each step\n",
    "        \"\"\"\n",
    "        losses = np.zeros(n_iter + 1)\n",
    "        for i in range(0, n_iter + 1):\n",
    "            dW1, dW2, db1, db2 = self.dloss(X, y)\n",
    "            self.W1 -= step_size * dW1\n",
    "            self.W2 -= step_size * dW2\n",
    "            self.b1 -= step_size * db1\n",
    "            self.b2 -= step_size * db2\n",
    "            \n",
    "            P = self.predict_proba(X)\n",
    "            losses[i] = self.loss(y, P)\n",
    "            \n",
    "            if i % 1000 == 0:\n",
    "                print(\"Iteration {}: loss={}\".format(i, losses[i]))\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, X, y, step=0.02):\n",
    "    x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "    y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, step), np.arange(y_min, y_max, step))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"Accuracy: {}%\".format(100 * np.mean(y == model.predict(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SingleLayerReLU(100, K)\n",
    "visualize_predictions(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SingleLayerReLU(100, K)\n",
    "losses = model.fit(X, y, step_size=2e-3, n_iter=20000)\n",
    "plt.plot(losses, '-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Total loss');\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "X, y = spiral_data(100, 5, 0.1)\n",
    "single_layer_model = MLPClassifier((8,), activation='identity', solver='lbfgs')\n",
    "single_layer_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(single_layer_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = spiral_data(100, 3, 0.1)\n",
    "multi_layer_model = MLPClassifier((100, 100), alpha=0.5, activation='relu', solver='lbfgs')\n",
    "multi_layer_model.fit(X, y)\n",
    "visualize_predictions(multi_layer_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can view the activations of the final layer if needed\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "X, y = spiral_data(100, 2, 0.1)\n",
    "multi_layer_model = MLPClassifier((64, 6), alpha=0.5, activation='relu', solver='lbfgs')\n",
    "multi_layer_model.fit(X, y)\n",
    "visualize_predictions(multi_layer_model, X, y)\n",
    "\n",
    "def visualize_activations(model, unit, X, y, step=0.02):\n",
    "    \"\"\"Visualize activations of ith neuron of last layer.\"\"\"\n",
    "    model = deepcopy(model)\n",
    "    model.coefs_[-1][:unit] = 0  # zero out other units\n",
    "    model.coefs_[-1][unit] = 1  # just want the activation function\n",
    "    model.coefs_[-1][(unit + 1):] = 0  # zero out other units\n",
    "    \n",
    "    x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "    y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, step), np.arange(y_min, y_max, step))\n",
    "    Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 0]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    \n",
    "fig, ax = plt.subplots(3, 2, figsize=(14, 20))\n",
    "for i in range(len(multi_layer_model.coefs_[-1])):\n",
    "    plt.sca(ax.ravel()[i])\n",
    "    visualize_activations(multi_layer_model, i, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-class exercise: Implementing a simple MLP Regressor with `sklearn`\n",
    "\n",
    "### Predict house prices in Boston with an MLP regressor\n",
    "### Use `cross_val_predict` to see how well you do after 5-fold cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute this cell \n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "boston = load_boston()\n",
    "print(boston.DESCR)\n",
    "\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "## YOUR CODE HERE\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.scatter(ytest,ypred, marker='.', color='k')\n",
    "plt.xlabel(\"Actual Value [x$1000]\")\n",
    "plt.ylabel(\"Predicted Value [x$1000]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can get the coefficients/weights of each layer if you need\n",
    "[coef.shape for coef in clf.coefs_]\n",
    "clf.coefs_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why have one hidden layer when you can have two for twice the price!\n",
    "\n",
    "<img src=\"dnn1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dnn2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of neural networks\n",
    "## - \"Neural Network Zoo\" (Asimov Institute)\n",
    "<img src=\"http://www.asimovinstitute.org/wp-content/uploads/2016/09/neuralnetworks.png\">\n",
    "\n",
    "https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is clearly much more sophisticated than our basic perceptron. \"Deep\" networks consist of tens of layers with thousands of neurons. These large networks have become usabel thanks to two breakthroughs: the use of sparse layers and the power of graphics processing units (GPUs).\n",
    "Many image processing tasks involve convolving an image with a 2-dimensional kernel as shown below.\n",
    "![Convolution example](https://developer.apple.com/library/content/documentation/Performance/Conceptual/vImage/Art/kernel_convolution.jpg)\n",
    "\n",
    "The sparse layers or convolutional layers in a deep network contain a large number of hidden nodes but very few synapses. The sparseness arises from the relatively small size of a typical convolution kernel (15x15 is a large kernel), so a hidden node representing one output of the convolution is connected to only a few input nodes. Compare this the our previous perceptron, in which every hidden node was connected to every input node.\n",
    "\n",
    "Even though the total number of connections is greatly reduced in the sparse layers, the total number of nodes and connections in a modern deep network is still enormous. Luckily, training these networks turns out to be a great task for GPU acceleration! Serious work using neural networks is almost always done usign specialized GPU-accelerated platforms.\n",
    "\n",
    "- Great flexibility: we can use NNs with images, sound, text, time series, probability distributions, etc.\n",
    "- Combination of linear algebra and scalar functions allow fast training and evaluation on GPUs and special processor units such as tensor PUs, neural PUs or even field-programmable gate arrays.\n",
    "- Re-usability: you can take someone else's architecture and train the network for your data, or you can even get someone else's weights and fine-tune it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning frameworks\n",
    "\n",
    "## - Python(-compatible) - like sklearn only for NN models:\n",
    "- Tensorflow (Google)\n",
    "- Keras (frontend for TensorFlow + Theano)\n",
    "- Theano (Université de Montréal)\n",
    "- Caffe (UC Berkeley)\n",
    "- CNTK (Microsoft)\n",
    "- MXNet (Amazon+Baidu+...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mindmap.jpg\">"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:yse]",
   "language": "python",
   "name": "conda-env-yse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
