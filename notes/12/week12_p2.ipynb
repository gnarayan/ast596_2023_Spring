{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1024,\n",
    "        'height': 768,\n",
    "        'scroll': True,\n",
    "})\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib notebook\n",
    "%pylab\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 12, ASTR 596: Fundamentals of Data Science\n",
    "\n",
    "\n",
    "## PCA Round 2, Clustering\n",
    "\n",
    "### Gautham Narayan \n",
    "##### <gsn@illinois.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Setup:\n",
    "\n",
    "#### Given some measurement of a source on the sky, we want to decide if they're a white dwarf/giant/main sequence stat/QSO/etc.\n",
    "\n",
    "#### This lets us look at a population of objects and that is useful if we want to do things like infer population hyperparameters\n",
    "\n",
    "#### We already have a way to do this with stars:\n",
    "\n",
    "<img src=\"HR_diagram.jpg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### BUT WE DO NOT MEASURE EITHER ABSOLUTE MAGNITUDE/TEMPERATURE!\n",
    "\n",
    "#### we don't know distance so we get apparent magnitudes, we can get colors, but lots of things affect colors of objects - e.g. dust\n",
    "\n",
    "#### Dust makes things red - it makes it hard to distinguish between a star that is red intrinsically or because it is extincted\n",
    "\n",
    "#### If broadband magnitudes don't really have sufficient information to determine spectral class (though one group is going to try for their final!) then the only thing we can do is get better measurements with more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### What we can measure that isn't impacted as much by dust is spectra \n",
    "\n",
    "#### Dust is a broad continuum effect and doesn't impact sharp spectral lines \n",
    "\n",
    "<img src=\"stellar_spectra.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Spectra are how Willamina Fleming first came up with the first classification scheme (A = most hydrogen, B = slightly less hydrogen...)\n",
    "\n",
    "#### (It wasn't until Annie Jump Cannon reordered this into a temperature scale that we got the current classification order) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Spectra might give us a way of classifying objects, but we can't be doing this by eye anymore like Willamina Fleming\n",
    "\n",
    "#### DESI has 5000 fibers so ~100,000 spectra/night\n",
    "\n",
    "\n",
    "<img src=\"DESI-focal-plane-5K-robots.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### We have some labelled spectra so one option is to train a ML classifier like a random forest on those spectra with labels and use them to predict labels for other objects without labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### But the problem is that the training set may not be representative - DESI is a new instrument on a 4m telescope on a good site, but if we use historical spectra they're from a different site with a different instrument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Moreover, DESI is looking at objects at higher-redshifts and will at not just stars, but also galaxies and everything else there is - entire classes may not be in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### So that suggests that we don't try supervised learning approaches but rather group objects by how similar their spectra are - clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Because this is an unsuperivsed learning approach and we don't need labels, we can potentially identify groups that we've not studied well before (e.g. Changing look AGN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## OK, so this is in principle a good idea but there is a challenge\n",
    "\n",
    "- If we want to define a cluster, what we really mean is that we are taking some N-dimensional space and within in enclosing off some volume\n",
    "\n",
    "- Points inside the volume are cluster members, and points outside are not\n",
    "\n",
    "- Lets imagine for a second that we define that volume as a multi-variate Gaussian\n",
    "\n",
    "- In 2D that looks like this ellipse:\n",
    "<img width=\"500\" src=\"pca-scatter.png\">\n",
    "\n",
    "- To define a multivariate Gaussian you need the mean and the covariance\n",
    "    - in 1-D this is 1 for the mean and 1 for the variance\n",
    "    - in 2-D this is 2 for the mean (x, y) and 4 for the covariance matrix (2x2)\n",
    "    - in N-D this is N for the mean + NxN for the covariance\n",
    "\n",
    "We've seen this already with MCMC - this is the *curse of dimensionality* (which would make a really good band name coming to think of it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA \n",
    "from astroML.datasets import sdss_corrected_spectra\n",
    "from astroML.decorators import pickle_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# AND RUN THIS\n",
    "#------------------------------------------------------------\n",
    "# Download data\n",
    "data = sdss_corrected_spectra.fetch_sdss_corrected_spectra()\n",
    "spectra = sdss_corrected_spectra.reconstruct_spectra(data)\n",
    "wavelengths = sdss_corrected_spectra.compute_wavelengths(data)\n",
    "\n",
    "for i in range(5):\n",
    "    plot(wavelengths, spectra[i] - spectra[i].mean() + 10*i, lw=0.5, alpha=0.5 )\n",
    "xlabel('Attributes/Columns: Wavlength ($\\AA$)')\n",
    "ylabel('Samples/Rows: Flux + constant')\n",
    "print(f'Our spectra have {len(wavelengths)} features/dimensions')\n",
    "print(f'We have {len(spectra)} spectra/samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### So a naive estimate is that to define just one cluster in spectrum space, we need ~1E6 parameters...\n",
    "\n",
    "#### That's obviously not going to cut it\n",
    "\n",
    "#### But just because a spectrum has a 1000 measurements of flux with wavelength, do we actually need a 1000 numbers to describe it?\n",
    "\n",
    "#### With periodic time-series, even if we had N measurements as a function of time, we could express them as a sum of Fourier components \n",
    "    - This is dimensionality reduction - rather than store the original N measurements, we store amplitudes and phases for hopefully a few Fourier components\n",
    "    - A clever choice of basis (sinusoids) makes this possible because the particular choice of basis vectors encode a lot of the variation in the data\n",
    "\n",
    "## Can we play the same game with spectra - i.e. can we describe a 1000 element row vector with fewer numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "from matplotlib import pyplot as plt\n",
    "from astroML.datasets import fetch_vega_spectrum\n",
    "from astroML.sum_of_norms import sum_of_norms, norm\n",
    "\n",
    "# Fetch the data\n",
    "x, y = fetch_vega_spectrum()\n",
    "\n",
    "# truncate the spectrum\n",
    "mask = (x >= 2000) & (x < 10000)\n",
    "x = x[mask]\n",
    "y = y[mask]\n",
    "\n",
    "for n_gaussians in (10, 50, 100, 200):\n",
    "    # compute the best-fit linear combination\n",
    "    w_best, rms, locs, widths = sum_of_norms(x, y, n_gaussians,\n",
    "                                             spacing='linear',\n",
    "                                             full_output=True)\n",
    "\n",
    "    norms = w_best * norm(x[:, None], locs, widths)\n",
    "\n",
    "    # plot the results\n",
    "    plt.figure()\n",
    "    plt.plot(x, y, '-k', label='input spectrum')\n",
    "    ylim = plt.ylim()\n",
    "\n",
    "    plt.plot(x, norms, ls='-', c='#FFAAAA')\n",
    "    plt.plot(x, norms.sum(1), '-r', label='sum of gaussians')\n",
    "    plt.ylim(-0.1 * ylim[1], ylim[1])\n",
    "\n",
    "    plt.legend(loc=0)\n",
    "\n",
    "    plt.text(0.97, 0.8,\n",
    "             \"rms error = %.2g\" % rms,\n",
    "             ha='right', va='top', transform=plt.gca().transAxes)\n",
    "    plt.title(\"Fit to a Spectrum with a Sum of %i Gaussians\" % n_gaussians)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"chernobyl.jpg\" width=\"30%\">\n",
    "\n",
    "### Gaussians are not the best choice of basis vectors to describe the variation of spectra\n",
    "\n",
    "### They work pretty well to describe spectral lines, but a Gaussian doesn't look like a whole spectrum...\n",
    "\n",
    "### But what this is telling us is that the flux (at least at nearby wavelengths) is *correlated*\n",
    "\n",
    "### We want to compare all the spectra on an equal footing, but they have vastly different fluxes, so we'll normalize first by subtracting the mean (i.e. centering on 0) and dividing by standard deviation - this way we are comparing **shapes** of the spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "from scipy.stats import zscore\n",
    "x = 100\n",
    "y = 120\n",
    "# This just subtracts the mean flux of each spectrum and divides by the standard deviation - i.e. renormalization\n",
    "whiten = zscore(spectra, axis=1)\n",
    "scatter(whiten[:,x], whiten[:,y], color='k', marker='o', alpha=0.1)\n",
    "xlabel(f'Flux at {wavelengths[x]:.1f} $\\AA$')\n",
    "ylabel(f'Flux at {wavelengths[y]:.1f} $\\AA$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Actually these correlations are not just on short length scales in wavelength, but on quite long scales\n",
    "\n",
    "### That shouldn't be shocking - each spectra is ~a Blackbody (1 number) + some hydrogen and helium + a little bit of ~ average stuff that is not hydrogen and helium in the Universe. \n",
    "\n",
    "### Instead of just looking at two wavelengths, we can look at all of them at once in a correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "cax = ax.matshow(np.corrcoef(whiten, rowvar=False), vmin=-1, vmax=1)\n",
    "ax.set_xticks(np.arange(0, len(wavelengths)-1, 200))\n",
    "ax.set_yticks(np.arange(0, len(wavelengths)-1, 200))\n",
    "xticks = ax.get_xticks()\n",
    "ticklabels = [f'{wavelengths[int(x)]:.0f}' for x in xticks]\n",
    "ax.set_yticklabels(ticklabels)\n",
    "ax.set_xticklabels(ticklabels)\n",
    "colorbar(cax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### This structure makes sense\n",
    "\n",
    "#### Since there is so much correlation (i.e. flux at different wavelengths are not independent of each other) we can reduce those 1000 numbers per spectra to a much smaller set if we can pick a good basis\n",
    "\n",
    "# Principal Component Analysis is one way to contruct a set of basis functions from the data itself\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The idea of principal components analysis (PCA) is to find a small number of linear combinations of the variables so as to capture most of the variation in the data as a whole.\n",
    "\n",
    "#### Principal components analysis finds a set of orthogonal standardized **linear combinations** which together explain all of the variation in the original data. \n",
    "\n",
    "#### There are as many principal components as there are features/attributes, but the way PCA is constructed the components are ordered such that the first explains most of the variance in the data, the second explains a bit less and so on\n",
    "\n",
    "#### So we can truncate the total number of components, trading off some accuracy for reducing dimensionality/complexity of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Because this is a linear transformation we can write this as \n",
    "\n",
    "\n",
    "$$T s_{1}=v_{1}$$\n",
    "$$T s_{2}=v_{2}$$\n",
    "$$T s_{n}=v_{n}$$\n",
    "\n",
    "or\n",
    "\n",
    "$$T s_{i}=v_{i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Under this transformation, there are some specific vectors that change length but do not change direction - i.e. basis vectors\n",
    "\n",
    "$$T b_{i} = \\lambda_{i} b_{i} $$\n",
    "\n",
    "or in matrix form\n",
    "\n",
    "$$ \\mathbf{T}\\cdot \\mathbf{B} = \\vec{L}\\cdot\\mathbf{B}$$\n",
    "\n",
    "$\\mathbf{B}$ is basis vectors, $\\vec{L}$ is eigenvalues\n",
    "\n",
    "We can sort the vectors/rows in $\\mathbf{B}$ such that the largest eigenvalue is first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### To find the basis vectors $\\mathbf{B}$ from the data $\\mathbf{S}$:\n",
    "\n",
    "- Subtract of the mean of each column and divide by standard deviation (optional but usually a good idea) = $\\mathbf{Z}$\n",
    "- Construct $\\mathbf{Z}^T \\cdot \\mathbf{Z}$\n",
    "- Decompose into $\\mathbf{B} \\cdot \\mathbf{L} \\cdot \\mathbf{B}^{-1}$ where $\\mathbf{L}$ is a diagonal matrix with the scalar eigenvalues on the diagonal \n",
    "- Take the eigenvalues and sort from highest to lowest and sort the columns of the $\\mathbf{B}$ accordingly\n",
    "- Figure out the projection of the data onto the new basis vectors\n",
    "\n",
    "<img src=\"eigendecomp.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# Compute PCA\n",
    "def compute_PCA(n_components=5):\n",
    "\n",
    "    nrows = 500\n",
    "    ind = np.random.randint(spectra.shape[0], size=nrows)\n",
    "    \n",
    "    # we're using only nrows to do the PCA and setting the mean as the first component\n",
    "    spec_mean = spectra[ind].mean(0)\n",
    "    pca = PCA(n_components - 1)\n",
    "    \n",
    "    pca.fit(spectra[ind])\n",
    "    # the PCA eigen\"spectra\" are:\n",
    "    pca_comp = np.vstack([spec_mean,\n",
    "                          pca.components_])\n",
    "    \n",
    "    # and the explained variance is:\n",
    "    evals = pca.explained_variance_ratio_\n",
    "\n",
    "    return pca_comp, evals, pca\n",
    "\n",
    "\n",
    "# YOU CAN CHANGE THIS \n",
    "n_components = 10\n",
    "decompositions, evals, pca = compute_PCA(n_components)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05,\n",
    "                    bottom=0.1, top=0.95, hspace=0.05)\n",
    "\n",
    "titles = 'PCA components'\n",
    "\n",
    "for j in range(n_components):\n",
    "    ax = fig.add_subplot(n_components, 2, 2*j+2)\n",
    "\n",
    "    ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(1000))\n",
    "    if j < n_components - 1:\n",
    "        ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    else:\n",
    "        ax.set_xlabel(r'wavelength ${\\rm (\\AA)}$')\n",
    "    ax.plot(wavelengths, decompositions[j], '-k', lw=1)\n",
    "\n",
    "    # plot zero line\n",
    "    xlim = [3000, 7999]\n",
    "    ax.plot(xlim, [0, 0], '-', c='gray', lw=1)\n",
    "    ax.set_xlim(xlim)\n",
    "\n",
    "    # adjust y limits\n",
    "    ylim = plt.ylim()\n",
    "    dy = 0.05 * (ylim[1] - ylim[0])    \n",
    "    ax.set_ylim(ylim[0] - dy, ylim[1] + 4 * dy)\n",
    "\n",
    "\n",
    "    ax2 = fig.add_subplot(n_components, 2, 2*j+1)\n",
    "    ax2.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax2.xaxis.set_major_locator(plt.MultipleLocator(1000))\n",
    "    if j < n_components - 1:\n",
    "        ax2.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    else:\n",
    "        ax2.set_xlabel(r'wavelength ${\\rm (\\AA)}$')\n",
    "    ax2.plot(wavelengths, spectra[j], '-k', lw=1)\n",
    "    \n",
    "    # plot zero line\n",
    "    ax2.plot(xlim, [0, 0], '-', c='gray', lw=1)\n",
    "    ax2.set_xlim(xlim)\n",
    "\n",
    "    if j == 0:\n",
    "        ax.set_title(titles, fontsize='medium')\n",
    "\n",
    "    if j == 0:\n",
    "        label = 'mean'\n",
    "    else:\n",
    "        label = 'component %i' % j\n",
    "\n",
    "    # adjust y limits\n",
    "    ylim = plt.ylim()\n",
    "    dy = 0.05 * (ylim[1] - ylim[0])    \n",
    "    ax2.set_ylim(ylim[0] - dy, ylim[1] + 4 * dy)\n",
    "\n",
    "\n",
    "    ax.text(0.02, 0.95, label, transform=ax.transAxes,\n",
    "            ha='left', va='top', bbox=dict(ec='w', fc='w'),\n",
    "            fontsize='small')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Comparisons between the eigenvectors derived from PCA and known spectral types of galaxies have shown that the eigenspectra correlate strongly with specific physical properties "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Now that you have your new basis vectors that are orthogonal, just as you calculated the variance of the data along say the $x$ axis, you can compute it along the $PC_1$ axis. \n",
    "\n",
    "#### The total variance is the sum of variances of all individual principal components.\n",
    "\n",
    "#### The fraction of variance explained by a principal component is the ratio between the variance of that principal component and the total variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot(evals, label='Variance explained by this component')\n",
    "plot(np.cumsum(evals), 'r-', label='Total Explained variance')\n",
    "xlabel('Principal Component number')\n",
    "ylabel('Explained Variance')\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(range(n_components-1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### This is useful because a small number of components is enough to represent the original data - DIMENSIONALITY REDUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pca_transformed = pca.transform(spectra)\n",
    "print('Original data:', spectra.shape)\n",
    "print('PCA transformed data:',pca_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Remember that PCA is a linear method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "scatter(pca_transformed[:,0], pca_transformed[:,2], marker='.', color='k', alpha=0.1)\n",
    "xlabel('Component 0')\n",
    "ylabel('Component 3');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index = 50\n",
    "plot(wavelengths, spectra[index],  lw=0.5, label='Original')\n",
    "plot(wavelengths, pca.inverse_transform(pca_transformed[index]), label='PCA Reconstructed', alpha=0.7)\n",
    "xlabel('Wavelength ($\\AA$)')\n",
    "ylabel('Flux')\n",
    "legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## WARNINGS: \n",
    "\n",
    "#### Given the cumulative nature of the sum of variances used in PCA, astrophysically interesting components within the spectra (e.g., sharp spectral lines or transient features for certain galaxy populations) may not be reflected in the largest PCA components. \n",
    "\n",
    "#### the assumption that a sum of linear components can efficiently reconstruct the features within the data does not always hold. \n",
    "\n",
    "#### An example of this is the variation in broad emission lines (such as those from quasars). The variation in line width is an inherently nonlinear process and can require a large number of components to fully characterize\n",
    "\n",
    "#### For broad line quasars over 30 components are required to reproduce the underlying spectra compared to the ~10 required for quiescent and star-forming galaxies. \n",
    "\n",
    "#### There are non-linear dimensionality reduction methods (Locally Linear Embedding, IsoMaps) that will do better\n",
    "\n",
    "#### How do you know if you need linear vs non-linear methods: LOOK AT YOUR DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "np.random.seed(42)\n",
    "nrows = 200\n",
    "ind = np.random.randint(spectra.shape[0], size=nrows)\n",
    "embedding = LocallyLinearEmbedding(n_components=n_components)\n",
    "embedding.fit(spectra[ind])\n",
    "lle_transformed = embedding.transform(spectra)\n",
    "scatter(lle_transformed[:,0], lle_transformed[:,3], marker='.', color='k', alpha=0.1)\n",
    "xlabel('Component 0')\n",
    "ylabel('Component 3');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### But because LLE is non-linear there is no unique way to reconstruct the data\n",
    "\n",
    "i.e. dimensionality reduction doesn't mean there is an inverse transformation - this is why PCA is still really useful compared to say t-SNE/LLE/Isomaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## OK so we've gone from 1000 dimensions to a few so now we if we want to define a cluster in N-dimensional space (which again needs ~ N^2 parameters, we've got a much easier job)\n",
    "\n",
    "\n",
    "#### So how do we find clusters or estimate density efficiently?\n",
    "\n",
    "The most basic approach to finding clusters that is $K$-means (simple, works well)\n",
    "\n",
    "$K$-means partitions points into $K$ disjoint subsets ($C_k$) with each subset containing $N_k$ points\n",
    "\n",
    "It minimizes the objective/cost/likelihood function, \n",
    "\n",
    "#### $$\\sum_{k=1}^K \\sum_{i \\in C_k} || x_i - \\mu_k ||^2$$\n",
    "\n",
    "where $\\mu_k = \\frac{1}{N_k} \\sum_{i \\in C_k} x_i$ is the mean of the points in set (hence, $k$ means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Procedure/psuedo-code for k-Means:\n",
    "\n",
    "1. define the number of clusters $K$\n",
    "2. choose the centroid, $\\mu_k$, of each of the $K$ clusters\n",
    "3. assign each point to the cluster that it is closest to\n",
    "4. update the centroid of each cluster by recomputing $\\mu_k$ according to the new assignments.\n",
    "5. goto (3) until there are no new assignments.\n",
    "\n",
    "Global optima are not guaranteed but the process never increases the sum-of-squares error.\n",
    "\n",
    "Like any algorithm with an element of stochasticity (here the initial guesses), it's a good idea to run multiple times with different starting values for the centroids of $C_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from astroML.datasets import fetch_sdss_sspp\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Get data\n",
    "data = fetch_sdss_sspp(cleaned=True)\n",
    "X = np.vstack([data['FeH'], data['alphFe']]).T.astype('float64')\n",
    "\n",
    "# truncate dataset for speed\n",
    "X = X[::5]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute a 2D histogram  of the input\n",
    "H, FeH_bins, alphFe_bins = np.histogram2d(data['FeH'], data['alphFe'], 50)\n",
    "\n",
    "# Visualize the results\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.imshow(H.T, origin='lower', interpolation='nearest', aspect='auto',\n",
    "          extent=[FeH_bins[0], FeH_bins[-1],\n",
    "                  alphFe_bins[0], alphFe_bins[-1]],\n",
    "          cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN THIS\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the KMeans clustering\n",
    "\n",
    "\n",
    "### CHANGE ME\n",
    "n_clusters = 10\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "clf = KMeans(n_clusters)\n",
    "clf.fit(scaler.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN THIS\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Visualize the results\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "# plot density again\n",
    "ax.imshow(H.T, origin='lower', interpolation='nearest', aspect='auto',\n",
    "          extent=[FeH_bins[0], FeH_bins[-1],\n",
    "                  alphFe_bins[0], alphFe_bins[-1]],\n",
    "          cmap=plt.cm.binary)\n",
    "\n",
    "# plot cluster centers\n",
    "cluster_centers = scaler.inverse_transform(clf.cluster_centers_)\n",
    "ax.scatter(cluster_centers[:, 0], cluster_centers[:, 1],\n",
    "           s=40, c='w', edgecolors='k')\n",
    "\n",
    "# plot cluster boundaries\n",
    "FeH_centers = 0.5 * (FeH_bins[1:] + FeH_bins[:-1])\n",
    "alphFe_centers = 0.5 * (alphFe_bins[1:] + alphFe_bins[:-1])\n",
    "\n",
    "Xgrid = np.meshgrid(FeH_centers, alphFe_centers)\n",
    "Xgrid = np.array(Xgrid).reshape((2, 50 * 50)).T\n",
    "\n",
    "H_new = clf.predict(scaler.fit_transform(Xgrid)).reshape((50, 50))\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    Hcp = H_new.copy()\n",
    "    flag = (Hcp == i)\n",
    "    Hcp[flag] = 1\n",
    "    Hcp[~flag] = 0\n",
    "\n",
    "    ax.contour(FeH_centers, alphFe_centers, Hcp, [-0.5, 0.5],\n",
    "               linewidths=2, colors='r')\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.3))\n",
    "ax.set_xlim(-1.101, 0.101)\n",
    "ax.set_ylim(alphFe_bins[0], alphFe_bins[-1])\n",
    "\n",
    "ax.set_xlabel(r'$\\rm [Fe/H]$')\n",
    "ax.set_ylabel(r'$\\rm [\\alpha/Fe]$')\n",
    "\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### k-Means is a special case of a Mixture Model\n",
    "\n",
    "Mixture models use the sum of functions to represent the density distributions - used in defining the density, classifications, cloning of a data set....\n",
    "\n",
    "Gaussian mixture models (GMMs) are the most common implementation of mixture models where the model consists of $M$ multivariate Gaussians with locations $\\mu_j$ and covariances $\\Sigma_j$.\n",
    "\n",
    "<img src=\"gmm.gif\">\n",
    "\n",
    "$k$-means is a special case of GMM, where we make a hard assignment of a point to a cluster rather than evaluate the probability that a point belongs to a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The log-likelihood is straightforward to define with Gaussians\n",
    "\n",
    "### We have a density field with $N$ points:\n",
    "\n",
    "$$\\rho(\\mathbf{x}) = N\\, p(\\mathbf{x}) = N\\, \\sum_{j=1}^M \\alpha_j \\mathcal{N}(\\mu_j, \\Sigma_j)$$\n",
    "\n",
    "\n",
    "### that is some linear combination of Gaussians with:\n",
    "\n",
    "$$p(\\mathbf{x}) = \\sum_j \\alpha_j \\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_{j},\\mathbf{\\Sigma}_{j})$$\n",
    "\n",
    "### and as usual each Gaussian is:\n",
    "\n",
    "$$\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_j,\\mathbf{\\Sigma}_j) = \\frac{1}{\\sqrt{(2\\pi)^D\\mbox{det}(\\mathbf{\\Sigma}_j)}} \\exp\\Big(-\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu})^T\\mathbf{\\Sigma}_j^{-1}(\\mathbf{x}-\\mathbf{\\mu})\\Big)\\, $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The challenge here is that there's way more variables than the number of points so we have a coupled set of equations \n",
    "\n",
    "#### Not just means and variances but the class memberships - probability that a point belongs to each cluster - these are latent variables associated with every point\n",
    "\n",
    "#### In the hierarchical Bayesian mold we could introduce hyperpriors to help. We're effectively doing the same thing here - fix population parameters (the means and variances) - compute class membership, then fix class membership and update population parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This is called the Expectation Maximization (EM) algorithm\n",
    "\n",
    "#### Expectation\n",
    "    - Given a set of Gaussians compute the “expected” classes of all points\n",
    "\n",
    "#### Maximization\n",
    "    - Estimate the MLE of $\\mu$, amplitude, and $\\Sigma$ given the data’s class membership\n",
    "\n",
    "As before iterate proceedure until variance does not change. \n",
    "\n",
    "#### Guaranteed to converge - but not to the correct answer (there isn't such a thing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "#Generate data\n",
    "def gendata():\n",
    "    obs = np.concatenate((1.6*np.random.randn(300, 2), 6 + 1.3*np.random.randn(300, 2), np.array([-5, 5]) + 1.3*np.random.randn(200, 2), np.array([2, 7]) + 1.1*np.random.randn(200, 2)))\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Generate GMM model and fit the data\n",
    "def gengmm(nc=4, n_iter = 2):\n",
    "    g = mixture.GaussianMixture(n_components=nc, max_iter=n_iter, init_params='kmeans')  # number of components\n",
    "    return g\n",
    "\n",
    "\n",
    "def plotGMM(g, n, plot_contours=1):\n",
    "    \n",
    "    delta = 0.025\n",
    "    x = np.arange(-10, 10, delta)\n",
    "    y = np.arange(-6, 12, delta)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    pos = np.empty(X.shape + (2,))\n",
    "    pos[:, :, 0] = X\n",
    "    pos[:, :, 1] = Y\n",
    " \n",
    "    for i in range(n):\n",
    "        plt.plot(g.means_[i][0],g.means_[i][1], '+', markersize=13, mew=3) \n",
    "        if plot_contours == 1:\n",
    "            dist = multivariate_normal(g.means_[i,:], g.covariances_[i,:])\n",
    "            Z1 = dist.pdf(pos)\n",
    "            plt.contour(X, Y, Z1, linewidths=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "obs = gendata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(1)\n",
    "\n",
    "# CHANGE ME\n",
    "ngauss = 4\n",
    "\n",
    "g = gengmm(ngauss, 10)\n",
    "g.fit(obs)\n",
    "plt.plot(obs[:, 0], obs[:, 1], '.', markersize=3)\n",
    "plotGMM(g, ngauss, 1)\n",
    "plt.title('Gaussian Mixture Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# note because of the stochasticity from the initial choices of positions from k-means\n",
    "# you'll get different result when you rerun but there's also a chance that all the iterations look the same\n",
    "\n",
    "g = gengmm(ngauss, 1)\n",
    "g.fit(obs)\n",
    "plt.plot(obs[:, 0], obs[:, 1], '.', markersize=3)\n",
    "plotGMM(g, ngauss, 1)\n",
    "plt.title('Gaussian Models (Iter = 1)')\n",
    "plt.show()\n",
    " \n",
    "g = gengmm(ngauss, 5)\n",
    "g.fit(obs)\n",
    "plt.plot(obs[:, 0], obs[:, 1], '.', markersize=3)\n",
    "plotGMM(g, ngauss, 1)\n",
    "plt.title('Gaussian Models (Iter = 5)')\n",
    "plt.show()\n",
    " \n",
    "g = gengmm(ngauss, 20)\n",
    "g.fit(obs)\n",
    "plt.plot(obs[:, 0], obs[:, 1], '.', markersize=3)\n",
    "plotGMM(g, 4, 1)\n",
    "plt.title('Gaussian Models (Iter = 20)')\n",
    "plt.show()\n",
    " \n",
    "g = gengmm(ngauss, 100)\n",
    "g.fit(obs)\n",
    "plt.plot(obs[:, 0], obs[:, 1], '.', markersize=3)\n",
    "plotGMM(g, ngauss, 1)\n",
    "plt.title('Gaussian Models (Iter = 100)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:yse]",
   "language": "python",
   "name": "conda-env-yse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
