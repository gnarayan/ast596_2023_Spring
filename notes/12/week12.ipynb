{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1024,\n",
    "        'height': 768,\n",
    "        'scroll': True,\n",
    "})\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "%pylab\n",
    "plt.rcParams['figure.figsize'] = [10, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 12, ASTR 596: Fundamentals of Data Science\n",
    "\n",
    "\n",
    "## Intro to ML, contd. - Distances in Feature Space and Dimensionality Reduction\n",
    "\n",
    "### Gautham Narayan \n",
    "##### <gsn@illinois.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap\n",
    "\n",
    "#### Supervised learning needs labelled data\n",
    "- Every algorithm will take features and labels for a training sample\n",
    "- and optimize some metric you choose\n",
    "- which results in a classifier with hyperparameters that are conditioned on the training sample\n",
    "- that can then be applied to a test sample for prediction \n",
    "\n",
    "#### Tree methods are easy to interpret and are really useful in ensembles (random forests/boosted decision trees) \n",
    "- by no means the only, or even the best choice of ML algorithm (single decision trees are almost completely useless)\n",
    "- if you are fortunate enough to have labelled data, decision tree ensembles like random forests are a good place to start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"decision_tree.png\" width=\"50%\">\n",
    "\n",
    "### Shortcomings:\n",
    "- Can't deal well with missing data\n",
    "- When dealing with predicting a categorical variable as output (i.e. class) it's hard to get $d\\mathrm{class}/d\\mathrm{parameter}$ so hard to incorporate uncertainty \n",
    "- Can be hard to construct training and testing sets that are representative - training data with labels often has different selection effects from test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## But the biggest single issue - what do we do if we *don't* have labelled data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Something like entropy/Gini purity doesn't work because you don't have labels \n",
    "- in the cartoon, you don't know which points are (+) and (-)\n",
    "\n",
    "<img src=\"entropy_twoclass.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Without labels, you have no recourse but to find structure in the data itself\n",
    "\n",
    "### Goal is still the same - partition the data into maximally homogenous and maximally distinguished subsets - except now without labels\n",
    "\n",
    "<img src=\"cluster_vs_class.png\">\n",
    "\n",
    "### This is usually a more common problem in astrophysics - you have to get lucky to start with labelled data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ALSO, just because you do have labels, doesn't mean you should believe them:\n",
    "To err is human.\n",
    "\n",
    "<img src=\"Galileo_Neptune.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Just as before, clustering requires some notion of a distance/metric\n",
    "\n",
    "- members of the cluster should be similar/close to each other\n",
    "- samples outside the cluster should be dissimilar/separated from cluster members\n",
    "\n",
    "### Optimal clustering therefore depends on how you define this measure of similarity/distance and what the purpose of the clustering is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ideally, we want clustering approaches to have \n",
    "\n",
    "- scalability (remember, we're dealing with big data i.e. high dimensional spaces with lots of samples)\n",
    "    - spoiler: Naieve clustering approaches are NP hard\n",
    "- ability to deal with different types of attributes\n",
    "- ability to discover clusters with arbitrary shapes\n",
    "\n",
    "<img src=\"HR_diagram.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- insensitivity to noise and outliers \n",
    "- insensitivity to order\n",
    "- flexibility to incorporate constraints/priors\n",
    "- be interpretable\n",
    "\n",
    "So we'll have to think about more abstract notions of **distance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"cluster_distance.png\">\n",
    "\n",
    "This map shows a spatial separation, but the feature space is 50-year mean monthly temperature, 50-year mean monthly precipitation, elevation, total plant-available water content of soil, total organic matter in soil, and total Kjeldahl soil nitrogen (i.e. the sum of ammonia + ammonium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distances - continuous variables on ${\\rm I\\!R}$\\:\n",
    "\n",
    "### The Minkowski family:\n",
    "\n",
    "## $$ D_p(i,j) =  \\left({\\sum_{k \\mathrm{\\, features}}|x_{i,k} - x_{j,k}|^p} \\right)^{1/p} $$\n",
    "\n",
    "Properties:\n",
    "- $D(i, j) >= 0$ (see the absolute value symbols)\n",
    "- $D(i, i) = 0$ (so similar things are close)\n",
    "- $D(i, j) = D(j, i)$ (so order doesn't matter)\n",
    "- $D(i, j) <= D(i, k) + D(k, j)$ (triangle inequality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"minkowski1.png\">\n",
    "Credit: Federica Bianco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"minkowski2.png\">\n",
    "Credit: Federica Bianco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### NOTE: NOT ALL CONTINUOUS VARIABLES ARE DEFINED ON ${\\rm I\\!R}$:\n",
    "<img src=\"minkowski_vs_gcd.png\">\n",
    "Credit: Federica Bianco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### OK, but some features aren't continuous at all\n",
    "\n",
    "- e.g. you either have a spectral line or you don't\n",
    "- a galaxy has some integer number of nearby companions \n",
    "- many variables affecting house prices\n",
    "\n",
    "In this case, you can use the presence or absence of data to define a metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Even categorical variables can be described as the presence or absence of data: [One hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "<img src=\"onehot.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"cat1.png\">\n",
    "Credit: Federica Bianco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"cat2.png\">\n",
    "Credit: Federica Bianco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### This is closely connected to something you've seen before\n",
    "\n",
    "<img src=\"cat3.png\">\n",
    "Credit: Federica Bianco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## OK, so we've now got a way to get distances between our samples - regardless of if features are continuous/categorical \n",
    "\n",
    "### But do those distances actually measure something useful vis-a-vis sample similarity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data can (and generally does) have covariance:\n",
    "\n",
    "Attributes are not independent - they aren't telling you different things\n",
    "\n",
    "[This can happen even with completely unrelated quantities:](https://www.tylervigen.com/spurious-correlations)\n",
    "\n",
    "<img src=\"spurious_corr.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"corr.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Aquisition\n",
    "\n",
    "From the SDSS Sky Server we've downloaded two types of photometry (aperature and petrosian), corrected for extinction, for a number of sources with redshifts. Here's the SQL for an example query, that gets us 10000 example quasars:\n",
    "\n",
    "<font color=\"blue\" size=2>\n",
    "<pre>SELECT *,dered_u - mag_u AS diff_u, dered_g - mag_g AS diff_g, dered_r - mag_r AS diff_g, dered_i - mag_i AS diff_i, dered_z - mag_z AS diff_z from\n",
    "(SELECT top 10000\n",
    "objid, ra, dec, dered_u,dered_g,dered_r,dered_i,dered_z,psfmag_u-extinction_u AS mag_u,\n",
    "psfmag_g-extinction_g AS mag_g, psfmag_r-extinction_r AS mag_r, psfmag_i-extinction_i AS mag_i,psfmag_z-extinction_z AS mag_z,z AS spec_z,dered_u - dered_g AS u_g_color, \n",
    "dered_g - dered_r AS g_r_color,dered_r - dered_i AS r_i_color,dered_i - dered_z AS i_z_color,class\n",
    "FROM SpecPhoto \n",
    "WHERE (class = 'QSO') ) as sp\n",
    " </pre>\n",
    "</font>\n",
    "\n",
    "\n",
    "Credit: Phil Marshall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In-class Exercise:\n",
    "\n",
    "#### Feature covariance can cause all manner of issues if you are trying to assess the most important features or build a model that generalizes well from the training set to the test set.\n",
    "\n",
    "#### Use the random forest regressor code from last class to train a model to predict redshifts from photometry - photo-zs. This is an active research problem because surveys like LSST will discover too many distant galaxies to get spectra of all of them (or even a large fraction of them) \n",
    "\n",
    "\n",
    "#### Look in particular at how well you generalize from the training set to the test set. This is how well we did last class for comparison\n",
    "\n",
    "<img src=\"photoz_old.png\">\n",
    "\n",
    "\n",
    "\n",
    "#### There's some data below for QSOs from Phil Marshall that you can use to construct a training and test split with `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "%pylab inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# AND THIS\n",
    "from astroML.decorators import pickle_results\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "qsos = pd.read_csv(\"qso10000.csv\",index_col=0,usecols=[\"objid\",\"dered_r\",\"spec_z\",\"u_g_color\",\\\n",
    "                                                \"g_r_color\",\"r_i_color\",\"i_z_color\",\"diff_u\",\\\n",
    "                                                \"diff_g1\",\"diff_i\",\"diff_z\"])\n",
    "\n",
    "# Clean out extreme colors and bad magnitudes:\n",
    "qsos = qsos[(qsos[\"dered_r\"] > -9999) & (qsos[\"g_r_color\"] > -10) & (qsos[\"g_r_color\"] < 10)]\n",
    "\n",
    "\n",
    "# Response variables: redshift\n",
    "qso_redshifts = qsos[\"spec_z\"]\n",
    "\n",
    "# Features or attributes: photometric measurements\n",
    "qso_features = copy.copy(qsos)\n",
    "del qso_features[\"spec_z\"]\n",
    "qso_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We'll split the training and test set up, using 3000 QSOs for training and the rest for testing\n",
    "mag = qso_features\n",
    "z = qso_redshifts\n",
    "mag_train, mag_test, z_train, z_test = train_test_split(mag, z, test_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# Compute the results\n",
    "#  This is a long computation, so we'll save the results to a pickle.\n",
    "@pickle_results('photoz_forest.pkl')\n",
    "def compute_photoz_forest(depth):\n",
    "    rms_test = np.zeros(len(depth))\n",
    "    rms_train = np.zeros(len(depth))\n",
    "    i_best = 0\n",
    "    z_fit_best = None\n",
    "\n",
    "    for i, d in enumerate(depth):\n",
    "        \n",
    "        ### YOU CAN CHANGE N_ESTIMATORS IF YOU LIKE \n",
    "        clf = RandomForestRegressor(n_estimators=100,\n",
    "                                    max_depth=d, random_state=0)\n",
    "        clf.fit(mag_train, z_train)\n",
    "\n",
    "        z_fit_train = clf.predict(mag_train)\n",
    "        z_fit = clf.predict(mag_test)\n",
    "        rms_train[i] = np.mean(np.sqrt((z_fit_train - z_train) ** 2))\n",
    "        rms_test[i] = np.mean(np.sqrt((z_fit - z_test) ** 2))\n",
    "\n",
    "        if rms_test[i] <= rms_test[i_best]:\n",
    "            i_best = i\n",
    "            z_fit_best = z_fit\n",
    "\n",
    "    return rms_test, rms_train, i_best, z_fit_best, clf\n",
    "\n",
    "depth = np.arange(1, 20)\n",
    "rms_test, rms_train, i_best, z_fit_best, clf = compute_photoz_forest(depth)\n",
    "best_depth = depth[i_best]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "fig.subplots_adjust(wspace=0.25,\n",
    "                    left=0.1, right=0.95,\n",
    "                    bottom=0.15, top=0.9)\n",
    "\n",
    "# left panel: plot cross-validation results\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(depth, rms_test, '-k', label='cross-validation')\n",
    "ax.plot(depth, rms_train, '--k', label='training set')\n",
    "ax.legend(loc=1, prop=dict(size=13))\n",
    "\n",
    "ax.set_xlabel('depth of tree')\n",
    "ax.set_ylabel('rms error')\n",
    "\n",
    "ax.set_xlim(0, 21)\n",
    "#ax.set_ylim(0.009,  0.04)\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.01))\n",
    "\n",
    "# right panel: plot best fit\n",
    "ax = fig.add_subplot(122)\n",
    "ax.scatter(z_test, z_fit_best, s=1, lw=0, c='k')\n",
    "ax.plot([-0.1, 0.4], [-0.1, 0.4], ':k')\n",
    "ax.text(0.03, 0.97, \"depth = %i\\nrms = %.3f\" % (best_depth, rms_test[i_best]),\n",
    "        ha='left', va='top', transform=ax.transAxes)\n",
    "\n",
    "ax.set_xlabel(r'$\\rm z_{true}$', fontsize=16)\n",
    "ax.set_ylabel(r'$\\rm z_{fit}$', fontsize=16)\n",
    "\n",
    "ax.set_xlim(-0.02, 0.4001)\n",
    "ax.set_ylim(-0.02, 0.4001)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### It's reasonable to ask why this is so much worse than our earlier result \n",
    "\n",
    "#### Part of why is that we were smarter about feature selection last time.\n",
    "\n",
    "#### Let's plot the features in the dataset and their importances from the random forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n_features = len(mag.columns)\n",
    "bar(range(n_features), clf.feature_importances_ )\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(range(n_features))\n",
    "ax.set_xticklabels(list(mag.columns))\n",
    "fig = plt.gcf()\n",
    "fig.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The most important features here are `diff_i` and `diff_z`\n",
    "\n",
    "### but lets look at them more closely - are they really independent of each other - do they really provide the random forest an indpendent split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Truncate the color at z=2.5 just to keep some contrast.\n",
    "norm = mpl.colors.Normalize(vmin=min(qso_redshifts.values), vmax=2.5)\n",
    "cmap = cm.jet_r\n",
    "m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "# Plot everything against everything else:\n",
    "rez = pd.plotting.scatter_matrix(qso_features, alpha=0.1, figsize=[15,15], c=m.to_rgba(qso_redshifts.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# AND THIS\n",
    "corrMatrix = qso_features.corr()\n",
    "sns.heatmap(corrMatrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### This shouldn't be shocking - after all there is just an underlying SED - of course these features are strongly correlated. \n",
    "\n",
    "#### We never told the random forest that it should expect a priori that these features are not indpendent, so multiple trees in the forest are picking some combination of the same features (thus increasing their importance) without actually learning anything useful.\n",
    "\n",
    "#### The message here is that you can train a model on features, but you should first consider if those features are all useful or if the model will overfit the same few features. \n",
    "\n",
    "#### Here we have a number of attributes/features/dimensions in our data that are tightly related - so we should try to reduce those correlations with *dimensionality reduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "Fitting and overfitting get worse with ''curse of dimensionality'' Bellman 1961\n",
    "\n",
    "Think about a hypersphere. Its volume is  given by\n",
    "\n",
    "\\begin{equation}\n",
    "  V_D(r) = \\frac{2r^D\\pi^{D/2}}{D\\  \\Gamma(D/2)},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\Gamma(z)$ is the complete gamma function, $D$ is the dimension, and $r$ the radius of the sphere.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If you populated a hypercube of size $2r$ how much data would be enclosed by the hypersphere\n",
    "- as $D$ increases the fractional volume enclosed by the hypersphere goes to 0! \n",
    "\n",
    "For example: the SDSS comprises a sample of 357 million sources. \n",
    "- each source has 448 measured attributes\n",
    "- selecting just 30 (e.g., magnitude, size..) and normalizing the data range $-1$ to $1$\n",
    "\n",
    "probability of having one of the 357 million sources reside within a unit hypersphere 1 in 1.4$\\times 10^5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy.special as sp\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def unitVolume(dimension, radius=1.):\n",
    "    return 2*(radius**dimension *np.pi**(dimension/2.))/(dimension*sp.gamma(dimension/2.))\n",
    "\n",
    "dim = np.linspace(1,100)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(dim,unitVolume(dim)/2.**dim)\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('$Dimension$')\n",
    "ax.set_ylabel('$Volume$')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "D=30;unitVolume(D)/(2.**D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What can we do to reduce the dimensions\n",
    "\n",
    "**Principal component analysis**\n",
    "\n",
    "The first refuge of a scoundrel...\n",
    "\n",
    "<img width=\"500\" src=\"pca-scatter.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Points are  correlated along a particular direction which doesn't align with the initial choice of axes. \n",
    "- we should rotate our axes to align with this correlation. \n",
    "- rotation preserves the relative ordering of data\n",
    "\n",
    "Choose  rotation to maximize the ability to discriminate between the data points\n",
    "*   first axis, or <u>principal component</u>, is direction of maximal variance\n",
    "*   second principal component is orthogonal to the first component and maximizes the residual variance\n",
    "*   ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of principal component analyses\n",
    "\n",
    "Set of data $X$: $N$ observations by $K$ measurements\n",
    "\n",
    "Center data by subtracting the mean \n",
    "\n",
    "The covariance is\n",
    "\n",
    ">$ \n",
    "C_X=\\frac{1}{N-1}X^TX,\n",
    "$\n",
    "\n",
    "$N-1$ as the sample covariance matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a projection, $R$,  aligned with the directions of maximal variance ($Y= X R$) with covariance \n",
    "\n",
    ">$\n",
    "C_{Y} = R^T X^T X R = R^T C_X R\n",
    "$\n",
    "\n",
    "Derive  principal component by maximizing its variance (using Lagrange multipliers and constraint)\n",
    "\n",
    "> $\n",
    "\\phi(r_1,\\lambda_1) = r_1^TC_X r_1 - \\lambda_1(r_1^Tr_1-1).\n",
    "$\n",
    "\n",
    "derivative of $\\phi(r_1,\\lambda)$ with respect to $r_1$ set to 0\n",
    "\n",
    "> $\n",
    "C_Xr_1 - \\lambda_1 r_1 = 0.\n",
    "$\n",
    "\n",
    "$\\lambda_1$ is the root of the equation $\\det(C_X -\n",
    "\\lambda_1 {\\bf I})=0$ and the largest eigenvalue\n",
    "\n",
    ">$\n",
    "\\lambda_1 =  r_1^T C_X r_1\n",
    "$\n",
    "\n",
    "Other  principal components  derived by\n",
    "applying additional constraint that components are uncorrelated (e.g., $r^T_2 C_X r_1 = 0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagrangian mulitpliers\n",
    "<img width=\"500\" src=\"LagrangeMultipliers2D.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computation of principal components##\n",
    "\n",
    "Common approach is eigenvalue decomposition of the covariance or correlation matrix,\n",
    "or singular value decomposition (SVD) of the data matrix\n",
    "\n",
    "** SVD given by**\n",
    "\n",
    ">$\n",
    "U \\Sigma V^T = \\frac{1}{\\sqrt{N - 1}} X,\n",
    "$\n",
    "\n",
    "columns of $U$ are  _left-singular vectors_\n",
    "\n",
    "columns of $V$ are the _right-singular vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The columns of $U$ and $V$ form orthonormal bases ($U^TU = V^TV = I$)\n",
    "\n",
    "Covariance matrix is\n",
    "\n",
    "> $\n",
    "\\begin{eqnarray}\n",
    "  C_X &=& \\left[\\frac{1}{\\sqrt{N - 1}}X\\right]^T \\left[\\frac{1}{\\sqrt{N - 1}}X\\right]\\nonumber\\\\\n",
    "      &=& V \\Sigma U^T U \\Sigma V^T\\nonumber\\\\\n",
    "      &=& V \\Sigma^2 V^T.\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "right singular vectors $V$ are the principal components. We can calculate principal components from the SVD of $X$ - we dont need $C_X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img width=\"500\" src=\"fig_svd_visual_1.png\">\n",
    "Singular value decomposition (SVD) can factorize an N x K matrix into $U \\Sigma V^T$. There are different conventions for computing the SVD in the literature, and this figure illustrates the convention used in this text. The matrix of singular values $\\Sigma$ is always a square matrix of size [R x R] where R = min(N, K). The shape of the resulting U and V matrices depends on whether N or K is larger. The columns of the matrix U are called the left-singular vectors, and the columns of the matrix V are called the right-singular vectors. The columns are orthonormal bases, and satisfy $U^T U = V^T V = I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preparing data for PCA\n",
    "\n",
    "- Center data by subtracting the mean of each dimension\n",
    "- For heterogeneous data (e.g., galaxy shape and flux) divide by  variance (whitening). **why?**\n",
    "- For spectra or images normalize each row so integrated flux of each object is one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA \n",
    "from astroML.datasets import sdss_corrected_spectra\n",
    "from astroML.decorators import pickle_results\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Download data\n",
    "data = sdss_corrected_spectra.fetch_sdss_corrected_spectra()\n",
    "spectra = sdss_corrected_spectra.reconstruct_spectra(data)\n",
    "wavelengths = sdss_corrected_spectra.compute_wavelengths(data)\n",
    "spectra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# Compute PCA\n",
    "def compute_PCA(n_components=5):\n",
    "#    np.random.seed(500)\n",
    "    nrows = 500\n",
    "    ind = np.random.randint(spectra.shape[0], size=nrows)\n",
    "    \n",
    "    spec_mean = spectra[ind].mean(0)\n",
    "#    spec_mean = spectra[:50].mean(0)\n",
    "\n",
    "    # PCA: use randomized PCA for speed\n",
    "    pca = PCA(n_components - 1)\n",
    "    pca.fit(spectra[ind])\n",
    "    pca_comp = np.vstack([spec_mean,\n",
    "                          pca.components_])\n",
    "    evals = pca.explained_variance_ratio_\n",
    "\n",
    "    return pca_comp, evals\n",
    "\n",
    "n_components = 5\n",
    "decompositions, evals = compute_PCA(n_components)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05,\n",
    "                    bottom=0.1, top=0.95, hspace=0.05)\n",
    "\n",
    "titles = 'PCA components'\n",
    "\n",
    "for j in range(n_components):\n",
    "    ax = fig.add_subplot(n_components, 2, 2*j+2)\n",
    "\n",
    "    ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(1000))\n",
    "    if j < n_components - 1:\n",
    "        ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    else:\n",
    "        ax.set_xlabel(r'wavelength ${\\rm (\\AA)}$')\n",
    "    ax.plot(wavelengths, decompositions[j], '-k', lw=1)\n",
    "\n",
    "    # plot zero line\n",
    "    xlim = [3000, 7999]\n",
    "    ax.plot(xlim, [0, 0], '-', c='gray', lw=1)\n",
    "    ax.set_xlim(xlim)\n",
    "\n",
    "    # adjust y limits\n",
    "    ylim = plt.ylim()\n",
    "    dy = 0.05 * (ylim[1] - ylim[0])    \n",
    "    ax.set_ylim(ylim[0] - dy, ylim[1] + 4 * dy)\n",
    "\n",
    "\n",
    "    ax2 = fig.add_subplot(n_components, 2, 2*j+1)\n",
    "    ax2.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax2.xaxis.set_major_locator(plt.MultipleLocator(1000))\n",
    "    if j < n_components - 1:\n",
    "        ax2.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    else:\n",
    "        ax2.set_xlabel(r'wavelength ${\\rm (\\AA)}$')\n",
    "    ax2.plot(wavelengths, spectra[j], '-k', lw=1)\n",
    "    \n",
    "    # plot zero line\n",
    "    ax2.plot(xlim, [0, 0], '-', c='gray', lw=1)\n",
    "    ax2.set_xlim(xlim)\n",
    "\n",
    "    if j == 0:\n",
    "        ax.set_title(titles, fontsize='medium')\n",
    "\n",
    "    if j == 0:\n",
    "        label = 'mean'\n",
    "    else:\n",
    "        label = 'component %i' % j\n",
    "\n",
    "    # adjust y limits\n",
    "    ylim = plt.ylim()\n",
    "    dy = 0.05 * (ylim[1] - ylim[0])    \n",
    "    ax2.set_ylim(ylim[0] - dy, ylim[1] + 4 * dy)\n",
    "\n",
    "\n",
    "    ax.text(0.02, 0.95, label, transform=ax.transAxes,\n",
    "            ha='left', va='top', bbox=dict(ec='w', fc='w'),\n",
    "            fontsize='small')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the PCA\n",
    "\n",
    "Reconstruction of spectrum, ${x}(k)$, from the\n",
    "eigenvectors, ${e}_i(k)$ \n",
    "\n",
    ">$ \\begin{equation}\n",
    "  {x}_i(k) = {\\mu}(k) + \\sum_j^R \\theta_{ij} {e}_j(k),\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Truncating this expansion (i.e., $r<R$)\n",
    "\n",
    ">$\\begin{equation}\n",
    "{x}_i(k) = {\\mu}(k) + \\sum_i^{r<R} \\theta_i {e}_i(k),\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "- eigenvectors ordered by their associated eigenvalues \n",
    "- eigenvalues reflect variance  within each eigenvector (sum of the eigenvalues is total variance of the system).\n",
    "- project a each spectrum onto these first few eigenspectra is a compression of the data \n",
    "\n",
    "This is the sense in which PCA gives for dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "from astroML.datasets import sdss_corrected_spectra\n",
    "from astroML.decorators import pickle_results\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Download data\n",
    "data = sdss_corrected_spectra.fetch_sdss_corrected_spectra()\n",
    "spectra = sdss_corrected_spectra.reconstruct_spectra(data)\n",
    "wavelengths = sdss_corrected_spectra.compute_wavelengths(data)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute PCA components\n",
    "\n",
    "# Eigenvalues can be computed using PCA as in the commented code below:\n",
    "\n",
    "#from sklearn.decomposition import PCA\n",
    "#pca = PCA()\n",
    "#pca.fit(spectra)\n",
    "#evals = pca.explained_variance_ratio_\n",
    "#evals_cs = evals.cumsum()\n",
    "\n",
    "#  because the spectra have been reconstructed from masked values, this\n",
    "#  is not exactly correct in this case: we'll use the values computed\n",
    "#  in the file compute_sdss_pca.py\n",
    "evals = data['evals'] ** 2\n",
    "evals_cs = evals.cumsum()\n",
    "evals_cs /= evals_cs[-1]\n",
    "evecs = data['evecs']\n",
    "spec_mean = spectra.mean(0)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Find the coefficients of a particular spectrum\n",
    "spec = spectra[1]\n",
    "coeff = np.dot(evecs, spec - spec_mean)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the sequence of reconstructions\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "fig.subplots_adjust(hspace=0)\n",
    "\n",
    "for i, n in enumerate([0, 4, 8, 20]):\n",
    "    ax = fig.add_subplot(411 + i)\n",
    "    ax.plot(wavelengths, spec, '-', c='gray')\n",
    "    ax.plot(wavelengths, spec_mean + np.dot(coeff[:n], evecs[:n]), '-k')\n",
    "\n",
    "    if i < 3:\n",
    "        ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "    ax.set_ylim(-2, 21)\n",
    "    ax.set_ylabel('flux')\n",
    "\n",
    "    if n == 0:\n",
    "        text = \"mean\"\n",
    "    elif n == 1:\n",
    "        text = \"mean + 1 component\\n\"\n",
    "        text += r\"$(\\sigma^2_{tot} = %.2f)$\" % evals_cs[n - 1]\n",
    "    else:\n",
    "        text = \"mean + %i components\\n\" % n\n",
    "        text += r\"$(\\sigma^2_{tot} = %.2f)$\" % evals_cs[n - 1]\n",
    "\n",
    "    ax.text(0.01, 0.95, text, ha='left', va='top', transform=ax.transAxes)\n",
    "\n",
    "fig.axes[-1].set_xlabel(r'${\\rm wavelength\\ (\\AA)}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- k-means\n",
    "- GMMs\n",
    "- EM\n",
    "- hierarchical clustering\n",
    "- DBScan"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:yse]",
   "language": "python",
   "name": "conda-env-yse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
