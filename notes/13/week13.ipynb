{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1024,\n",
    "        'height': 768,\n",
    "        'scroll': True,\n",
    "})\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%pylab\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 13, ASTR 596: Fundamentals of Data Science\n",
    "\n",
    "\n",
    "## More clustering, Density Estimation\n",
    "\n",
    "### Gautham Narayan \n",
    "##### <gsn@illinois.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "- ML is broadly divided into supervised and unsupervised methods\n",
    "- Supervised = you've got labels for the data you are trying to predict, unsupervised = you don't \n",
    "\n",
    "- Unsupervised methods then are about finding structure in the data itself\n",
    "    - clustering is trying to find group similar samples in your dataset\n",
    "    - you need some measure of distance to define similarity - a **metric**\n",
    "    \n",
    "- To effectively use clustering algorithms, you have to limit the number of dimensions or your metric isn't useful\n",
    "    - viewed as an optimization problem, clustering (like all problems) gets hard to solve as the number of dimensions gets large\n",
    "    - viewed as a density problem, as the number of dimensions gets large, the fractional volume within some hypersphere becomes 0 \n",
    "        - i.e. your cluster isn't likely to enclose many points, because as the dimensionality grows, samples themselves are spread further apart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- To tackle this we use **dimensionality reduction** \n",
    "    - **Principal Component Analysis** - express your data as a sum of orthogonal eigenvectors constructed such that (hopefully) a few eigenvectors explain most of the variance of the data\n",
    "        - linear methods don't necessarily work well for non-linear structure in the data but are invertible\n",
    "    - ```astronomers will often try to derive physical insight from PCA eigenspectra or eigentimeseries, but this is not advisable as there is no physical reason for the data to be linearly and orthogonally separable``` - snark from Adam Miller (Northwestern)\n",
    "    - there are non-linear methods - manifold learning (i.e. IsoMaps)/LLE/autoencoders and variants  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Once you have some reasonably low-dimensional representation of your data:    \n",
    "        - k-means: splits dataset into $k \\sim$ approximately equally sized clusters by finding the mean cluster centers using MSE as a metric\n",
    "        - GMMs: splits dataset into a sum of $k$ Gaussians (you don't have to use Gaussians but the likelihood/metric is easy)\n",
    "- in both cases, for each sample in the dataset, we're creating a latent variable which encodes cluster membership \n",
    "- clustering can be viewed as a hierarchical model where the cluster parameters specify the population\n",
    "\n",
    "\n",
    "- **Expectation Maximiation** - hold cluster membership fixed, update population parameters, then hold population parameters fixed and update cluster membership "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clustering isn't just finding groups of things that are similar\n",
    "\n",
    "What you are doing is specifying a **parametric** model for the **density of the data** \n",
    "\n",
    "This is usually a better use for clustering algorithms - the cluster centers and sizes may not be useful/physically meaningful, but the sum of all the components is a representation of the underlying density field reconstructed from sparse samples:\n",
    "\n",
    "Our Universe is only isotropic and homogenous on the largest scales - when you smooth over filaments in the cosmic web:\n",
    "\n",
    "<img src=\"galaxy_map.gif\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Demo: The use of GMMs as a tool for density estimation\n",
    "\n",
    "- We'll use the Sloan Dataset to estimate density with k=50 and k=200 GMMs\n",
    "- Look at how clusters in the data (i.e. look at your data and identify these by eye) map to centers of Gaussians\n",
    "- And look at how the GMM traces the overall density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from astroML.datasets import fetch_great_wall\n",
    "from astroML.utils.decorators import pickle_results\n",
    "from sklearn.neighbors import KernelDensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# load great wall data\n",
    "X = fetch_great_wall()\n",
    "\n",
    "\n",
    "def compute_GMM(n_clusters, max_iter=1000, tol=3, covariance_type='full'):\n",
    "    clf = GaussianMixture(n_clusters, covariance_type=covariance_type,\n",
    "                          max_iter=max_iter, tol=tol, random_state=0)\n",
    "    clf.fit(X)\n",
    "    return clf\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute a grid on which to evaluate the result\n",
    "Nx = 100\n",
    "Ny = 250\n",
    "xmin, xmax = (-375, -175)\n",
    "ymin, ymax = (-300, 200)\n",
    "\n",
    "Xgrid = np.vstack(map(np.ravel, np.meshgrid(np.linspace(xmin, xmax, Nx),\n",
    "                                            np.linspace(ymin, ymax, Ny)))).T\n",
    "\n",
    "clf1 = compute_GMM(n_clusters=50)\n",
    "clf2 = compute_GMM(n_clusters=200)\n",
    "log_dens1 = clf1.score_samples(Xgrid).reshape(Ny, Nx)\n",
    "log_dens2 = clf2.score_samples(Xgrid).reshape(Ny, Nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig = plt.figure(figsize=(10, 12))\n",
    "fig.subplots_adjust(hspace=0, left=0.08, right=0.95, bottom=0.13, top=0.9)\n",
    "\n",
    "ax = fig.add_subplot(311, aspect='equal')\n",
    "ax.scatter(X[:, 1], X[:, 0], s=1, lw=0, c='k')\n",
    "\n",
    "ax.set_xlim(ymin, ymax)\n",
    "ax.set_ylim(xmin, xmax)\n",
    "\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "plt.ylabel(r'$x\\ {\\rm (Mpc)}$')\n",
    "\n",
    "ax = fig.add_subplot(312, aspect='equal')\n",
    "ax.imshow(np.exp(log_dens1.T), origin='lower', cmap=plt.cm.binary,\n",
    "          extent=[ymin, ymax, xmin, xmax])\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax.set_ylabel(r'$x\\ {\\rm (Mpc)}$')\n",
    "\n",
    "ax = fig.add_subplot(313, aspect='equal')\n",
    "ax.imshow(np.exp(log_dens2.T), origin='lower', cmap=plt.cm.binary,\n",
    "          extent=[ymin, ymax, xmin, xmax])\n",
    "ax.set_xlabel(r'$y\\ {\\rm (Mpc)}$')\n",
    "ax.set_ylabel(r'$x\\ {\\rm (Mpc)}$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## KDEs\n",
    "\n",
    "Mixture models are really useful for estimating density (you can still use them for clustering, but you've got to work to interpret what the clusters are and how many you need)\n",
    "\n",
    "While a mixture model is parametric, you can take the extreme limit of making the number of clusters = the number of datapoints, and instead of treating the mean as a parameter, use the value of the data itself as the mean. \n",
    "\n",
    "To simplify things further, we'll assume all the components of the mixture have the same scale/$\\sigma$ or more generally a **bandwidth** - which you specify \n",
    "\n",
    "This is called **kernel density estimation** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A common kernel is the (YOU GUESSED IT) Gaussian kernel that we just used above:\n",
    "\n",
    "$$K(u) = \\frac{1}{(2\\pi)^{D/2}}\\exp^{-u^2/2}$$\n",
    "\n",
    "\n",
    "Once a kernel is chosen the kernel density estimate at a given point, $x$, is given by \n",
    "\n",
    "$$ \\rho(x) = \\frac{1}{Nh^D}\\sum_{i=1}^N K\\left(\\frac{d(x,x_i)}{h}\\right),$$ where $\\rho(x)$ is an estimator of our distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where does this come from? \n",
    "\n",
    "If you wanted to know the density of points you could compute \n",
    "\n",
    "$$\\frac{\\sum_1^N\\delta (x-x_i)}{V}$$ \n",
    "\n",
    "where $\\delta (x-x_i)$ is the Delta function, $V$ is the volume, and $N$ is the number of points. \n",
    "\n",
    "In $D$-dimensional space a volume element is just $h^D$. Then instead of representing our observation as a delta function, we represent it by our kernel function. To normalize for the number of points, divide by $N$.\n",
    "\n",
    "The argument of $K$ is just some measure of the distance between $x$ and each $x_i$. \n",
    "\n",
    "Normally $d(x,x_i) = (x-x_i)$. For the gaussian kernel that makes $h=\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# CHANGE PARAMETERS FOR KERNEL, METRIC AND BANDWIDTH AS YOU LIKE\n",
    "kde = KernelDensity(bandwidth=5, kernel='gaussian', metric='euclidean')\n",
    "kde.fit(X)\n",
    "log_dens3 = kde.score_samples(Xgrid).reshape(Ny, Nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "fig.subplots_adjust(hspace=0, left=0.08, right=0.95, bottom=0.13, top=0.9)\n",
    "\n",
    "ax = fig.add_subplot(211, aspect='equal')\n",
    "ax.scatter(X[:, 1], X[:, 0], s=1, lw=0, c='k')\n",
    "\n",
    "ax.set_xlim(ymin, ymax)\n",
    "ax.set_ylim(xmin, xmax)\n",
    "\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "plt.ylabel(r'$x\\ {\\rm (Mpc)}$')\n",
    "\n",
    "ax = fig.add_subplot(212, aspect='equal')\n",
    "ax.imshow(np.exp(log_dens3.T), origin='lower', cmap=plt.cm.binary,\n",
    "          extent=[ymin, ymax, xmin, xmax])\n",
    "\n",
    "\n",
    "ax.set_xlabel(r'$y\\ {\\rm (Mpc)}$')\n",
    "ax.set_ylabel(r'$x\\ {\\rm (Mpc)}$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can view this as a **convolution** operation - just as with the 2D PSF and single source, you are convolving each point with a kernel (shape, metric and bandwidth is what you specify) \n",
    "\n",
    "You then sum up all the kernels and get a model for the underlying density field - just as with a GMM. \n",
    "\n",
    "The distinction is that with GMMs you were specifying a parametric model for the **population**\n",
    "\n",
    "Here, *just as with the kernel in Gaussian Processes* you are specifying how points are correlated with each other in the density field.\n",
    "\n",
    "And just as with Gaussian processes, you've got several potential choices for kernels. Common ones are below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute Kernels.\n",
    "u = np.linspace(-5, 5, 10000)\n",
    "du = u[1] - u[0]\n",
    "\n",
    "gauss = (1. / np.sqrt(2 * np.pi)) * np.exp(-0.5 * u ** 2)\n",
    "\n",
    "exp = 0.5 * np.exp(-abs(u))\n",
    "\n",
    "tophat = 0.5 * np.ones_like(u)\n",
    "tophat[abs(u) > 1] = 0 # Range of the tophat kernel\n",
    "\n",
    "ep = (3/4.)*(1-u**2)  \n",
    "ep[abs(u)>1]=0 # Set the range of the Epanechnikov kernel\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the kernels\n",
    "fig = plt.figure(figsize=(5, 3.75))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(u, gauss, '-', c='C0', lw=3, label='Gaussian')\n",
    "ax.plot(u, exp, '-', c='C1', lw=2, label='Exponential')\n",
    "ax.plot(u, tophat, '-', c='C2', lw=1, label='Top-hat')\n",
    "ax.plot(u,ep,'--',c='C3',label='Epanechnikov')  \n",
    "ax.legend(loc=1)\n",
    "\n",
    "ax.set_xlabel('$u$')\n",
    "ax.set_ylabel('$K(u)$')\n",
    "\n",
    "ax.set_xlim(-5, 5)\n",
    "ax.set_ylim(0, 0.8001)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nonparametric density estimation is useful when we know nothing about the underlying population distribution of the data since we don't have to specify a model (again, just as in Gaussian processes).\n",
    "\n",
    "This flexibility allows us to capture the shape of the distribution well, at the expense of more difficulty interpreting the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why should I care about density estimates anyway and what has this got to do with machine learning???\n",
    "\n",
    "You can draw new samples from density fields - i.e. you can simulate new data given a density estimate constructed on other data - i.e. you can use if for **data augmentation**\n",
    "\n",
    "Also note that PDFs are just density fields - you can use KDEs to get smooth representations of the underlying PDF (you've already done this multiple times this semester) - and you know that sampling PDFs is sorta useful..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## But why go through all this - can't I just histogram the data to get an estimate of the density???\n",
    "\n",
    "Sure, histograms are useful, but they're also risky - look at what happens when we adjust bins below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Draw the random data\n",
    "np.random.seed(1)\n",
    "x = np.concatenate([np.random.normal(-0.5, 0.3, size=14), np.random.normal(1, 0.3, size=7)])\n",
    "\n",
    "\n",
    "# First figure: silly histogram binning\n",
    "fig1 = plt.figure(figsize=(8, 4))\n",
    "fig1.subplots_adjust(left=0.12, right=0.95, wspace=0.05, bottom=0.15, top=0.9, hspace=0.05)\n",
    "\n",
    "\n",
    "XLIM = (-2, 2.9)\n",
    "YLIM = (-0.09, 1.1)\n",
    "\n",
    "ax = fig1.add_subplot(121)\n",
    "bins = np.linspace(-1.8, 2.7, 13)\n",
    "ax.hist(x, bins=bins, density=True, histtype='stepfilled', fc='k', alpha=0.3)\n",
    "ax.plot(XLIM, [0, 0], '-k', lw=1)\n",
    "ax.plot(x, 0 * x - 0.05, '+k')\n",
    "ax.set_xlim(XLIM)\n",
    "ax.set_ylim(YLIM)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$p(x)$')\n",
    "\n",
    "#Shift bin centers by 0.25\n",
    "ax = fig1.add_subplot(122)\n",
    "ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax.hist(x, bins=bins + 0.25, density=True, histtype='stepfilled', fc='k', alpha=0.3)\n",
    "ax.plot(XLIM, [0, 0], '-k', lw=1)\n",
    "ax.plot(x, 0 * x - 0.05, '+k')\n",
    "ax.set_xlim(XLIM)\n",
    "ax.set_ylim(YLIM)\n",
    "ax.set_xlabel('$x$')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The underlying distribution in both panels is the same, that is the data points that make up the histogram are the same. All we have done is shifted the locations of the bins by 0.25.\n",
    "\n",
    "KDEs take away one of the issues - where the bins are - but you still are left with how to choose the bandwidth for the KDE (analogous to chosing the bin-width for the histogram) and you have a new issue - how to choose the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "fig2 = plt.figure(figsize=(8, 8))\n",
    "fig2.subplots_adjust(left=0.12, right=0.95, wspace=0.05, bottom=0.0, top=1.0, hspace=0.05)\n",
    "\n",
    "ax = fig2.add_subplot(311)\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "binwidth = bins[1] - bins[0]\n",
    "x_plot = np.linspace(-4, 4, 1000)\n",
    "y_plot = binwidth * stats.norm.pdf(x_plot, x[:, None], 0.1)\n",
    "y_plot /= (binwidth * len(x))\n",
    "ax.fill(x_plot, y_plot.sum(0), ec='k', lw=1, fc='k', alpha=0.3)\n",
    "ax.plot(x_plot, y_plot.T, '-k', lw=1)\n",
    "ax.plot(x, 0 * x - 0.05, '+k')\n",
    "ax.set_xlim(XLIM)\n",
    "ax.set_ylim(YLIM)\n",
    "\n",
    "ax = fig2.add_subplot(312)\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "binwidth = bins[1] - bins[0]\n",
    "x_plot = np.linspace(-4, 4, 1000)\n",
    "y_plot = binwidth * stats.norm.pdf(x_plot, x[:, None], 0.7)\n",
    "y_plot /= (binwidth * len(x))\n",
    "ax.fill(x_plot, y_plot.sum(0), ec='k', lw=1, fc='k', alpha=0.3)\n",
    "ax.plot(x_plot, 4 * y_plot.T, '-k', lw=1)\n",
    "ax.plot(x, 0 * x - 0.05, '+k')\n",
    "ax.set_xlim(XLIM)\n",
    "ax.set_ylim(YLIM)\n",
    "ax.set_ylabel('$p(x)$')\n",
    "ax.set_xlabel('$x$')\n",
    "\n",
    "ax = fig2.add_subplot(313)\n",
    "ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "binwidth = bins[1] - bins[0]\n",
    "x_plot = np.linspace(-4, 4, 1000)\n",
    "y_plot = binwidth * stats.norm.pdf(x_plot, x[:, None], 0.2)\n",
    "y_plot /= (binwidth * len(x))\n",
    "ax.fill(x_plot, y_plot.sum(0), ec='k', lw=1, fc='k', alpha=0.3)\n",
    "ax.plot(x_plot, y_plot.T, '-k', lw=1)\n",
    "ax.plot(x, 0 * x - 0.05, '+k')\n",
    "ax.set_xlim(XLIM)\n",
    "ax.set_ylim(YLIM)\n",
    "ax.set_xlabel('$x$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This looks better, but gives us a \"Goldilocks\" problem. The first plot uses a kernel that is too narrow. The second is too wide. The third is \"just right\".\n",
    "\n",
    "#### The crucial part of KDE is to determine the optimal value for the width of the kernel.\n",
    "\n",
    "We can empirically determine the optimal bandwidth through **cross validation** \n",
    "\n",
    "## Cross-Validation\n",
    "\n",
    "Cross validation is related to the construction of training and test sets that we talked about last time (see week 11). Except now we're using the same idea in the context of unsupervised learning (and you can use it for for all of the first half of the semester, not just ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are a number of different ways to do **cross-validation**. For example, you could randomly sample to decide which data goes into the training or test sets: \n",
    "\n",
    "<img src=\"http://i.stack.imgur.com/4Lrff.png\" width=\"60%\">\n",
    "\n",
    "Where we aren't just doing this once, but rather many times so that each data point is treated both as a training point and as a test point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We could do this in a more ordered way (e.g., to make sure that each point gets sampled as training/test the same number of times) and do a $K$-fold cross validation.  Here $K$ is the number of \"experiments\" that need to be done so that each data point appears in a test sample once.\n",
    "\n",
    "<img src=\"http://i.stack.imgur.com/fhMza.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can take that to the extreme by having $K\\equiv N$, so that in each experiment we leave out just one object.  This is called \"Leave-One-Out\" cross validation:\n",
    "\n",
    "<img src=\"http://images.slideplayer.com/16/4977882/slides/slide_35.jpg\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can implement this in `sklearn` with `GridSearchCV` and replot our histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Complete and Execute this cell to determine the bandwidth\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "bwrange = np.linspace(0.1, 1.0, 30) # Test 30 bandwidths from 0.1 to 1.0\n",
    "K = 5 # 5-fold cross validation\n",
    "grid = GridSearchCV(KernelDensity(), {'bandwidth': bwrange}, cv=K) \n",
    "grid.fit(x[:, None]) #Fit the histogram data that we started the lecture with.\n",
    "h_opt = grid.best_params_['bandwidth']\n",
    "print(h_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell to see the new \"histogram\"\n",
    "fig2 = plt.figure(figsize=(5, 5))\n",
    "ax = fig2.add_subplot(111)\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "binwidth = bins[1] - bins[0]\n",
    "x_plot = np.linspace(-4, 4, 1000)\n",
    "y_plot = binwidth * stats.norm.pdf(x_plot, x[:, None], h_opt)\n",
    "y_plot /= (binwidth * len(x))\n",
    "ax.fill(x_plot, y_plot.sum(0), ec='k', lw=1, fc='k', alpha=0.3)\n",
    "ax.plot(x_plot, y_plot.T, '-k', lw=1)\n",
    "ax.plot(x, 0 * x - 0.05, '+k')\n",
    "ax.set_xlim(XLIM)\n",
    "ax.set_ylim(YLIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "OK, so we've gone from $k$ clusters to replacing each data point with it's own kernel representing points in the underlying latent density field that are clustered with it. \n",
    "\n",
    "This was good for density estimation, but sometimes we do actually want clustering - grouping similar objects together. \n",
    "\n",
    "So it's reasonable to ask if this non-parametric density estimation technique can be converted back to clustering like we had with $k$-means and GMMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "In Hierarchical Clustering, we don't specify the number of clusters ahead of time, we start with $N$ clusters representing each data point. \n",
    "\n",
    "Then the most similar clusters are joined together, the process repeating until some threshhold is reached.\n",
    "\n",
    "\n",
    "<img src=\"Hierarchical-clustering-2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Clusters are merged at each step according to which are \"nearest\" to each other---where the definition of nearest needs to be specified - i.e. you still need a **metric** and your output is going to depend on how you make this choice.\n",
    "\n",
    "After selecting a distance metric, it is necessary to determine from **where** distance is computed. \n",
    "    - Can be computed between the two most similar parts of a cluster (single-linkage), \n",
    "    - the two least similar bits of a cluster (complete-linkage)\n",
    "    - the center of the clusters (mean or average-linkage)\n",
    "    ....\n",
    "\n",
    "As with distance metrics, the choice of linkage criteria should be made based on domain knowledge. \n",
    "\n",
    "**What causes variation between groups**\n",
    "\n",
    "The default is just reducing the sum of squared distances of each observation from the average observation in a cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some threshhold needs to be specified to tell the process where to stop clustering - i.e. the usual bias variance tradeoff - you can grow your tree upwards to where all points are clustered, but this may not be useful.\n",
    "\n",
    "The typical choice for distance (Euclidean) and linkage criterion (average) results in what is called a **\"minimum spanning tree\"** \n",
    "\n",
    "You can of course visualize this minimum spanning tree and the result is a **dendrogram** just as you saw with decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from astroML.clustering import HierarchicalClustering, get_graph_segments\n",
    "\n",
    "X = fetch_great_wall()\n",
    "\n",
    "xmin, xmax = (-375, -175)\n",
    "ymin, ymax = (-300, 200)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the MST clustering model\n",
    "n_neighbors = 10\n",
    "edge_cutoff = 0.9\n",
    "cluster_cutoff = 10\n",
    "model = HierarchicalClustering(n_neighbors=10,\n",
    "                               edge_cutoff=edge_cutoff,\n",
    "                               min_cluster_size=cluster_cutoff)\n",
    "model.fit(X)\n",
    "print(\" scale: %2g Mpc\" % np.percentile(model.full_tree_.data,\n",
    "                                        100 * edge_cutoff))\n",
    "\n",
    "n_components = model.n_components_\n",
    "labels = model.labels_\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Get the x, y coordinates of the beginning and end of each line segment\n",
    "T_x, T_y = get_graph_segments(model.X_train_,\n",
    "                              model.full_tree_)\n",
    "T_trunc_x, T_trunc_y = get_graph_segments(model.X_train_,\n",
    "                                          model.cluster_graph_)\n",
    "\n",
    "Nx = 100\n",
    "Ny = 250\n",
    "xmin, xmax = (-375, -175)\n",
    "ymin, ymax = (-300, 200)\n",
    "\n",
    "Xgrid = np.vstack(map(np.ravel, np.meshgrid(np.linspace(xmin, xmax, Nx),\n",
    "                                            np.linspace(ymin, ymax, Ny)))).T\n",
    "\n",
    "\n",
    "total_dens = None\n",
    "\n",
    "\n",
    "\n",
    "for i in range(n_components):\n",
    "    ind = (labels == i)\n",
    "    Npts = ind.sum()\n",
    "    Nclusters = int(min(12, Npts / 5))\n",
    "    clf = compute_GMM(n_clusters=Nclusters)\n",
    "    log_dens = clf.score_samples(Xgrid).reshape(Ny, Nx)\n",
    "    dens = np.exp(log_dens)\n",
    "    if total_dens is None:\n",
    "        total_dens = dens\n",
    "    else:\n",
    "        total_dens += dens\n",
    "\n",
    "\n",
    "density = total_dens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(10, 12))\n",
    "fig.subplots_adjust(hspace=0, left=0.1, right=0.95, bottom=0.1, top=0.9)\n",
    "\n",
    "ax = fig.add_subplot(311, aspect='equal')\n",
    "ax.scatter(X[:, 1], X[:, 0], s=1, lw=0, c='k')\n",
    "ax.set_xlim(ymin, ymax)\n",
    "ax.set_ylim(xmin, xmax)\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax.set_ylabel('(Mpc)')\n",
    "\n",
    "ax = fig.add_subplot(312, aspect='equal')\n",
    "ax.plot(T_y, T_x, c='k', lw=0.5)\n",
    "ax.set_xlim(ymin, ymax)\n",
    "ax.set_ylim(xmin, xmax)\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax.set_ylabel('(Mpc)')\n",
    "\n",
    "ax = fig.add_subplot(313, aspect='equal')\n",
    "ax.plot(T_trunc_y, T_trunc_x, c='k', lw=0.5)\n",
    "ax.imshow(density.T, origin='lower', cmap=plt.cm.hot_r,\n",
    "          extent=[ymin, ymax, xmin, xmax])\n",
    "\n",
    "ax.set_xlim(ymin, ymax)\n",
    "ax.set_ylim(xmin, xmax)\n",
    "ax.set_xlabel('(Mpc)')\n",
    "ax.set_ylabel('(Mpc)')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:yse]",
   "language": "python",
   "name": "conda-env-yse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
