{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1024,\n",
    "        'height': 768,\n",
    "        'scroll': True,\n",
    "})\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%pylab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 13, ASTR 596: Fundamentals of Data Science\n",
    "\n",
    "\n",
    "## Density-based clustering methods, Anomaly Detection, Dealing with Errors\n",
    "\n",
    "### Gautham Narayan \n",
    "##### <gsn@illinois.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "- Parametric methods of clustering (k-means, GMMs)\n",
    "    - stochastic - results depend on initial guesses and number of clusters\n",
    "    - k-means is **centroid-based** - separating sample into ~equal numbers of objects in each of $k$ clusters\n",
    "    - GMMs is **distribution-based** - assume a shape for a distribution (and therefore density) and model the data as being drawn from that distribution\n",
    "\n",
    "- Neither work well when the cluster is defined in terms of density of points\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>\n",
    "            <img src=\"KMeans-density-data.svg.png\", width=\"80%\">\n",
    "        </th>\n",
    "        <th>\n",
    "            <img src=\"EM-density-data.svg.png\">\n",
    "        </th>        \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " \n",
    "- Clustering is more general than finding groups of similar things - **density estimation**\n",
    "    - Non-parametric methods (kernel density estimation -> hierarchical clustering)\n",
    "        - deterministic - you'll get the exact same results each time and you don't have to pick a number of clusters\n",
    "        - **linkage/connectivity-based** \n",
    "            - **agglomerative** - hierarchical clustering builds up clusters by associating points together\n",
    "        - Because you have to build a tree structure out of $N$ datapoints, these methods are inefficient for large $N$ ($O(n^3)$ for hierarchical clustering) \n",
    "        - there is no notion of *noise* - outliers can become their own clusters - or worse an outlier from two clusters can caused both to be linked incorrectly\n",
    "        \n",
    "<img src=\"HLINK-density-data.svg.png\">\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Density-based clustering\n",
    "\n",
    "- Rather than a linkage based approach that which considers $k$ neighbors to each point (regardless of distance modulo some overall distance threshold), these approaches look at how closely packed points are - i.e. the **local density**\n",
    "\n",
    "- Points that are in low-density regions are marked as **outliers**\n",
    "    - effectively demand that each point have at least MinPts neighbors within some distance, epsilon\n",
    "\n",
    "\n",
    "Implementation:\n",
    "    - Pick a random point in the data that hasn't been checked yet\n",
    "    - Given some epsilon, find if there are at least MinPts within epsilon of point\n",
    "        - if yes\n",
    "            - start a cluster\n",
    "        - else\n",
    "            - mark as noise (may be later marked as member of a different cluster)\n",
    "        - mark any points within epsilon of cluster points as also part of the cluster\n",
    "        - grow cluster until there are no other points that can be added\n",
    "    - repeat until all points visited          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Common implementations: DBSCAN and OPTICS (both in `sklearn.cluster`)\n",
    "    - OPTICS handles clusters with different densities better but is significantly slower\n",
    "- not completely deterministic - depends on order points are visited, but still **agglomerative** (compare vs decision trees that splits the data up into smaller groups - **divisive** \n",
    "- still don't need to specify a number of clusters or cluster centers/widths - i.e. clusters can have arbitrary shapes rather than say a GMM\n",
    "    - can be fiddly and needs some fine-tuning of epsilon **LOOK AT YOUR DATA**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## Uses in Astrophysics \n",
    " \n",
    " <img src=\"gamma_DBSCAN.jpg\">\n",
    " \n",
    " Finding clusters in Fermi-LAT $\\gamma$ ray data: https://www.aanda.org/articles/aa/abs/2013/01/aa20133-12/aa20133-12.html\n",
    " \n",
    " \n",
    "Gamma-ray astronomers could feasibly name all their photons, so finding clusters this way is really looking for overdensities on a map that is otherwise white noise - because number counts are so low, detections are very significant, but this is a way to automate over large area\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The opposite problem also works - the background is high and the clusters are barely variation on the background:\n",
    "\n",
    "<img src=\"GAIA_DR2_clusters.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And of course, you don't have to do this in RA, Dec space at all - here in 15 dimensional PCA on spectra space:\n",
    "\n",
    "<img src=\"Clustering_chem_abundances.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In-class exercise:\n",
    "\n",
    "There's some noisy real color-magnitude data for stars in `colormag.csv`\n",
    "\n",
    "1. preprocess the data (i.e. scale the magnitudes and colors in some way that makes your results insenstivie to the scale of the values - call this variable Xt\n",
    "\n",
    "2. use k-means, GMMs, hierarchical clustering and DBSCAN to cluster the dataset into similar groups. You can pick the sizes of clusters/initial estimates for parameters however you like\n",
    "\n",
    "In each case, the clustering instance that you create will have a `.labels_` attribute - I've given you plotting code to visualze your results. Each of you in a team can take one approach and work in parallel, or work together as you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "X = pd.read_csv('colormag.csv', names=['color', 'mag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## YOUR SOLUTION TO PART 1 HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## YOUR SOLUTION USING A GAUSSIAN MIXTURE HERE\n",
    "\n",
    "\n",
    "color = [f'C{i}' for i in labels]\n",
    "scatter(Xt[:,0], Xt[:,1], color=color)\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel('Color', fontsize='xx-large')\n",
    "ax.set_ylabel('Mag', fontsize='xx-large')\n",
    "ax.set_title('GMMs', fontsize='xx-large');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## YOUR SOLUTION USING DBSCAN HERE\n",
    "\n",
    "labels += labels.min()\n",
    "labels = np.abs(labels)\n",
    "color = [f'C{i}' for i in labels]\n",
    "scatter(Xt[:,0], Xt[:,1], color=color)\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel('Color', fontsize='xx-large')\n",
    "ax.set_ylabel('Mag', fontsize='xx-large')\n",
    "ax.set_title('DBSCAN', fontsize='xx-large');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extreme Deconvolution\n",
    "\n",
    "Real data come with uncertainties\n",
    "\n",
    "Extreme Deconvolution is parametric density estimation on a *noisy* d-dimensional dataset\n",
    "- Straightforward generalization of Gaussian Mixture Models conceptually but much slower\n",
    "    \n",
    "\n",
    "You have some real obsevations $\\mathbf{x}_i$ in d-dimensions *with zero-mean Gaussian noise* $\\epsilon_i$ with known per-datapoint covariance $S_i$ to a projection $R_i$ of a true value $\\mathbf{v}_i$\n",
    "\n",
    "\n",
    "## $$\n",
    "\\mathbf{x}_{i}=R_{i} \\mathbf{v}_{i}+\\epsilon_{i}, \\quad \\epsilon_{i} \\sim \\mathcal{N}\\left(\\mathbf{0}, S_{i}\\right)\n",
    "$$\n",
    "\n",
    "We assume that $\\mathbf{v}_i$ can be modelled by a mixture of Gaussians with K components (i.e. a GMM):\n",
    "\n",
    "## $$\n",
    "p\\left(\\mathbf{v}_{i} | \\theta\\right)=\\sum_{j}^{K} \\alpha_{j} \\mathcal{N}\\left(\\mathbf{v} | \\mathbf{\\mu}_{j}, \\Sigma_{j}\\right)$$\n",
    "\n",
    "with the components mean the mean, covariance and mixture coefficients of each Gaussian being the parameters that we solve for as before:\n",
    "\n",
    "## $$\\theta=\\left\\{\\alpha_{j}, \\mathbf{\\mu}_{j}, \\Sigma_{j}\\right\\}_{j=1}^{K}\n",
    "$$\n",
    "\n",
    "\n",
    "Just as before the likelihood is easy:\n",
    "## $$\n",
    "\\mathcal{L}(\\theta)=\\sum_{i}^{N} \\log \\sum_{j}^{K} \\alpha_{j} \\mathcal{N}\\left(\\mathbf{x}_{i} | R_{i} \\mathbf{\\mu}_{j}, C_{i j}\\right)$$\n",
    "\n",
    "#### BUT NOW THE COVARIANCE MATRIX INCLUDES THE ERRORS IN THE OBSERVATIONS\n",
    "\n",
    "## $$ C_{i j}=R_{i} \\Sigma_{j} R_{i}^{T}+S_{i}\n",
    "$$\n",
    "\n",
    "Proceed with EM as before, except now instead of estimating the covariance of just the $k$ GMM components, you now have the the $n \\times n$ covariance matrix of the observations.\n",
    "\n",
    "This is unfortunate because the covariance matrix appears as the inverse in the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from astroML.density_estimation import XDGMM\n",
    "from astroML.crossmatch import crossmatch\n",
    "from astroML.datasets import fetch_sdss_S82standards, fetch_imaging_sample\n",
    "from astroML.plotting.tools import draw_ellipse\n",
    "from astroML.utils.decorators import pickle_results\n",
    "from astroML.stats import sigmaG\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# define u-g-r-i-z extinction from Berry et al, arXiv 1111.4985\n",
    "# multiply extinction by A_r\n",
    "extinction_vector = np.array([1.810, 1.400, 1.0, 0.759, 0.561])\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Fetch and process the noisy imaging data\n",
    "data_noisy = fetch_imaging_sample()\n",
    "\n",
    "# select only stars\n",
    "data_noisy = data_noisy[data_noisy['type'] == 6]\n",
    "\n",
    "# Get the extinction-corrected magnitudes for each band\n",
    "X = np.vstack([data_noisy[f + 'RawPSF'] for f in 'ugriz']).T\n",
    "Xerr = np.vstack([data_noisy[f + 'psfErr'] for f in 'ugriz']).T\n",
    "\n",
    "# extinction terms from Berry et al, arXiv 1111.4985\n",
    "X -= (extinction_vector * data_noisy['rExtSFD'][:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# Fetch and process the stacked imaging data\n",
    "data_stacked = fetch_sdss_S82standards()\n",
    "\n",
    "# cut to RA, DEC range of imaging sample\n",
    "RA = data_stacked['RA']\n",
    "DEC = data_stacked['DEC']\n",
    "data_stacked = data_stacked[(RA > 0) & (RA < 10) &\n",
    "                            (DEC > -1) & (DEC < 1)]\n",
    "\n",
    "# get stacked magnitudes for each band\n",
    "Y = np.vstack([data_stacked['mmu_' + f] for f in 'ugriz']).T\n",
    "Yerr = np.vstack([data_stacked['msig_' + f] for f in 'ugriz']).T\n",
    "\n",
    "# extinction terms from Berry et al, arXiv 1111.4985\n",
    "Y -= (extinction_vector * data_stacked['A_r'][:, None])\n",
    "\n",
    "# quality cuts\n",
    "g = Y[:, 1]\n",
    "mask = ((Yerr.max(1) < 0.05) &\n",
    "        (g < 20))\n",
    "data_stacked = data_stacked[mask]\n",
    "Y = Y[mask]\n",
    "Yerr = Yerr[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# cross-match\n",
    "#  the imaging sample contains both standard and variable stars.  We'll\n",
    "#  perform a cross-match with the standard star catalog and choose objects\n",
    "#  which are common to both.\n",
    "Xlocs = np.hstack((data_noisy['ra'][:, np.newaxis],\n",
    "                   data_noisy['dec'][:, np.newaxis]))\n",
    "Ylocs = np.hstack((data_stacked['RA'][:, np.newaxis],\n",
    "                   data_stacked['DEC'][:, np.newaxis]))\n",
    "\n",
    "print(\"number of noisy points:  \", Xlocs.shape)\n",
    "print(\"number of stacked points:\", Ylocs.shape)\n",
    "\n",
    "# find all points within 0.9 arcsec.  This cutoff was selected\n",
    "# by plotting a histogram of the log(distances).\n",
    "dist, ind = crossmatch(Xlocs, Ylocs, max_distance=0.9 / 3600)\n",
    "\n",
    "noisy_mask = (~np.isinf(dist))\n",
    "stacked_mask = ind[noisy_mask]\n",
    "\n",
    "# select the data\n",
    "data_noisy = data_noisy[noisy_mask]\n",
    "X = X[noisy_mask]\n",
    "Xerr = Xerr[noisy_mask]\n",
    "\n",
    "data_stacked = data_stacked[stacked_mask]\n",
    "Y = Y[stacked_mask]\n",
    "Yerr = Yerr[stacked_mask]\n",
    "\n",
    "# double-check that our cross-match succeeded\n",
    "assert X.shape == Y.shape\n",
    "print(\"size after crossmatch:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# perform extreme deconvolution on the noisy sample\n",
    "\n",
    "# first define mixing matrix W\n",
    "W = np.array([[0, 1, 0, 0, 0],    # g magnitude\n",
    "              [1, -1, 0, 0, 0],   # u-g color\n",
    "              [0, 1, -1, 0, 0],   # g-r color\n",
    "              [0, 0, 1, -1, 0],   # r-i color\n",
    "              [0, 0, 0, 1, -1]])  # i-z color\n",
    "\n",
    "X = np.dot(X, W.T)\n",
    "Y = np.dot(Y, W.T)\n",
    "\n",
    "# compute error covariance from mixing matrix\n",
    "Xcov = np.zeros(Xerr.shape + Xerr.shape[-1:])\n",
    "Xcov[:, range(Xerr.shape[1]), range(Xerr.shape[1])] = Xerr ** 2\n",
    "\n",
    "# each covariance C = WCW^T\n",
    "# best way to do this is with a tensor dot-product\n",
    "Xcov = np.tensordot(np.dot(Xcov, W.T), W, (-2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# This is a long calculation: save results to file\n",
    "@pickle_results(\"XD_stellar.pkl\")\n",
    "def compute_XD(n_clusters=12, rseed=0, max_iter=100, verbose=True):\n",
    "    np.random.seed(rseed)\n",
    "    clf = XDGMM(n_clusters, max_iter=max_iter, tol=1E-5, verbose=verbose)\n",
    "    clf.fit(X, Xcov)\n",
    "    return clf\n",
    "\n",
    "\n",
    "clf = compute_XD(12)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Fit and sample from the underlying distribution\n",
    "np.random.seed(42)\n",
    "X_sample = clf.sample(X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# plot the results\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "fig.subplots_adjust(left=0.12, right=0.95,\n",
    "                    bottom=0.1, top=0.95,\n",
    "                    wspace=0.02, hspace=0.02)\n",
    "\n",
    "# only plot 1/10 of the stars for clarity\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax1.scatter(Y[::10, 2], Y[::10, 3], s=9, lw=0, c='k')\n",
    "\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax2.scatter(X[::10, 2], X[::10, 3], s=9, lw=0, c='k')\n",
    "\n",
    "ax3 = fig.add_subplot(223)\n",
    "ax3.scatter(X_sample[::10, 2], X_sample[::10, 3], s=9, lw=0, c='k')\n",
    "\n",
    "ax4 = fig.add_subplot(224)\n",
    "for i in range(clf.n_components):\n",
    "    draw_ellipse(clf.mu[i, 2:4], clf.V[i, 2:4, 2:4], scales=[2],\n",
    "                 ec='k', fc='gray', alpha=0.2, ax=ax4)\n",
    "\n",
    "titles = [\"Standard Stars\", \"Single Epoch\",\n",
    "          \"Extreme Deconvolution\\n  resampling\",\n",
    "          \"Extreme Deconvolution\\n  cluster locations\"]\n",
    "ax = [ax1, ax2, ax3, ax4]\n",
    "\n",
    "for i in range(4):\n",
    "    ax[i].set_xlim(-0.6, 1.8)\n",
    "    ax[i].set_ylim(-0.6, 1.8)\n",
    "\n",
    "    ax[i].xaxis.set_major_locator(plt.MultipleLocator(0.5))\n",
    "    ax[i].yaxis.set_major_locator(plt.MultipleLocator(0.5))\n",
    "\n",
    "    ax[i].text(0.05, 0.95, titles[i],\n",
    "               ha='left', va='top', transform=ax[i].transAxes)\n",
    "\n",
    "    if i in (0, 1):\n",
    "        ax[i].xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    else:\n",
    "        ax[i].set_xlabel('$g-r$')\n",
    "\n",
    "    if i in (1, 3):\n",
    "        ax[i].yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    else:\n",
    "        ax[i].set_ylabel('$r-i$')\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Second figure: the width of the locus\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "labels = ['single epoch', 'standard stars', 'XD resampled']\n",
    "linestyles = ['solid', 'dashed', 'dotted']\n",
    "for data, label, ls in zip((X, Y, X_sample), labels, linestyles):\n",
    "    g = data[:, 0]\n",
    "    gr = data[:, 2]\n",
    "    ri = data[:, 3]\n",
    "\n",
    "    r = g - gr\n",
    "    i = r - ri\n",
    "\n",
    "    mask = (gr > 0.3) & (gr < 1.0)\n",
    "    g = g[mask]\n",
    "    r = r[mask]\n",
    "    i = i[mask]\n",
    "\n",
    "    w = -0.227 * g + 0.792 * r - 0.567 * i + 0.05\n",
    "\n",
    "    sigma = sigmaG(w)\n",
    "\n",
    "    ax.hist(w, bins=np.linspace(-0.08, 0.08, 100), linestyle=ls,\n",
    "            histtype='step', label=label + '\\n\\t' + r'$\\sigma_G=%.3f$' % sigma,\n",
    "            density=True)\n",
    "\n",
    "ax.legend(loc=2)\n",
    "ax.text(0.95, 0.95, '$w = -0.227g + 0.792r$\\n$ - 0.567i + 0.05$',\n",
    "        transform=ax.transAxes, ha='right', va='top')\n",
    "\n",
    "ax.set_xlim(-0.07, 0.07)\n",
    "ax.set_ylim(0, 55)\n",
    "\n",
    "ax.set_xlabel('$w$')\n",
    "ax.set_ylabel('$N(w)$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Anomaly Detection\n",
    "\n",
    "ML algorithms suffer in terms of their performance when outliers are not taken care of. \n",
    "\n",
    "Bad ideas include \n",
    "- dropping them from your sample\n",
    "- leaving them in, using ML methods, and then ignoring the impact of the outliers\n",
    "\n",
    "\n",
    "Imagine if a bank did either of these for fradulent credit card transactions..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One efficient way of performing outlier detection in high-dimensional datasets is to use random forests\n",
    "\n",
    "Isolation forests ‘isolate’ observations by constructing decesion trees:\n",
    "- randomly selecting a feature and then \n",
    "- randomly selecting a split value between the maximum and minimum values of the selected feature\n",
    "\n",
    "Number of splittings required to isolate a sample = path length from the root node to the terminating node\n",
    "\n",
    "Path length, averaged over a forest of such random trees, is a measure of normality and our decision function.\n",
    "\n",
    "Random partitioning produces noticeable shorter paths for anomalies. \n",
    "\n",
    "When a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies - i.e. **linkage-based**\n",
    "\n",
    "<img src=\"IsolationForest1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Random forest are **supervised** so why are they showing up here in **unsupervised learning**?\n",
    "- there's only two labels (outlier/not outlier)\n",
    "- but you still have to condition the algorithm on a training set (even if it doesn't have labels) \n",
    "\n",
    "\n",
    "## Local Outlier Fraction:\n",
    "\n",
    "LOF uses density-based outlier detection to identify local outliers\n",
    "- points that are outliers with respect to their local neighborhood, rather than with respect to the global distribution. \n",
    "\n",
    "A point is labeled as an outlier if the density around that point is significantly different from the density around its neighbors.\n",
    "  \n",
    "The higher the LOF value for an observation, the more anomalous the observation.\n",
    "\n",
    "Useful because can identify a point that’s an outlier relative to a nearby cluster of points (a local outlier) even if that whole region is an outlying region in the global space of data points.\n",
    "    - Application SNIa have subclasses that are odd, and even amongst those sub-classes there are real oddballs - these are good to identify because they tell us the most about the physics of the explosion.\n",
    "\n",
    "\n",
    "<img src=\"LOF.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In class-exercise:\n",
    "\n",
    "Train an isolation forest and LOF on a random subset of the color-mag data. \n",
    "\n",
    "(For isolation forest, you can use `sklearn.model_selection.train_test_split` to split it into a training set and test set)\n",
    "\n",
    "and as before train the algoritin \n",
    "\n",
    "```\n",
    "clf = IsolationForest(max_samples=100, random_state=rng)\n",
    "# you can also set max_samples to auto and set a contamination fraction - roughly how much \n",
    "# contamination does your dataset have\n",
    "clf.fit(Xtrain)\n",
    "y_pred_train = clf.predict(Xtrain)\n",
    "y_pred_test = clf.predict(Xtest)\n",
    "```\n",
    "\n",
    "or with `sklearn.neighbors.LocalOutlierFactor`\n",
    "\n",
    "```\n",
    "clf = LocalOutlierFactor(n_neighbors=2)\n",
    "y_pred_train = clf.predict(Xtrain)\n",
    "y_pred_test = clf.predict(Xtest)\n",
    "```\n",
    "\n",
    "As before I've given you the plotting code using the variable names in the example.\n",
    "Tweak the values - see what happens to your output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## YOUR CODE WITH ISOLATION FORESTS HERE\n",
    "\n",
    "\n",
    "#outliers are -1, inliers are 1\n",
    "y_pred_train[y_pred_train == -1] += 1\n",
    "y_pred_test[y_pred_test == -1] += 1\n",
    "\n",
    "color_train = [f'C{i}' for i in y_pred_train]\n",
    "scatter(Xtrain[:,0], Xtrain[:,1], color=color_train, marker='.', label='Training set')\n",
    "color_test = [f'C{i}' for i in y_pred_test]\n",
    "scatter(Xtest[:,0], Xtest[:,1], color=color_test, marker='s', label='Test set')\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.legend(fontsize='x-large', loc='lower left')\n",
    "\n",
    "ax.set_xlabel('Color', fontsize='xx-large')\n",
    "ax.set_ylabel('Mag', fontsize='xx-large')\n",
    "ax.set_title('Isolation Forest', fontsize='xx-large');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## YOUR CODE WITH THE LOCAL OUTLIER FRACTION HERE \n",
    "\n",
    "\n",
    "\n",
    "#outliers are -1, inliers are 1\n",
    "labels[labels == -1] += 1\n",
    "\n",
    "\n",
    "color = [f'C{i}' for i in labels]\n",
    "scatter(Xt[:,0], Xt[:,1], color=color, marker='o')\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.set_xlabel('Color', fontsize='xx-large')\n",
    "ax.set_ylabel('Mag', fontsize='xx-large')\n",
    "ax.set_title('Local Outlier Fraction', fontsize='xx-large');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary of Chapter 6 methods from Ivezic Table 6.1 \n",
    "\n",
    "|Method          |Accuracy|Interpretability|Simplicity|Speed|\n",
    "|----------------|--------|----------------|----------|-----|\n",
    "|K-nearest Neighbor| H | H | H | M |\n",
    "|Kernel Density Estimation| H | H | H | H |\n",
    "|Hierarchical Clustering| H | L | L | L |\n",
    "||||||\n",
    "|K-Means| L | M | H | M |\n",
    "|Max-radius minimization| L | M | M | M |\n",
    "|Mean shift| M | H | H | M |\n",
    "|Gaussian Mixture Models| H | M | M | M |\n",
    "|Extreme Deconvolution| H | H | M | M |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Unsupervised ML \"wisdom\"\n",
    "\n",
    "\n",
    "* Do you have labels that you trust?\n",
    "    - Why are you using unsupervised methods at all then\n",
    "* Do you have an a priori reason for there to be $k$ separate groups?\n",
    "    - k-Means/GMM\n",
    "* Are your observations really noise and wiping out structure?\n",
    "    - Extreme deconvolution\n",
    "\n",
    "* Do you really need clusters or are you trying to get a sense of the underlying density distribution\n",
    "    - KDEs\n",
    "* Do you need clustering but your clusters are defined by an overdensity against a background\n",
    "    - DBSCAN/OPTICS\n",
    "* Does your sample have some natural hierarchy\n",
    "    - Hierarchical clustering\n",
    "    \n",
    "* Do you care about the stuff that isn't part of some underlying distribution model\n",
    "    - IsolationForest/LOF\n",
    "\n",
    "There is no one right answer.\n",
    "\n",
    "You are looking for structure in your data with unsupervised methods, so the right answer is to **look at your data** - i.e. try a few different things and refine"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:yse]",
   "language": "python",
   "name": "conda-env-yse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
