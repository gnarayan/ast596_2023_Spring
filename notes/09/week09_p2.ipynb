{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1920,\n",
    "        'height': 1080,\n",
    "        'scroll': True,\n",
    "})\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from IPython.lib.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 09, ASTR 596: Fundamentals of Data Science\n",
    "\n",
    "\n",
    "## Gaussian Processes contd.\n",
    "\n",
    "### Gautham Narayan \n",
    "##### <gsn@illinois.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's step back and go over GPs a bit more slowly.\n",
    "\n",
    "#### First, conceptually why even bother with GPs\n",
    "\n",
    "<img src=\"GP_as_priors_on_functions.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## You can’t get anywhere without making some assumptions\n",
    "\n",
    "### You did this in the first half of the semester as well - you assumed a model (likelihood, priors...) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Where we were interpreting priors as probability distributions from which the observations could concievably be drawn, here we are treating priors as probability distributions on functions that could concievably be consistent with the observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### GPs are a nice way of expressing this ‘prior on functions’ idea\n",
    "\n",
    "Under certain assumptions GPs can answer the following questions:\n",
    "- Here’s where the function will **most likely be** (expected function)\n",
    "- Here are some **examples** of what it might look like (sampling from the posterior distribution)\n",
    "- Here is a prediction of what you’ll see if you evaluate your function at x’, **with confidence**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap:\n",
    "\n",
    "The first half of the semester, you got famililar with observations drawn from a distribution e.g.\n",
    "\n",
    "$y1$, drawn from a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$:\n",
    "\n",
    "\\begin{align}\n",
    "p(y_1 | \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left[ - \\frac{(y_1-\\mu)^2}{2 \\sigma^2} \\right] \n",
    "\\end{align}\n",
    "\n",
    "i.e.\n",
    "\n",
    "### $$y_1 \\sim \\mathcal{N}(\\mu,\\sigma^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"1DGauss.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If pair of variables $y_1$ and $y_2$, drawn from a *bivariate Gaussian distribution*. The *joint probability density* for $y_1$ and $y_2$ is:\n",
    "\n",
    "### $$\n",
    "\\left[ \\begin{array}{l} y_1 \\\\ y_2 \\end{array} \\right] \\sim \\mathcal{N} \\left(\n",
    "\\left[ \\begin{array}{l} \\mu_1 \\\\ \\mu_2 \\end{array}  \\right] , \n",
    "\\left[ \\begin{array}{ll} \n",
    "\\sigma_1^2 & C \\\\\n",
    "C & \\sigma_2^2 \n",
    "\\end{array}  \\right] \n",
    "\\right),\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "### $$C = {\\rm cov}(y_1,y_2)$$ \n",
    "\n",
    "is the *covariance* between $y_1$ and $y_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"2DGauss.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the first half of the semester, we dealt with independent variables i.e.\n",
    "\n",
    "### $$P(y_1 \\cap y_2) = P(y_1) \\cdot P(y_2) $$\n",
    "\n",
    "and consequently\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "P(y_2|y_1) = \\frac{P(y_1 \\cap y_2)}{P(y_1)} = P(y_2)\n",
    "\\end{align}\n",
    "\n",
    "If two variables are independent, then $C = 0$ (remember converse isn't true). \n",
    "\n",
    "The observations are *uncorrelated* so measuring $y_1$ doesn't teach us anything about $y_2$.\n",
    "\n",
    "(If in addition $\\mu_1 = \\mu_2$ and $\\sigma_1 = \\sigma_2$ the variables are i.i.d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# With time-series, $C \\ne 0$ \n",
    "\n",
    "If we know the value of $y_1$, the probability density for $y_2$ collapses to the the *conditional distribution* of $y_2$ given $y_1$:\n",
    "\n",
    "### $$\n",
    "p(y_2 \\mid y_1) = \\mathcal{N} \\left( \\mu_2 + C (y_1-\\mu_1)/\\sigma_1^2, \\sigma_2^2-C^2\\sigma_1^2 \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Consider $N$ variables drawn from a multivariate Gaussian distribution:\n",
    "\n",
    "### $$\n",
    "\\boldsymbol{y} \\sim \\mathcal{N} (\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "### $$\\boldsymbol{y} = (y_1,y_2,\\ldots,y_N)^T$$\n",
    "\n",
    "### $$\\boldsymbol{\\mu} = (\\mu_1,\\mu_2,\\ldots,\\mu_N)^T$$ \n",
    "\n",
    "\n",
    "is the *mean vector*, and $\\boldsymbol{\\Sigma}$ is an $N \\times N$ positive semi-definite *covariance matrix*, with elements \n",
    "\n",
    "### $$\\Sigma_{ij}={\\rm cov}(y_i,y_j)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### And then the likelihood generalizes from 1D:\n",
    "\n",
    "\\begin{align}\n",
    "p(y_1 | \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left[ - \\frac{(y_1-\\mu)^2}{2 \\sigma^2} \\right] \n",
    "\\end{align}\n",
    "\n",
    "### to ND:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "p(\\boldsymbol{y} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{\\sqrt{2 \\pi^N |\\Sigma|} } \\exp \\left[ -\\frac{1}{2} (\\boldsymbol{y} - \\boldsymbol{\\mu})^T \\Sigma^{-1} (\\boldsymbol{y} - \\boldsymbol{\\mu}) \\right] \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This works because:\n",
    "\n",
    "<img src=\"gaussians_all_the_way_down.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# A Gaussian process is an extension of this concept to infinite $N$.\n",
    "\n",
    "# This gives rise to a probability distribution over functions, rather than finite $N$ samples. \n",
    "\n",
    "<img src=\"gp.png\">\n",
    "\n",
    "# Informally - infinitely long vector ~ function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Again, for finite number of $y$ drawn from a multivariate normal distribution:\n",
    "\n",
    "### $$\n",
    "\\boldsymbol{y} \\sim \\mathcal{N} (\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})\n",
    "$$\n",
    "\n",
    "This clearly doesn't make sense for infinite $N$, but the essential feature remains the same:\n",
    "### A Gaussian process is completely specified by its *mean function* and *covariance function*.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Incorporating observational error is similar to what you did in the past as well:\n",
    "\n",
    "### $$ y \\sim f(t) + \\epsilon$$ \n",
    "\n",
    "### with deviations from the truth related to the observational uncertainties\n",
    "\n",
    "### $$ \\epsilon \\sim \\mathcal{N}(0, \\sigma_y^2) $$\n",
    "\n",
    "\n",
    "### except now, $f(t)$ is a function not of some parameters, but rather of functions thenselves:\n",
    "\n",
    "###  $$ f(t) \\sim \\mathcal{GP}(m(t), k(t,t'))$$\n",
    "\n",
    "### where I'm switching from $\\mu$ to $m(t)$ and $\\Sigma$ to $k(t, t')$ just to make explicit that these are not vectors.\n",
    "\n",
    "I'm using $k$ because this function that describes the covariance between time $t$ and $t'$ is called a **kernel** function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic Graphical Model for a GP\n",
    "\n",
    "\n",
    "### Recall:\n",
    "\n",
    "A **probabilistic graphical model** (PGM) is a very useful way of visualizing a generative model.\n",
    "* They sketch out the procedure for how one would generate mock data in practice.\n",
    "* They illustrate the interdependence of model parameters, and the dependence of data on parameters.\n",
    "* _They also (therefore) represent a conditional factorization of the PDF for all the data and model parameters._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ingredients of a PGM:\n",
    "* **Nodes** represent PDFs for parameters\n",
    "* **Edges** represent conditional relationships\n",
    "* **Plates** represent repeated model components whose contents are **conditionally independent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Types of nodes:\n",
    "* **Circles** represent a PDF. This parameter is a *stochastic* function of the parameters feeding into it.\n",
    "* **Points** represent a delta-function PDF. This parameter is a *deterministic* function of the parameters feeding into it.\n",
    "* **Double circles** (or shading) indicate measured data. They are stochastic in the context of generating mock data, but fixed in the context of parameter inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# If we were dealing with i.i.d. data\n",
    "\n",
    "### $$ y \\sim f(t) + \\epsilon$$ \n",
    "\n",
    "### with deviations from the truth related to the observational uncertainties\n",
    "\n",
    "### $$ \\epsilon \\sim \\mathcal{N}(0, \\sigma_y^2) $$\n",
    "\n",
    "\n",
    "<img src=\"pgm_conditionally_independent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# With time-series the data are not conditionally independent\n",
    "\n",
    "i.e. you don't have a nice plate:\n",
    "\n",
    "<img src=\"gp_pgm.png\">\n",
    "\n",
    "From [Rasmussen & Williams (aka the GP bible)](http://www.gaussianprocess.org/gpml/chapters/RW.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### We don't actually observe the function  $f$\n",
    "\n",
    "### As you've seen, there isn't one single function, but infinitely many for a specific choice of $m$, $k$\n",
    "\n",
    "### We marginalize over them to find the posterior mean - $f$ is behaving like a parameter \n",
    "\n",
    "### The paramters of $m$ and $k$, which actually specify $f$ are called \"hyper parameters\"\n",
    "\n",
    "### The interesting bit here is the covariance function/kernel, $k$ (we can always recenter the data to have mean = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Thankfully, in the real world, we only have a finite number of observations\n",
    "\n",
    "Previously, we saw:\n",
    "\n",
    "### $$\\Sigma_{ij}={\\rm cov}(y_i,y_j)$$\n",
    "\n",
    "We don't have a parameteric model for $y$ anymore, but that's OK, we can write down a parametric model for the covariance itself, i.e.:\n",
    "\n",
    "### $$\n",
    "\\mathrm{cov}(y(t),y(t'))=k(t,t') $$\n",
    "\n",
    "\n",
    "That's helpful to do, because with finite observations:\n",
    "\n",
    "### $$\n",
    "\\mathrm{cov}(y_i,y_j)=k(t_i,t_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# So we don't have parametrized model, but do have parametrized covariance - what can we do with this thing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The prior\n",
    "\n",
    "Now consider a finite set of observations: inputs $\\boldsymbol{t}$, with corresponding outputs $\\boldsymbol{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The *joint distribution* of $\\boldsymbol{y}$ given $\\boldsymbol{t}$, $m$ and $k$ is\n",
    "\n",
    "### $$\n",
    "\\mathrm{p}(\\boldsymbol{y} \\mid \\boldsymbol{t},m,k) = \\mathcal{N}( \\boldsymbol{m},K),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "where again, $\\boldsymbol{m}=m(\\boldsymbol{t})$ is the *mean vector* \n",
    "\n",
    "and $K$ is the *covariance matrix*, with elements $K_{ij} = k(t_i,t_j)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Test and training sets\n",
    "\n",
    "Suppose we have an (observed) *training set* $(\\boldsymbol{t},\\boldsymbol{y})$. \n",
    "\n",
    "We are interested in some other *test set* of inputs $\\boldsymbol{t}_*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The joint distribution over the training and test sets is\n",
    "### $$\n",
    "\\mathrm{p} \\left( \\left[ \\begin{array}{l} \\boldsymbol{y} \\\\ \\boldsymbol{y}_* \\end{array} \\right] \\right) \n",
    "= \\mathcal{N} \\left( \\left[ \\begin{array}{l} \\boldsymbol{m} \\\\ \\boldsymbol{m}_* \\end{array} \\right], \n",
    "\\left[ \\begin{array}{ll} K & K_* \\\\ K_*^T & K_{**} \\end{array} \\right] \\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "where $\\boldsymbol{m}_* = m(\\boldsymbol{x}_*)$, $K_{**,ij} = k(t_{*,i},t_{*,j})$ and $K_{*,ij} = k(t_i,t_{*,j})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is not really any different from when we just had two observations:\n",
    "\n",
    "### $$\n",
    "\\left[ \\begin{array}{l} y_1 \\\\ y_2 \\end{array} \\right] \\sim \\mathcal{N} \\left(\n",
    "\\left[ \\begin{array}{l} \\mu_1 \\\\ \\mu_2 \\end{array}  \\right] , \n",
    "\\left[ \\begin{array}{ll} \n",
    "\\sigma_1^2 & C \\\\\n",
    "C & \\sigma_2^2 \n",
    "\\end{array}  \\right] \n",
    "\\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# For notational brevity I'm going to set the mean to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The conditional distribution\n",
    "\n",
    "The *conditional distribution* for the test set given the training set is:\n",
    "\n",
    "### $$ \n",
    "\\mathrm{p} ( \\boldsymbol{y}_* \\mid \\boldsymbol{y},k) = \\mathcal{N} ( \n",
    "K_*^T K^{-1} \\boldsymbol{y}, K_{**} - K_*^T K^{-1} K_* ).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is also just a straight forward generalization from what we had with just two points:\n",
    "\n",
    "### $$\n",
    "p(y_2 \\mid y_1) = \\mathcal{N} \\left( \\mu_2 + C (y_1-\\mu_1)/\\sigma_1^2, \\sigma_2^2-C^2\\sigma_1^2 \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is called the **predictive distribution**, because it can be use to predict future (or past) observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "More generally, it can be used for *interpolating* the observations to any desired set of inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is one of the most widespread applications of GPs in some fields (e.g. kriging in geology, economic forecasting, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Real observations always contain a component of *white noise*\n",
    "\n",
    "We need to account for this, but don't necessarily want to include in the predictions. \n",
    "\n",
    "\n",
    "If the white noise variance $\\sigma^2$ is constant, we can write \n",
    "\n",
    "### $$\n",
    "\\mathrm{cov}(y_i,y_j)=k(t_i,t_j)+\\delta_{ij} \\sigma^2,\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "and the conditional distribution becomes\n",
    "\n",
    "\n",
    "### $$ \n",
    "\\mathrm{p} ( \\boldsymbol{y}_* \\mid \\boldsymbol{y},k) = \\mathcal{N} ( \n",
    "K_*^T (K + \\sigma^2 \\mathbb{I})^{-1} \\boldsymbol{y}, K_{**} - K_*^T (K + \\sigma^2 \\mathbb{I})^{-1} K_* ).\n",
    "$$\n",
    "\n",
    "\n",
    "We assumed constant white noise, but it's trivial to allow for different $\\sigma$ for each data point.\n",
    "\n",
    "You could also add some intrinsic dispersion as you often have to do. \n",
    "\n",
    "In real life, we may need to learn $\\sigma_{\\text{int}}$ from the data, alongside the other contribution to the covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Single-point prediction\n",
    "\n",
    "Let us look more closely at the predictive distribution for a single test point $t_*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is a Gaussian with mean:\n",
    "### $$\n",
    "\\overline{y}_* = \\boldsymbol{k}_*^T (K + \\sigma^2 \\mathbb{I})^{-1} \\boldsymbol{y}\n",
    "$$\n",
    "\n",
    "and variance\n",
    "### $$\n",
    "\\mathbb{V}[y_*] = k(t_*,t_*) - \\boldsymbol{k}_*^T (K + \\sigma^2 \\mathbb{I})^{-1} \\boldsymbol{k}_*,\n",
    "$$\n",
    "where $\\boldsymbol{k}_*$ is the vector of covariances between the test point and the training points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notice the mean is a linear combination of the observations: the GP is a *linear predictor*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is also a linear combination of covariance functions, each centred on a training point:\n",
    "\n",
    "### $$\n",
    "\\overline{y}_* = \\sum_{i=1}^N \\alpha_i k(x_i,x_*),\n",
    "$$\n",
    "where $\\alpha_i = (K + \\sigma^2 \\mathbb{I})^{-1} y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# So how do you choose the kernel $k$?\n",
    "\n",
    "Common choices: http://www.cs.toronto.edu/~duvenaud/cookbook/index.html\n",
    "\n",
    "Lets fiddle:\n",
    "\n",
    "# In-class exercise:\n",
    "\n",
    "[Click this](https://distill.pub/2019/visual-exploration-gaussian-processes/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The likelihood\n",
    "\n",
    "The *likelihood* of the data under the GP model is simply:\n",
    "\n",
    "### $$\n",
    "\\mathrm{p}(\\boldsymbol{y} \\,|\\, \\boldsymbol{t}) = \\mathcal{N}(\\boldsymbol{y} \\, | \\, \\boldsymbol{0},K + \\sigma^2 \\mathbb{I}).\n",
    "$$\n",
    "\n",
    "This is a measure of how well the model explains, or predicts, the training set.\n",
    "\n",
    "i.e. **The observed $\\boldsymbol{y}$ are noisy realisations of a latent (unobserved) Gaussian process $\\boldsymbol{f}$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We are marginalizing over the function values $\\boldsymbol{f}$:\n",
    "### $$\n",
    "\\mathrm{p}(\\boldsymbol{y} \\,|\\, \\boldsymbol{t}) = \\int \\mathrm{p}(\\boldsymbol{y} \\,|\\, \\boldsymbol{f},\\boldsymbol{t}) \\, \\mathrm{p}(\\boldsymbol{f} \\,|\\, \\boldsymbol{t}) \\, \\mathrm{d}\\boldsymbol{f},\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "\n",
    "### $$\n",
    "\\mathrm{p}(\\boldsymbol{f} \\,|\\, \\boldsymbol{t}) = \\mathcal{N}(\\boldsymbol{f} \\, | \\, \\boldsymbol{0},K)\n",
    "$$\n",
    "\n",
    "\n",
    "is the *prior*, and \n",
    "\n",
    "\n",
    "### $$\n",
    "\\mathrm{p}(\\boldsymbol{y} \\,|\\, \\boldsymbol{f},\\boldsymbol{t}) = \\mathcal{N}(\\boldsymbol{y} \\, | \\, \\boldsymbol{0},\\sigma^2 \\mathbb{I})\n",
    "$$\n",
    "is the *likelihood*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# You \"condition\" the hyperparameters on some observed data\n",
    "\n",
    "i.e. evaluate the conditional (or predictive) distribution for a given covariance matrix (i.e. covariance function and hyper-parameters), and training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## *Training* the GP...\n",
    "\n",
    "...means maximising the *likelihood* of the model with respect to the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# In-class Exercise: \n",
    "\n",
    "[Click this](http://chifeng.scripts.mit.edu/stuff/gp-demo/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Spitzer exoplanet transits and eclipses (Evans et al. 2015)\n",
    "\n",
    "<img src=\"Evans_Spitzer.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### GPs to deal with correlated noise in fitting spectra (Narayan et al., 2019)\n",
    "\n",
    "<img src=\"GP_spectra.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Mauna Kea CO$_2$ dataset\n",
    "\n",
    "(From Rasmussen & Williams textbook)\n",
    "\n",
    "<img height=\"700\" src=\"RW_mauna_kea.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### GPz photometric redshifts (Almosallam, Jarvis & Roberts 2016)\n",
    "\n",
    "<img src=\"Almosallam_GPz.png\" width=\"600\">"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:yse]",
   "language": "python",
   "name": "conda-env-yse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
