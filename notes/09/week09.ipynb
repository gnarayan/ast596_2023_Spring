{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1024,\n",
    "        'height': 768,\n",
    "        'scroll': True,\n",
    "})\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from IPython.lib.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 09, ASTR 596: Fundamentals of Data Science\n",
    "\n",
    "\n",
    "## Autoregressive Processes, Gaussian Processes \n",
    "\n",
    "### Gautham Narayan \n",
    "##### <gsn@illinois.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "If you get a time-series:\n",
    "\n",
    "# $$ (t_1, m_1), (t_2, m_2), ... , (t_N, m_n) $$\n",
    "\n",
    "- and you get luck enough to have a parametric model then you know what to do from first half of semester\n",
    "- you can say something about the **correlation structure** (*even if no explicit model for observations*)\n",
    "    - periodic processes\n",
    "        - periodogram to find strong periods\n",
    "        - can express as Fourier sum to construct a model and make predictions at future times\n",
    "    - stochastic processes\n",
    "        - ACF/SF\n",
    "        - PSD\n",
    "            - These are good for **characterizing** stochastic processes\n",
    "                - can use *features* for tasks like *classification* (soon! week 11!) \n",
    "                \n",
    "## But we're scientists and like to predict things (i.e. forecasting), not just characterize them - how do we do that for a generic time-series/process without an explicit model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autoregressive Processes\n",
    "\n",
    "Processes (like time-series) that \"retain memory\" of previous states, can be described by [autoregressive models](https://en.wikipedia.org/wiki/Autoregressive_model).\n",
    "\n",
    "### You have already seen one of these:\n",
    "\n",
    "Our old friend the random walk - every new value is given by the preceeding value plus some noise:\n",
    "\n",
    "# $$y_i = y_{i-1} + \\epsilon_i$$\n",
    "\n",
    "If the coefficient of $y_{i-1}$ is $>1$ then it is known as a geometric random walk, which is typical of the stock market. These are **Markov Chains** \n",
    "\n",
    "(recall that not all Markov chains are stationary - they have to be positive recurrent and irreducible - i.e. you have to be able to get from every state to every other state in some finite time - it'd be dull if the stock market was stationary)\n",
    "\n",
    "So, if you interview for a quant position on Wall Street, you tell them that you are an expert in using autoregressive geometric random walks to model stochastic processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the random walk case above, each new value depends only on the immediately preceeding value.  But we can generalized this to include $p$ values:\n",
    "\n",
    "# $$y_i = \\sum_{j=1}^pa_jy_{i-j} + \\epsilon_i$$\n",
    "\n",
    "We refer to this as an [**autoregressive (AR)**](https://en.wikipedia.org/wiki/Autoregressive_model) process of order $p$: $AR(p)$.  \n",
    "\n",
    "For a random walk, we have $p=1$, and the weights are just $a_1=1$.\n",
    "\n",
    "If the data are drawn from a \"stationary\" process (one where it doesn't matter what region of the light curve you sample [so long as it is representative]), the $a_j$ satisfy certain conditions.\n",
    "\n",
    "One thing that we might do then is ask whether a system is more consistent with $a_1=0$ or $a_1=1$ (noise vs. a random walk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## An aside:\n",
    "\n",
    "You might wonder why the stock market is well described by a geometric random walk, and doesn't need an AR(10) or something. Recall that geometric random walks are also good at describing the motions of inebriated humans. Most economic theory is based on the assumption of people being \"Rational actors\", leading to \"Efficient Markets.\" \n",
    "\n",
    "Larry Summers would famously dispute this notion with the first line of a unpublished paper:\n",
    "\"There are idiots, look around.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here are some example light curves for specific $AR(p)$ processes.  \n",
    "\n",
    "![AR Examples](https://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/ArTimeSeries.svg/1000px-ArTimeSeries.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* $AR(0)$: the light curve is simply responding to noise fluctuations.  \n",
    "\n",
    "\n",
    "* $AR(1)$: the noise fluctuation responses are persisting for slightly longer as the next time step depends positively on the time before.  \n",
    "\n",
    "\n",
    "* $AR(1)$ w/ $a_1 = 0.9$: nearly the full effect of the noise spike from the previous time step is applied again, giving particularly long and high chains of peaks and valleys.  \n",
    "\n",
    "\n",
    "* $AR(2)$:  we have long, but low chains of peaks and valleys as a spike persists for an extra time step.  \n",
    "\n",
    "\n",
    "* $AR(2)$ w/ $a_1 = 0.9$ and $a_2 = -0.8$: the response of a spike in the second time step has the opposite sign as for the first time step - both have large coefficients - peaks and valleys are both quite high and quite narrowly separated.\n",
    "\n",
    "i.e. More general than simple periodic processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Moving Average Processes\n",
    "\n",
    "A [**moving average (MA)**](https://en.wikipedia.org/wiki/Moving-average_model) process is similar to an AR process, but instead the value at each time step depends not on the *value* of previous time step, but rather the *perturbations* from previous time steps.  It is defined as\n",
    "\n",
    "# $$y_i = \\epsilon_i + \\sum_{j=1}^qb_j\\epsilon_{i-j}$$\n",
    "\n",
    "So, for example, an MA(q=1) process would look like\n",
    "\n",
    "# $$y_i = \\epsilon_{i} + b_1\\epsilon_{i-1},$$\n",
    "\n",
    "whereas an AR(p=2) process would look like\n",
    "\n",
    "# $$y_i = a_1y_{i-1} +  a_2y_{i-2} + \\epsilon_i$$\n",
    "\n",
    "\n",
    "### So, in an $MA$ process a shock affects only the current value and $q$ values into the future.  In an $AR$ process a shock affects *all* future values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## You can combine AR and MA processes. These are creatively called ARMA processes\n",
    "\n",
    "E.g. ARMA(2,1) model, which combines AR(2) and MA(1):\n",
    "\n",
    "\n",
    "# $$y_i = a_1y_{i-1} +  a_2y_{i-2} + \\epsilon_i + b_1 \\epsilon_{i-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# In-class Exercise: AR vs MA vs ARMA processes with a single shock\n",
    "\n",
    "Generate data from \n",
    "* an $AR(2)$ w/ $a_1 = 0.5, a_2=0.2$\n",
    "* an $MA(2)$ w/ $b_1 = 0.5, b_2=0.5$\n",
    "* an $ARMA(2, 1)$ w/ $a_1 = 0.5, a_2=0.25, b_1 =0.5$\n",
    "\n",
    "Add a \"shock\" (high epsilon value at t=3 - see how the different procsses respond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "N=20\n",
    "\n",
    "epsilon = np.random.randn(N+2)  \n",
    "epsilon[3] = # complete me\n",
    "\n",
    "\n",
    "yAR=np.zeros(N+2)\n",
    "yMA=np.zeros(N+2)\n",
    "yARMA=np.zeros(N+2)\n",
    "\n",
    "for i in np.arange(N)+2:\n",
    "\n",
    "    yAR[i] = 0.5*yAR[i-1] + 0.2*yAR[i-2] + epsilon[i]  \n",
    "    yMA[i] = # complete me\n",
    "    yARMA[i] = # complete me\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "t = np.arange(len(yAR))\n",
    "plt.plot(t,yAR,label=\"AR(2), a_1=0.5, a_2=0.2\")\n",
    "plt.plot(t,yMA,label=\"MA(2), b_1=0.5, b_2=0.5\")\n",
    "plt.plot(t,yARMA,label=\"ARMA(2,1), a_1=0.5, a_2=0.25, b_1=0.5\",zorder=0)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(loc=\"upper right\",prop={'size':8})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "These videos might be useful:\n",
    "\n",
    "[MA(1)](https://www.youtube.com/watch?v=lUhtcP2SUsg)\n",
    "\n",
    "[AR(1)](https://www.youtube.com/watch?v=AN0a58F6cxA)\n",
    "\n",
    "[ARMA(1,1)](https://www.youtube.com/watch?v=Pg0RnP1uLVc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CARMA Models\n",
    "\n",
    "$AR$ and $ARMA$ models assume evenly sampled time-series data.  However, we can extend this to unevenly sampled data.\n",
    "\n",
    "These are **continuous** ARMA or CARMA models.\n",
    "\n",
    "\n",
    "A $CAR(1)$ process is described by a stochastic differential equation which includes a damping term that pushes $y(t)$ back towards the mean, so it is also a **damped random walk (DRW)**.  \n",
    "\n",
    "For evenly sampled data a CAR(1) process is the same as an AR(1) process with $a_1=\\exp(-1/\\tau)$.  \n",
    "\n",
    "That is, the next value is the previous value times the damping factor (plus noise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Connecting this back to the ACF/SF/PSD\n",
    "\n",
    "The ACF for a DRW is given by\n",
    "# $$ ACF(t) = \\exp(-t/\\tau),$$\n",
    "where $\\tau$ is the characteristic timescale (i.e., the damping timescale).\n",
    "\n",
    "The structure function can be written as\n",
    "# $$ SF(t) = SF_{\\infty}[1-\\exp(-t/\\tau)]^{1/2}.$$\n",
    "\n",
    "The PSD is then\n",
    "# $$ PSD(f) = \\frac{\\tau^2 SF_{\\infty}^2}{1+(2\\pi f \\tau)^2},$$\n",
    "\n",
    "\n",
    "which means that a DRW is a $1/f^2$ process at high frequency.  \n",
    "\n",
    "The *damped* part comes from the flat PSD at low frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# So if you have a correlation function, how do you get to $y(t)$?\n",
    "\n",
    "\n",
    "A stochastic process is collection of variables drawn from _a probability distribution over functions_.\n",
    "\n",
    "In other words, if our function of interest is $y(t)$, a stochastic process assigns probabilities $P\\left[y(t)\\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gaussian Processes\n",
    "\n",
    "A Gaussian process has the property that\n",
    "\n",
    "$P\\left[y(x) | y(x_1), y(x_2), \\ldots\\right]$\n",
    "\n",
    "is a Gaussian depending on the $x_i$ and $y(x_i)$. The process is specified by a \"mean function\" $\\mu(x)$ and a \"covariance function\" $C(x)$, or \"kernel,\" which determines how quickly $y(x)$ can vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian Processes in Data Analysis\n",
    "\n",
    "A draw from $P[y(x^*)]$ would represent a prior prediction for the function value $y(x^*)$\n",
    "\n",
    "Typically we are more interested in the posterior prediction, drawn from $P[y(x^*)\\vert y^{\\rm obs}(x_{\\rm obs})]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $$ \\log L = - \\log | \\Sigma| - |y -u|^T \\Sigma^{-1} |y-u|$$\n",
    "\n",
    "\n",
    "# $$ \\Sigma = (n x n)\\text{ matrix}$$ \n",
    "\n",
    "with \n",
    "\n",
    "# $$ \\Sigma_{i,j} = \\text{Cov}(y(t_i), y(t_j)) = \\int_{-\\infty}^{\\infty} \\text{PSD} e^{2\\pi if |t_i - t_j|} \\,df $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The posterior PDF for $y(x^*)$ is a Gaussian, whose mean and standard deviation can be computed algebraically, and which is constrained by _all the previously observed $y(x)$_.\n",
    "\n",
    "\n",
    "<img src=\"mfm_gp_example.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GP Regression\n",
    "\n",
    "GP's provide a natural way to achieve high flexibility (and uncertainty) when _interpolating_ data. \n",
    "\n",
    "With the appropriate assumptions (e.g. Gaussian measurement errors), the calculation of the posterior for $y(x)$ is an _algebraic_ operation (no Monte Carlo required).\n",
    "\n",
    "Marginalization over the GP hyperparameters (the width of the kernel, for example) is more computationally expensive (involving the determinants of the matrices), but [fast methods have been developed](http://dan.iel.fm/george/current/user/hyper/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model \"free\" models - aka \"non-parametric models\"\n",
    "\n",
    "\n",
    "Sometimes we simply don't have a good first-principles model for what's going on in our data, but we're also confident that making a simple assumption (e.g. Gaussian scatter) is dead wrong.\n",
    "\n",
    "### What does \"model-free\" mean?\n",
    "\n",
    "In these situations, we're motivated to avoid strong modeling assumptions and instead be more empirical.\n",
    "\n",
    "Common adjectives:\n",
    "* non-parametric\n",
    "* model-independent\n",
    "* data-driven\n",
    "* empirical\n",
    "\n",
    "(Strictly speaking, these tend to correspond to models with very many parameters, but the terminology persists.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Gaussian processes appear to be \"non-parametric\" because the algebraic evaluation of the posterior PDF includes analytic marginalization over all the (nuisance) parameters in the model (the true values of $y$ at each $x_{\\rm obs}$).\n",
    "\n",
    "As with all non-parametric models, GPs are not \"assumption-free\" or \"model-independent\": they are just not _simply_ or _physically_ parametrized, and so involve different _types_ of assumptions.\n",
    "\n",
    "The trade-off between simply-parametrized and non-parametric models is between _interpretability_ (typically high for simply-parametrized physical models) and _prediction accuracy_ (typically high for non-parametric models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# In-class exercise: A full worked example of a GP\n",
    "\n",
    "We'll work through this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Data\n",
    "\n",
    "Let's generate a simple Cepheids-like dataset: observations of $y$ with reported uncertainties $\\sigma_y$, at given $x$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "\n",
    "xlimits = [0,350]\n",
    "ylimits = [0,250]\n",
    "\n",
    "def generate_data(seed=None):\n",
    "    \"\"\"\n",
    "    Generate a 30-point data set, with x and sigma_y as standard, but with\n",
    "    y values given by\n",
    "\n",
    "    y = a_0 + a_1 * x + a_2 * x**2 + a_3 * x**3 + noise\n",
    "    \"\"\"\n",
    "    Ndata = 30\n",
    "\n",
    "    xbar = 0.5*(xlimits[0] + xlimits[1])\n",
    "    xstd = 0.25*(xlimits[1] - xlimits[0])\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed=seed)\n",
    "\n",
    "    x = xbar + xstd * np.random.randn(Ndata)\n",
    "\n",
    "    meanerr = 0.025*(xlimits[1] - xlimits[0])\n",
    "\n",
    "    sigmay = meanerr + 0.3 * meanerr * np.abs(np.random.randn(Ndata))\n",
    "\n",
    "    a = np.array([37.2,0.93,-0.002,0.0])\n",
    "    y = a[0] + a[1] * x + a[2] * x**2 + a[3] * x**3 + sigmay*np.random.randn(len(x))\n",
    "\n",
    "    return x,y,sigmay\n",
    "\n",
    "def plot_yerr(x, y, sigmay):\n",
    "    \"\"\"\n",
    "    Plot an (x,y,sigma) dataset as a set of points with error bars \n",
    "    \"\"\"\n",
    "    plt.errorbar(x, y, yerr=sigmay, fmt='.', ms=7, lw=1, color='k')\n",
    "    plt.xlabel('$x$', fontsize=16)\n",
    "    plt.ylabel('$y$', fontsize=16)\n",
    "    plt.xlim(*xlimits)\n",
    "    plt.ylim(*ylimits)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "(x, y, sigmay) = generate_data(seed=13)\n",
    "\n",
    "plot_yerr(x, y, sigmay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting a Gaussian Process\n",
    "\n",
    "Let's follow [Jake VanderPlas' example](http://www.astroml.org/book_figures/chapter8/fig_gp_example.html#book-fig-chapter8-fig-gp-example), to see how to work with the [`scikit-learn` v0.18](http://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html#gaussian-processes-regression-basic-introductory-example) Gaussian Process regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF as SquaredExponential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Defining a GP\n",
    "\n",
    "First we define a kernel function, for populating the covariance matrix of our GP. To avoid confusion, a Gaussian kernel is referred to as a \"squared exponential\" (or a \"radial basis function\", RBF). The squared exponential kernel has one hyper-parameter, the length scale that is the Gaussian width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "h = 10.0\n",
    "\n",
    "kernel = SquaredExponential(length_scale=h, length_scale_bounds=(0.01, 1000.0))\n",
    "gp0 = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, let's draw some samples from the unconstrained process, o equivalently, the prior. Each sample is a function $y(x)$, which we evaluate on a grid. We'll need to assert a value for the kernel hyperparameter $h$, which dictates the correlation length between the datapoints. That will allow us to compute a mean function (which for simplicity we'll set to the mean observed $y$ value), and a covariance matrix that captures the correlations between datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "xgrid = np.atleast_2d(np.linspace(0, 399, 100)).T\n",
    "print(\"y(x) will be predicted on a grid of length\", len(xgrid))\n",
    "\n",
    "# Draw three sample y(x) functions:\n",
    "draws = gp0.sample_y(xgrid, n_samples=3)\n",
    "\n",
    "print(\"Drew 3 samples, stored in an array with shape \", draws.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's plot these, to see what our prior looks like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Start a 4-panel figure:\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "# Plot our three prior draws:\n",
    "ax = fig.add_subplot(221)\n",
    "ax.plot(xgrid, draws[:,0], '-r')\n",
    "ax.plot(xgrid, draws[:,1], '-g')\n",
    "ax.plot(xgrid, draws[:,2], '-b', label='Rescaled prior sample $y(x)$')\n",
    "ax.set_xlim(0, 399)\n",
    "ax.set_ylim(-5, 5)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y(x)$')\n",
    "ax.legend(fontsize=8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each predicted $y(x)$ is drawn from a Gaussian of unit variance, and with off-diagonal elements determined by the covariance function. \n",
    "\n",
    "Try changing `h` to see what happens to the smoothness of the predictions. \n",
    "\n",
    "> Go back up to the cell where `h` is assigned, and re-run that cell and the subsequent ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For our data to be well interpolated by this Gaussian Process, it will need to be rescaled such that it has zero mean and unit variance. There are [standard methods for doing this](http://scikit-learn.org/stable/modules/preprocessing.html), but we'll do this rescaling here for transparency - and so we know what to add back in later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Rescale():\n",
    "    def __init__(self, y, err):\n",
    "        self.original_data = y\n",
    "        self.original_err = err\n",
    "        self.mean = np.mean(y)\n",
    "        self.std = np.std(y)\n",
    "        self.transform()\n",
    "        return\n",
    "    def transform(self):\n",
    "        self.y = (self.original_data - self.mean) / self.std\n",
    "        self.err = self.original_err / self.std\n",
    "        return()\n",
    "    def invert(self, scaled_y, scaled_err):\n",
    "        return (scaled_y * self.std + self.mean, scaled_err * self.std)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "rescaled = Rescale(y, sigmay)\n",
    "print('Mean, variance of original data: ',np.round(np.mean(y)), np.round(np.var(y)))\n",
    "print('Mean, variance of rescaled data: ',np.round(np.mean(rescaled.y)), np.round(np.var(rescaled.y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "y2, sigmay2 = rescaled.invert(rescaled.y, rescaled.err)\n",
    "print('Mean, variance of inverted, rescaled data: ',np.round(np.mean(y2)), np.round(np.var(y2)))\n",
    "print('Maximum differences in y, sigmay, after round trip: ',np.max(np.abs(y2 - y)), np.max(np.abs(sigmay2 - sigmay)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Constraining the GP\n",
    "\n",
    "Now, using the same covariance function, lets \"fit\" the GP by constraining each draw from the GP to go through our data points, and _optimizing_ the length scale hyperparameter `h`. \n",
    "\n",
    "Let's first look at how this would work for two data points with no uncertainty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Choose two of our (rescaled) datapoints:\n",
    "x1 = np.array([x[10], x[12]])\n",
    "rescaled_y1 = np.array([rescaled.y[10], rescaled.y[12]])\n",
    "rescaled_sigmay1 = np.array([rescaled.err[10], rescaled.err[12]])\n",
    "\n",
    "# Instantiate a GP model, with initial length_scale h=10:\n",
    "kernel = SquaredExponential(length_scale=10.0, length_scale_bounds=(0.01, 1000.0))\n",
    "gp1 = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "\n",
    "# Fit it to our two noiseless datapoints:\n",
    "gp1.fit(x1[:, None], rescaled_y1)\n",
    "\n",
    "# We have fit for the length scale parameter: print the result here:\n",
    "params = gp1.kernel_.get_params()\n",
    "print('Best-fit kernel length scale =', params['length_scale'],'cf. input',10.0)\n",
    "\n",
    "# Now predict y(x) everywhere on our xgrid: \n",
    "rescaled_ygrid1, rescaled_ygrid1_err = gp1.predict(xgrid, return_std=True)\n",
    "\n",
    "# And undo scaling, of both y(x) on our grid, and our two constraining data points:\n",
    "ygrid1, ygrid1_err = rescaled.invert(rescaled_ygrid1, rescaled_ygrid1_err)\n",
    "y1, sigmay1 = rescaled.invert(rescaled_y1, rescaled_sigmay1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ax = fig.add_subplot(222)\n",
    "ax.plot(xgrid, ygrid1, '-', color='gray', label='Posterior mean $y(x)$')\n",
    "ax.fill(np.concatenate([xgrid, xgrid[::-1]]),\n",
    "        np.concatenate([(ygrid1 - ygrid1_err), (ygrid1 + ygrid1_err)[::-1]]),\n",
    "        alpha=0.3, fc='gray', ec='None', label='68% confidence interval')\n",
    "ax.plot(x1, y1, '.k', ms=6, label='Noiseless constraints')\n",
    "ax.set_xlim(0, 399)\n",
    "ax.set_ylim(0, 399)\n",
    "ax.set_xlabel('$x$')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the absence of information, the GP tends to produce $y(x)$ that fluctuate around the prior mean function, which we chose to be a constant. Let's draw some samples from the posterior PDF, and overlay them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "draws = gp1.sample_y(xgrid, n_samples=3)\n",
    "for k in range(3):\n",
    "    draws[:,k], dummy = rescaled.invert(draws[:,k], np.zeros(len(xgrid)))\n",
    "\n",
    "ax.plot(xgrid, draws[:,0], '-r')\n",
    "ax.plot(xgrid, draws[:,1], '-g')\n",
    "ax.plot(xgrid, draws[:,2], '-b', label='Posterior sample $y(x)$')\n",
    "ax.legend(fontsize=8)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "See how the posterior sample $y(x)$ functions all pass through the constrained points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Choose two of our datapoints:\n",
    "x2 = np.array([x[10], x[12]])\n",
    "rescaled_y2 = np.array([rescaled.y[10], rescaled.y[12]])\n",
    "rescaled_sigmay2 = np.array([rescaled.err[10], rescaled.err[12]])\n",
    "\n",
    "# Instantiate a GP model, including observational errors:\n",
    "kernel = SquaredExponential(length_scale=10.0, length_scale_bounds=(0.01, 1000.0))\n",
    "gp2 = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9, \n",
    "                               alpha=(rescaled_sigmay2 / rescaled_y2) ** 2,\n",
    "                               random_state=0)\n",
    "\n",
    "# Fit it to our two noisy datapoints:\n",
    "gp2.fit(x2[:, None], rescaled_y2)\n",
    "\n",
    "# We have fit for the length scale parameter: print the result here:\n",
    "params = gp2.kernel_.get_params()\n",
    "print('Best-fit kernel length scale =', params['length_scale'],'cf. input',10.0)\n",
    "\n",
    "# Now predict y(x) everywhere on our xgrid: \n",
    "rescaled_ygrid2, rescaled_ygrid2_err = gp2.predict(xgrid, return_std=True)\n",
    "\n",
    "# And undo scaling:\n",
    "ygrid2, ygrid2_err = rescaled.invert(rescaled_ygrid2, rescaled_ygrid2_err)\n",
    "y2, sigmay2 = rescaled.invert(rescaled_y2, rescaled_sigmay2)\n",
    "\n",
    "# Draw three posterior sample y(x):\n",
    "draws = gp2.sample_y(xgrid, n_samples=3)\n",
    "for k in range(3):\n",
    "    draws[:,k], dummy = rescaled.invert(draws[:,k], np.zeros(len(xgrid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ax = fig.add_subplot(223)\n",
    "\n",
    "def gp_plot(ax, xx, yy, ee, datax, datay, datae, samples, legend=True):\n",
    "    ax.cla()\n",
    "    ax.plot(xx, yy, '-', color='gray', label='Posterior mean $y(x)$')\n",
    "    ax.fill(np.concatenate([xx, xx[::-1]]),\n",
    "            np.concatenate([(yy - ee), (yy + ee)[::-1]]),\n",
    "            alpha=0.3, fc='gray', ec='None', label='68% confidence interval')\n",
    "    ax.errorbar(datax, datay, datae, fmt='.k', ms=6, label='Noisy constraints')\n",
    "    ax.set_xlim(0, 399)\n",
    "    ax.set_ylim(0, 399)\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y(x)$')\n",
    "    ax.plot(xgrid, samples[:,0], '-r')\n",
    "    ax.plot(xgrid, samples[:,1], '-g')\n",
    "    ax.plot(xgrid, samples[:,2], '-b', label='Posterior sample $y(x)$')\n",
    "    if legend: ax.legend(fontsize=8)\n",
    "    return\n",
    "\n",
    "gp_plot(ax, xgrid, ygrid2, ygrid2_err, x2, y2, sigmay2, draws, legend=True)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, the posterior sample $y(x)$ functions pass through the constraints _within the errors_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using all the Data\n",
    "\n",
    "Now let's extend the above example to use all of our datapoints. This additional information should pull the predictions further away from the initial mean function. We'll also compute the marginal log likelihood of the best fit hyperparameter, in case we want to compare this choice of kernel with another one (in the exercises, for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Use all of our datapoints:\n",
    "x3 = x\n",
    "rescaled_y3 = rescaled.y\n",
    "rescaled_sigmay3 = rescaled.err\n",
    "\n",
    "# Instantiate a GP model, including observational errors:\n",
    "kernel = SquaredExponential(length_scale=10.0, length_scale_bounds=(0.01, 1000.0))\n",
    "# Could comment this out, and then import and use an \n",
    "# alternative kernel here. \n",
    "\n",
    "gp3 = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9, \n",
    "                               alpha=(rescaled_sigmay3 / rescaled_y3) ** 2,\n",
    "                               random_state=0)\n",
    "\n",
    "# Fit it to our noisy datapoints:\n",
    "gp3.fit(x3[:, None], rescaled_y3)\n",
    "\n",
    "# Now predict y(x) everywhere on our xgrid: \n",
    "rescaled_ygrid3, rescaled_ygrid3_err = gp3.predict(xgrid, return_std=True)\n",
    "\n",
    "# And undo scaling:\n",
    "ygrid3, ygrid3_err = rescaled.invert(rescaled_ygrid3, rescaled_ygrid3_err)\n",
    "y3, sigmay3 = rescaled.invert(rescaled_y3, rescaled_sigmay3)\n",
    "\n",
    "# We have fitted the length scale parameter - print the result here:\n",
    "params = gp3.kernel_.get_params()\n",
    "print('Kernel: {}'.format(gp3.kernel_))\n",
    "print('Best-fit kernel length scale =', params['length_scale'],'cf. input',10.0)\n",
    "print('Marginal log-Likelihood: {:.3f}'.format(gp3.log_marginal_likelihood(gp3.kernel_.theta)))\n",
    "\n",
    "# Draw three posterior sample y(x):\n",
    "draws = gp3.sample_y(xgrid, n_samples=3)\n",
    "for k in range(3): \n",
    "    draws[:,k], dummy = rescaled.invert(draws[:,k], np.zeros(len(xgrid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ax = fig.add_subplot(224)\n",
    "\n",
    "gp_plot(ax, xgrid, ygrid3, ygrid3_err, x3, y3, sigmay3, draws, legend=True)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We now see the Gaussian Process model providing a smooth interpolation between the points. The posterior samples show fluctuations, but all are plausible under our assumptions."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:yse]",
   "language": "python",
   "name": "conda-env-yse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
