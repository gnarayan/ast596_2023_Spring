{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1024,\n",
    "        'height': 768,\n",
    "        'scroll': True,\n",
    "})\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 11, ASTR 596: Fundamentals of Data Science\n",
    "\n",
    "\n",
    "## Forests and Networks\n",
    "\n",
    "### Gautham Narayan, Konstantin Malanchev\n",
    "##### <gsn@illinois.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap\n",
    "\n",
    "### Terminology\n",
    "\n",
    "- Supervised learning - function approximation\n",
    "  * Classification vs regression\n",
    "  * Train, validation, test data sets\n",
    "- Unsupervised learning - distribution approximation\n",
    "  * Clustering vs dimensionality reeduction vs **anomaly detection**\n",
    "- Loss function\n",
    "\n",
    "\n",
    "### Decision trees\n",
    "\n",
    "- Unbalanced binary trees\n",
    "- Split criteria: select axis split value minimizing impurity\n",
    "- Impurity measure $G$ is based on criteria $H$: $G = \\frac{n_{left}}{n} H({y}_{left}) + \\frac{n_{right}}{n} H({y}_{right})$\n",
    "  * Classification: Shennon entropy $-p_i \\log_2{p_i}$ and Gini coefficient $p_i (1 - p_i)$\n",
    "  * Regression: inter-partition $\\sum |y_i - \\overline{y}|$ (L1) and $\\sum (y_i - \\overline{y})^2$ (L2)\n",
    "  * **NB:** these split criteria depend only on $y$, not on $x$\n",
    "- Hyperparameters:\n",
    "  * Maximum depth\n",
    "  * Shrinkage (pre-pruning) condition, e.g. minimum impurity change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's build a decision tree for ELAsTiCC SN classification\n",
    "\n",
    "<!--\n",
    "clf = DecisionTreeClassifier(max_depth=6, criterion='entropy', random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "\n",
    "%matplotlib inline\n",
    "import graphviz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics, model_selection, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "\n",
    "# Read in the data\n",
    "df = pd.read_csv('elasticc.csv.bz2')\n",
    "# Here we have some features extracted from light curves in LSST gr passbands\n",
    "# and features of potential host galaxies\n",
    "display(df.head())\n",
    "\n",
    "# Let SN Type Ia to be a positive class, and SN Type II to be a negative class\n",
    "y = df.pop('type') == 'Ia'\n",
    "# We removed type from the data frame, so X is just df\n",
    "X = df\n",
    "\n",
    "# Split into training and test sets, 70/30\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0, shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TWO LINES OF CODE TO IMPLEMENT A DECISION TREE\n",
    "\n",
    "# | put your hyperparameters here\n",
    "# |\n",
    "# V\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Plot the Tree\n",
    "dot_data = sklearn.tree.export_graphviz(clf, feature_names=X.columns, out_file=None) \n",
    "graph = graphviz.Source(dot_data)\n",
    "display(graph)\n",
    "\n",
    "print('Accuracy train:', metrics.accuracy_score(y_train, clf.predict(X_train)))\n",
    "print('Log-loss train:', metrics.log_loss(y_train, clf.predict(X_train)))\n",
    "print('Accuracy test:', metrics.accuracy_score(y_test, clf.predict(X_test)))\n",
    "print('Log-loss test:', metrics.log_loss(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The big issues with trees\n",
    "\n",
    "* Variance - different data lead to different trees\n",
    "    - intuitively if you have just two continuous variables, then calculating the split for every node even with a depth = 2 tree is of order $\\infty^2$\n",
    "    \n",
    "You can view each tree as a single path that you can take do get the desired outcome.\n",
    "\n",
    "There are many possible paths, so we do the thing we always do and marginalize over them.\n",
    "\n",
    "In other words, go from a single decision tree to a many decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ensemble Methods: Bagging\n",
    "\n",
    "Run multiple different models on the same data and learn from the ensemble\n",
    "\n",
    "We can improve the performance of decisions trees (especially when there are many features) by **bagging** (Bootstrap AGGregation). This averages the predictive results of a series of bootstrap samples.\n",
    "\n",
    "For a sample of $N$ points in a training set, bagging generates $K$ equally sized bootstrap samples from which to estimate the function $f_i(x)$. The final estimator, defined by bagging, is then\n",
    "\n",
    "\\begin{equation}\n",
    "\\huge\n",
    "f(x)=\\frac{1}{K} \\sum_{i}^{K} f_{i}(x)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "(Yep, this is just take the average of all the individual methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Can Random Forest beat a single tree?\n",
    "\n",
    "<!--\n",
    "clf = RandomForestClassifier(n_estimators=1000, criterion='entropy', max_depth=14, random_state=0, n_jobs=4)\n",
    "clf.fit(X_train, y_train)\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# TWO LINES OF CODE TO IMPLEMENT A RANDOM FOREST\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# | put your hyperparameters here\n",
    "# |\n",
    "# V\n",
    "clf = RandomForestClassifier(random_state=0, n_jobs=4)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy train:', metrics.accuracy_score(y_train, clf.predict(X_train)))\n",
    "print('Log-loss train:', metrics.log_loss(y_train, clf.predict(X_train)))\n",
    "print('Accuracy test:', metrics.accuracy_score(y_test, clf.predict(X_test)))\n",
    "print('Log-loss test:', metrics.log_loss(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods: Boosting\n",
    "\n",
    "Boosting is an ensemble approach motivated by the idea that combining many weak classifiers can result in an improved classification. \n",
    "\n",
    "Boosting creates models to attempt to correct the errors of the ensemble so far \n",
    "**i.e. we reweight the data based on how incorrectly the data were classified in the previous iteration.**\n",
    "\n",
    "- if you have $N$ data points in your sample with features $x$, and target $y$\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "x_{i} \\in \\mathbb{R}^{d}, y_{i} \\in\\{-1,1\\}\n",
    "\\end{equation}\n",
    "**initalize the weights to $1/N$**\n",
    "\n",
    "- Run the classification $h$ (i.e. hypotheses) with a weak clasifier $t$ times and compute the weighted classification error for each classifier\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "\\epsilon = \\frac{\\sum_{i=1}^{N} w_{i} I\\left(y_{i} \\neq h_{j}\\left(x_{i}\\right)\\right)}{\\sum_{i=1}^{N} w_{i}}\n",
    "\\end{equation}\n",
    "\n",
    "where $I$ is the indicator variable (1 if $y_i$ matches the hypotheses, 0 if not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The weight for each weak classifier is related to the weighted classification error\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "\\theta_{t}=\\frac{1}{2} \\ln \\left(\\frac{1-\\epsilon_{t}}{\\epsilon_{t}}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "- each time reweight each sample $i$ based on the previous performance of the classifier $t$\n",
    "\n",
    "For any classifier with accuracy higher than 50%, the weight is positive. \n",
    "The more accurate the classifier, the larger the weight.\n",
    "For a classifer with less than 50% accuracy, the weight is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\large\n",
    "w_{t+1}=\\frac{w_{t} \\exp \\left[-\\theta_{t} y_{i} h_{t} \\right]}{Z_{t}}\n",
    "\\end{equation}\n",
    "        \n",
    "Where the denominator is the normalization constant to make the sum of the weights 1. \n",
    "\n",
    "If a misclassified case is from a positive weighted classifier, the “exp” term in the numerator would be always larger than 1. \n",
    "\n",
    "**NOTE THAT A WEAK CLASSIFIER WITH A NEGATIVE WEIGHT STILL CONTRIBUTES**\n",
    "(you can be wrong, but if you are wrong consistently, then you are still useful)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view this as just iteratively minimizing the exponential loss function\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "L(y, F(x))=E\\left(e^{-y F(x)}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $E$ is just weighted expectation value.\n",
    "\n",
    "[I'll spare you the math](https://web.archive.org/web/20190418150438/https://towardsdatascience.com/boosting-algorithm-adaboost-b6737a9ee60c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At the end of this procedure we allow the classifiers a weighted vote on the final classification\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "H(x)=\\operatorname{sign}\\left(\\sum_{t=1}^{T} \\theta_{t} h_{t}(x)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "    \n",
    "The most popular form of boosting is that of adaptive boosting (helpfully implemented in a package called **AdaBoost**)\n",
    "\n",
    "A fundamental limitation of the boosted decision tree is the computation time for large data sets (they rely on a chain of classifiers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting can be thought of as a function decomposition into trees, similar to the Taylor series. Through an iterative process, each new tree's output is subtracted from the true value of $y$, with the goal of minimizing residuals with each subsequent tree.\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "y_{true} = \\theta_1 h_1(x) + \\theta_2 h_2(x) + \\dots\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The textbook has a couple of examples of using Random Forests and BDTs for regression using SDSS galaxies with known redshifts (the target) and magnitudes as features - i.e. a photo-z estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from astroML.datasets import fetch_sdss_specgals\n",
    "from astroML.utils import pickle_results\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Fetch and prepare the data\n",
    "data = fetch_sdss_specgals()\n",
    "\n",
    "# put magnitudes in a matrix\n",
    "mag = np.vstack([data['modelMag_%s' % f] for f in 'ugriz']).T\n",
    "z = data['z']\n",
    "\n",
    "# train on ~60,000 points\n",
    "mag_train = mag[::10]\n",
    "z_train = z[::10]\n",
    "\n",
    "# test on ~6,000 distinct points\n",
    "mag_test = mag[1::100]\n",
    "z_test = z[1::100]\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the results\n",
    "#  This is a long computation, so we'll save the results to a pickle.\n",
    "@pickle_results('photoz_forest.pkl')\n",
    "def compute_photoz_forest(depth):\n",
    "    rms_test = np.zeros(len(depth))\n",
    "    rms_train = np.zeros(len(depth))\n",
    "    i_best = 0\n",
    "    z_fit_best = None\n",
    "\n",
    "    for i, d in enumerate(depth):\n",
    "        clf = RandomForestRegressor(n_estimators=10,max_features=3,\n",
    "                                    max_depth=d, random_state=0)\n",
    "        clf.fit(mag_train, z_train)\n",
    "\n",
    "        z_fit_train = clf.predict(mag_train)\n",
    "        z_fit = clf.predict(mag_test)\n",
    "        rms_train[i] = np.mean(np.sqrt((z_fit_train - z_train) ** 2))\n",
    "        rms_test[i] = np.mean(np.sqrt((z_fit - z_test) ** 2))\n",
    "\n",
    "        if rms_test[i] <= rms_test[i_best]:\n",
    "            i_best = i\n",
    "            z_fit_best = z_fit\n",
    "\n",
    "    return rms_test, rms_train, i_best, z_fit_best\n",
    "\n",
    "\n",
    "depth = np.arange(1, 20)\n",
    "rms_test, rms_train, i_best, z_fit_best = compute_photoz_forest(depth)\n",
    "best_depth = depth[i_best]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "fig.subplots_adjust(wspace=0.25,\n",
    "                    left=0.1, right=0.95,\n",
    "                    bottom=0.15, top=0.9)\n",
    "\n",
    "# left panel: plot cross-validation results\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(depth, rms_test, '-k', label='cross-validation')\n",
    "ax.plot(depth, rms_train, '--k', label='training set')\n",
    "ax.legend(loc=1, prop=dict(size=13))\n",
    "\n",
    "ax.set_xlabel('depth of tree')\n",
    "ax.set_ylabel('rms error')\n",
    "\n",
    "ax.set_xlim(0, 21)\n",
    "ax.set_ylim(0.009,  0.04)\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.01))\n",
    "\n",
    "# right panel: plot best fit\n",
    "ax = fig.add_subplot(122)\n",
    "ax.scatter(z_test, z_fit_best, s=1, lw=0, c='k')\n",
    "ax.plot([-0.1, 0.4], [-0.1, 0.4], ':k')\n",
    "ax.text(0.03, 0.97, \"depth = %i\\nrms = %.3f\" % (best_depth, rms_test[i_best]),\n",
    "        ha='left', va='top', transform=ax.transAxes)\n",
    "\n",
    "ax.set_xlabel(r'$\\rm z_{true}$', fontsize=16)\n",
    "ax.set_ylabel(r'$\\rm z_{fit}$', fontsize=16)\n",
    "\n",
    "ax.set_xlim(-0.02, 0.4001)\n",
    "ax.set_ylim(-0.02, 0.4001)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from astroML.datasets import fetch_sdss_specgals\n",
    "from astroML.decorators import pickle_results\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Fetch and prepare the data\n",
    "data = fetch_sdss_specgals()\n",
    "\n",
    "# put magnitudes in a matrix\n",
    "mag = np.vstack([data['modelMag_%s' % f] for f in 'ugriz']).T\n",
    "z = data['z']\n",
    "\n",
    "# train on ~60,000 points\n",
    "mag_train = mag[::10]\n",
    "z_train = z[::10]\n",
    "\n",
    "# test on ~6,000 distinct points\n",
    "mag_test = mag[1::100]\n",
    "z_test = z[1::100]\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the results\n",
    "#  This is a long computation, so we'll save the results to a pickle.\n",
    "@pickle_results('photoz_boosting.pkl')\n",
    "def compute_photoz_forest(N_boosts):\n",
    "    rms_test = np.zeros(len(N_boosts))\n",
    "    rms_train = np.zeros(len(N_boosts))\n",
    "    i_best = 0\n",
    "    z_fit_best = None\n",
    "\n",
    "    for i, Nb in enumerate(N_boosts):\n",
    "        clf = GradientBoostingRegressor(n_estimators=Nb, learning_rate=0.1,\n",
    "                                        max_depth=3, random_state=0)\n",
    "        clf.fit(mag_train, z_train)\n",
    "\n",
    "        z_fit_train = clf.predict(mag_train)\n",
    "        z_fit = clf.predict(mag_test)\n",
    "        rms_train[i] = np.mean(np.sqrt((z_fit_train - z_train) ** 2))\n",
    "        rms_test[i] = np.mean(np.sqrt((z_fit - z_test) ** 2))\n",
    "\n",
    "        if rms_test[i] <= rms_test[i_best]:\n",
    "            i_best = i\n",
    "            z_fit_best = z_fit\n",
    "\n",
    "    return rms_test, rms_train, i_best, z_fit_best\n",
    "\n",
    "N_boosts = (10, 100, 200, 300, 400, 500)\n",
    "rms_test, rms_train, i_best, z_fit_best = compute_photoz_forest(N_boosts)\n",
    "best_N = N_boosts[i_best]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "fig.subplots_adjust(wspace=0.25,\n",
    "                    left=0.1, right=0.95,\n",
    "                    bottom=0.15, top=0.9)\n",
    "\n",
    "# left panel: plot cross-validation results\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(N_boosts, rms_test, '-k', label='cross-validation')\n",
    "ax.plot(N_boosts, rms_train, '--k', label='training set')\n",
    "ax.legend(loc=1, prop=dict(size=13))\n",
    "\n",
    "ax.set_xlabel('number of boosts')\n",
    "ax.set_ylabel('rms error')\n",
    "ax.set_xlim(0, 510)\n",
    "ax.set_ylim(0.009,  0.032)\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.01))\n",
    "\n",
    "ax.text(0.03, 0.03, \"Tree depth: 3\",\n",
    "        ha='left', va='bottom', transform=ax.transAxes)\n",
    "\n",
    "# right panel: plot best fit\n",
    "ax = fig.add_subplot(122)\n",
    "ax.scatter(z_test, z_fit_best, s=1, lw=0, c='k')\n",
    "ax.plot([-0.1, 0.4], [-0.1, 0.4], ':k')\n",
    "ax.text(0.03, 0.97, \"N = %i\\nrms = %.3f\" % (best_N, rms_test[i_best]),\n",
    "        ha='left', va='top', transform=ax.transAxes)\n",
    "\n",
    "ax.set_xlabel(r'$\\rm z_{true}$', fontsize=16)\n",
    "ax.set_ylabel(r'$\\rm z_{fit}$', fontsize=16)\n",
    "\n",
    "ax.set_xlim(-0.02, 0.4001)\n",
    "ax.set_ylim(-0.02, 0.4001)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised learning with trees: Isolation forest\n",
    "\n",
    "Decision trees are an effective tool for exploring the feature space, but can they be used to learn about the distribution of features themselves?\n",
    "\n",
    "While decision trees rely on split criteria for growth, all of the criteria we have encountered thus far require labeled data. **What can we do if no labels are available?**\n",
    "\n",
    "### Split randomly!\n",
    "\n",
    "![Das+2017](itree_das2017.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ensemble of isolation trees\n",
    "\n",
    "The iTree algorithm, developed by Liu et al. in 2008, randomly selects a feature and split point within the range of the current data subset during each iteration.\n",
    "\n",
    "The algorithm stops splitting when it isolates a single sample or reaches the maximum depth, which is usually set to $\\log_2 N$.\n",
    "\n",
    "The Isolation Forest is an ensemble of iTrees, with each individual tree typically built on a small subset of the data, containing around 64-1024 data samples.\n",
    "\n",
    "![pruzhinakaya+2019](iforest_pruzhinskaya.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we score anomalies?\n",
    "\n",
    "Let's say that initially the feature volume is $V = \\prod_j (\\max(x_j) - \\min(x_j))$ - a volume of a hyper-parallelepiped containing the data set.\n",
    "\n",
    "In average, each split cuts the feature space by half, so if iTree isolates a sample $x$ in a leaf at depth $d(x)$, this leaf would roughly represent volume of $2^{-d(x)} \\times V$.\n",
    "\n",
    "This makes the iTree to be a **kernel density estimator**, it splits the space into hyper-parallelepipeds and gives probability density for each of the leaves. This probability is proportional to the density and inverse to the volume:\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "p_{iTree}(x) \\sim 2^{d(x)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, an anomaly is a sample located in a very **sparse part of the parameter space**. In other words, it is an **out-of-distribution** sample, an object having a small probability to be sampled from the distribution.\n",
    "\n",
    "Following the ideas of the original Isolation Forest papers, we can define the probability density of the forest as the probability density of the joint iTree distribution, assuming that all iTrees are independent (since each of them is trained on a small portion of data):\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "p_\\mathrm{iso.~forest}(x) = \\prod_{t=1}^{T} p_{t\\mathrm{-th~iTree}}(x) \\sim 2^{\\sum_{t=1}^{T} d_t(x)} \\sim 2^{\\overline{d(x)}}\n",
    "\\end{equation}\n",
    "\n",
    "For convenience, the anomaly score is defined as the inverse of the probability density, and it is normalized in a way making it between zero and one (usually normalies are around 0.3-0.4, and anomalies are around 0.6-0.8). Therefore, the closer the score is to one, the more anomalous the object is considered to be.\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "\\mathrm{Score}(x) \\sim 2^{-\\overline{d(x)}}, \\mathrm{Score} \\in (0, 1)\n",
    "\\end{equation}\n",
    "\n",
    "![Liu+2012](iforest_liu2012.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "def anomaly_pipeline(X, true_anomaly=None, top_n=100):\n",
    "    X = QuantileTransformer(output_distribution='uniform', random_state=0).fit_transform(X)\n",
    "\n",
    "    X_2d = PCA(n_components=2, random_state=0).fit_transform(X)\n",
    "\n",
    "    isoforest = IsolationForest(n_estimators=1000, max_samples=64, random_state=0)\n",
    "    isoforest.fit(X)\n",
    "    anomaly_scores = isoforest.score_samples(X)\n",
    "    idx_by_score = np.argsort(anomaly_scores)\n",
    "\n",
    "    # Plot\n",
    "    plt.scatter(*X_2d.T, s=0.1, alpha=0.1, color='k', label='data')\n",
    "    plt.scatter(*X_2d[idx_by_score[:top_n]].T, s=3, color='r', label='pred anomalies')\n",
    "    if true_anomaly is not None:\n",
    "        plt.scatter(*X_2d[true_anomaly].T, s=1, color='b', label='true anomalies')\n",
    "    plt.xlabel('PCA 1')\n",
    "    plt.ylabel('PCA 2')\n",
    "    plt.legend()\n",
    "    \n",
    "    return idx_by_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Detect anomalies in Cardiotocography dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "cardiotocography = fetch_openml('cardiotocography', version=2, parser='auto')\n",
    "y = cardiotocography['target'].astype('int')\n",
    "# 1 - normal, 3 - pathalogical\n",
    "idx = y.isin([1, 3])\n",
    "y = y[idx]\n",
    "X = cardiotocography['data'][idx]\n",
    "\n",
    "anomaly_pipeline(X, true_anomaly=y == 3, top_n=np.sum(y == 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect anomalous light curves in M31 ZTF field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# RUN THIS\n",
    "\n",
    "# Get data\n",
    "with np.load('m31.npz') as archive:\n",
    "    oid = archive['oid']\n",
    "    X = archive['feature']\n",
    "    \n",
    "idx_by_score = anomaly_pipeline(X, top_n=100)\n",
    "\n",
    "# Get links to top 5 anomalies\n",
    "for object_id in oid[idx_by_score[:5]]:\n",
    "    print(f'https://ztf.snad.space/dr3/view/{object_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised vs Unsupervised trees\n",
    "\n",
    "Can we use random splits for supervised learning?..\n",
    "\n",
    "Yes, we can! But, but using completely random splits comes with certain drawbacks. Random splits do not take into account any underlying patterns or structures in the data, and thus may lead to suboptimal results, even with more complex models such as deep trees and forests.\n",
    "\n",
    "Furthermore, it is important to note that in supervised learning, we do not directly use feature values, but rather their ordering. This means that **supervised trees are not sensitive to monotonic transformations of individual features**, which is a significant advantage over other popular machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The first step to neural networks: Linear Regression\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "f(x) = \\sum_j w_j x_j + c\n",
    "\\end{equation}\n",
    "\n",
    "### MSE loss makes it to be the ordinal linear least sqaures problem\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "\\min_{\\theta=\\{w_j,c\\}} \\sum_i \\left[y_i - \\left(\\sum_j w_j x_{ij} + c\\right)\\right]^2\n",
    "\\end{equation}\n",
    "\n",
    "Do $\\partial / \\partial w_i = 0, \\partial / \\partial c = 0$ and you have the (analytical) solution!\n",
    "\n",
    "### Regression\n",
    "\n",
    "![](tf_linear_regression_good.png)\n",
    "\n",
    "[TF playground](http://playground.tensorflow.org/#activation=sigmoid&batchSize=5&dataset=gauss&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=&seed=0.65532&showTestData=false&discretize=true&percTrainData=40&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=regression&initZero=false&hideText=false)\n",
    "\n",
    "### Classification\n",
    "\n",
    "Actualy it is also regression, but the real output value is converted to a binary label.\n",
    "However, if we'd like to have outputs to be between 0 and 1 we need some normalisation, for example logistic function $\\sigma(z) = 1 / (1 + \\exp{-z})$.\n",
    "In this case the model is called logistic regression.\n",
    "\n",
    "![](tf_linear_regression_clf_good.png)\n",
    "\n",
    "[TF playground](http://playground.tensorflow.org/#activation=sigmoid&batchSize=5&dataset=gauss&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=&seed=0.73566&showTestData=false&discretize=true&percTrainData=40&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
    "\n",
    "### But what if the problem is not linear...\n",
    "\n",
    "![](tf_linear_regression_clf_bad.png)\n",
    "\n",
    "[TF playground](http://playground.tensorflow.org/#activation=sigmoid&batchSize=5&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=&seed=0.12820&showTestData=false&discretize=true&percTrainData=40&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
    "\n",
    "### Sometimes we can solve it with a bit of feature engineering\n",
    "\n",
    "![](tf_linear_regression_engineering.png)\n",
    "\n",
    "[TF playground](http://playground.tensorflow.org/#activation=sigmoid&batchSize=5&dataset=circle&regDataset=reg-gauss&learningRate=0.03&regularizationRate=0&noise=0&networkShape=2&seed=0.17882&showTestData=false&discretize=true&percTrainData=40&x=false&y=false&xTimesY=false&xSquared=true&ySquared=true&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&numHiddenLayers_hide=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the model non-linear\n",
    "\n",
    "\\begin{eqnarray}\n",
    "&\\large h_1(x) = g\\left(\\sum_j w^{\\{1\\}}_j x + c^{\\{1\\}}\\right), \\\\\n",
    "&\\large h_2(x) = g\\left(\\sum_j w^{\\{2\\}}_j x + c^{\\{2\\}}\\right), \\\\\n",
    "&\\large h_3(x) = g\\left(\\sum_j w^{\\{3\\}}_j x + c^{\\{3\\}}\\right), \\\\\n",
    "&\\large f(x) = w_{h1} h_1(x) + w_{h2} h_2(x) + w_{h3} h_3(x) + c_{h},\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $g$ is some (non-linear) scalar function, same for all transformations. Note that \"weights\" $w$ and \"biases\" (or, if we'd write minus instead of plus, \"thresholds\") $c$ are different for each transformation.\n",
    "\n",
    "![](tf_slp3.png)\n",
    "\n",
    "[TF playground](http://playground.tensorflow.org/#activation=sigmoid&batchSize=5&dataset=circle&regDataset=reg-gauss&learningRate=0.03&regularizationRate=0&noise=0&networkShape=3&seed=0.17882&showTestData=false&discretize=true&percTrainData=40&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&numHiddenLayers_hide=false)\n",
    "\n",
    "### Can we make it better? Easy!\n",
    "\n",
    "It has $2 \\times 8 + 8 \\times 1 = 24$ weights.\n",
    "\n",
    "![](tf_slp8.png)\n",
    "\n",
    "[TF playground](http://playground.tensorflow.org/#activation=sigmoid&batchSize=5&dataset=circle&regDataset=reg-gauss&learningRate=0.03&regularizationRate=0&noise=0&networkShape=8&seed=0.17882&showTestData=false&discretize=true&percTrainData=40&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&numHiddenLayers_hide=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mark 1, 1960, Cornell Aeronautical Laboratory\n",
    "\n",
    "![](mark1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why does it work?\n",
    "\n",
    "### Universal approximation theorem\n",
    "\n",
    "In 1989, George Cybenko proved that for neural networks with a single hidden layer and logistic activation function, the network can approximate any continuous function to arbitrary precision. In 1991 Kurt Hornik shown it for other activation function.\n",
    "\n",
    "This is the case of \"arbitrary width\" theorem, we have a single hidden layer, the larger the layer the better the precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer perceptron = dense neural network\n",
    "\n",
    "In practice, single layer is not sufficient, we need huge neuron to solve real world problems.\n",
    "\n",
    "We can easialy add layers feeding the next perceptron to the output of the previous one.\n",
    "\n",
    "![](tf_mlp_relu.png)\n",
    "[TF Playground](http://playground.tensorflow.org/#activation=relu&regularization=L2&batchSize=5&dataset=circle&regDataset=reg-gauss&learningRate=0.001&regularizationRate=0.001&noise=0&networkShape=3,3,3,3,3,3&seed=0.95354&showTestData=false&discretize=true&percTrainData=40&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&numHiddenLayers_hide=false)\n",
    "\n",
    "Multi-layer neural network is covered by \"arbitrary depth\" universal approximation theorem, which is proven for some specific cases.\n",
    "\n",
    "### Fit spiral dataset yourself: [TF playground](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=&seed=0.04277&showTestData=false&discretize=false&percTrainData=70&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
    "\n",
    "<!--\n",
    "![](tf_mlp_relu_spiral.png)\n",
    "[TF playground](http://playground.tensorflow.org/#activation=relu&regularization=L2&batchSize=5&dataset=spiral&regDataset=reg-gauss&learningRate=0.001&regularizationRate=0.001&noise=0&networkShape=4,4,4,4,4,4&seed=0.75252&showTestData=false&discretize=true&percTrainData=70&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&numHiddenLayers_hide=false)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://askabiologist.asu.edu/neuron-anatomy\n",
    "![](neurons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we train neural networks? Backpropagation\n",
    "\n",
    "We have what nature has not: calculus!\n",
    "[(No.)](https://www.nature.com/articles/s41583-020-0277-3)\n",
    "This allows us to find gradients of the loss function with respect to weights and update weights accordingly for each new sample:\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "w_k \\leftarrow w_k + r \\left(\\frac{\\partial l(x_i, y_i)}{\\partial w_k}\\right)^{-1},\n",
    "\\end{equation}\n",
    "where $w_k$ is some weight, $l(x_i, y_i)$ is the loss function for the i-th sample, and $r$ is the learning rate, higher learning rate provide larger, but more chaotic changes.\n",
    "\n",
    "In practice, the simple gradient descent approach is rarely used because it can result in large and chaotic weight value updates. Instead, stochastic gradient optimizers are commonly employed, such as [Adam](https://arxiv.org/abs/1412.6980).\n",
    "\n",
    "Furthermore, one-by-one sample feeding is the extreme case, in contrast to \"normal\" least squares optimization, where all samples are used simultaneously. In general, samples are grouped into batches to be fed into a neural network. The batch size is chosen as a trade-off between training speed, memory usage, and final model quality.\n",
    "\n",
    "![](backprop.png)\n",
    "\n",
    "https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why neural networks?\n",
    "\n",
    "- Great flexibility: we can use NNs with images, sound, text, time series, probability distributions, etc.\n",
    "- Combination of linear algebra and scalar functions allow fast training and evaluation on GPUs and special processor units such as tensor PUs, neural PUs or even field-programmable gate arrays.\n",
    "- Re-usability: you can take someone else's architecture and train the network for your data, or you can even get someone else's weights and fine-tune it.\n",
    "\n",
    "![](nn.webp)\n",
    "https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
