{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASTR 596: FDS Homework 6+7: Gaussian Processes (200 pts)\n",
    "\n",
    "### This is a double HW set so you get extra time - until reading day (May 4th, 2023) at noon to do it. \n",
    "### After that, it's finals time. \n",
    "\n",
    "\n",
    "# P1. Gaussian Processes\n",
    "\n",
    "### Last HW, you worked on finding periodic planet signals in the light curve of Kepler-90, a star that is photometrically stable. The periodogram worked nicely because \n",
    "\n",
    "### a) we cleaned the light curve to squelch red noise\n",
    "### b) the signals really were periodic and we could implictly make a strong assumption about the covariance between points.\n",
    "\n",
    "### Life gets harder when the star itself has quasi-periodic variations because it has a magnetic field and is rotating (ruh oh...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%pylab\n",
    "\n",
    "from astropy.table import Table\n",
    "import scipy.stats as st\n",
    "import sklearn\n",
    "import sklearn.ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = Table.read('KIC2157356.txt',format='ascii')\n",
    "tab['quarter'] = tab['quarter'].astype('int')\n",
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = sorted(np.unique(tab['quarter']))\n",
    "plt.figure()\n",
    "means = []\n",
    "cycle_map = {}\n",
    "for i, q in enumerate(qs):\n",
    "    ind = tab['quarter']==q\n",
    "    t = tab[ind]\n",
    "    plt.errorbar(t['time'],t['flux'], yerr=t['error'], marker='.', linestyle='None', alpha=0.01)\n",
    "    meanflux = np.mean(t['flux'])\n",
    "    cycle_map[q] = ind\n",
    "    means.append(meanflux)\n",
    "    if i == 0:\n",
    "        plt.axhline(meanflux, label='m', color='grey', ls=\":\")\n",
    "    else:\n",
    "\n",
    "        vmin = means[0]\n",
    "        vmax = meanflux\n",
    "\n",
    "        plt.plot((t['time'][0], t['time'][0]), (vmin, vmax), label=rf'$c_{i}$', color=f'C{i}', ls='--') \n",
    "    \n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Flux')\n",
    "plt.legend(frameon=False);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you can see there is some kind of periodic singal, but it's not perfectly regular. There is also the usual offsets between Kepler photometry in different cycles.\n",
    "\n",
    "### You'll need four parameters to describe constants ($m, c_1, c_2, c_3$) to renormalize the flux to the first cycle, illustrated in the figure above. \n",
    "### $m$ specifies the mean of the Gaussian process, while $c_1, c_2, c_3$ are nuisance parameters. \n",
    "\n",
    "### You know how to implement a model with one common zeropoint and multiple offsets - this was what you did on your midterm.\n",
    "\n",
    "\n",
    "### You'll also need some model to describe the quasi-periodic oscillations. There's no good way to write down a model in real for these in real space because stellar magnetic fields are incredibly complicated. \n",
    "\n",
    "### Instead we'll write down a model for the covariance between the observations and use a Gaussian process to model the star. You can model quasi-periodic correlation structure as something periodic + something that varies the periodicity smoothly:\n",
    "\n",
    "## $$k(t_i, t_j) = A\\cdot \\exp\\left(-\\Gamma_1\\cdot \\sin^2\\left(\\frac{\\pi}{P}|t_i - t_j|\\right) -  \\frac{|t_i-t_j|^2}{\\lambda}) \\right) $$\n",
    "\n",
    "### This is another 4 parameters, ($A, \\Gamma_1, P, \\lambda$) parameters for a total of 8: ($m, c_1, c_2, c_3, A, \\Gamma_1, P, \\lambda$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr>\n",
    "\n",
    "### Q1: To implement the GP correlations, use the `george` package to construct this quasi-periodic kernel\n",
    "https://george.readthedocs.io/en/latest/user/kernels/\n",
    "\n",
    "\n",
    "### In particular, you should be able to combine `ExpSine2Kernel` and `ExpSquaredKernel` to get a model for the quasi-periodic oscillations. (20 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: To implement the full model, read how to use `george`'s modeling protocol: (20 pts)\n",
    "https://george.readthedocs.io/en/latest/tutorials/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: With your model and likelihood constructured, write down priors on the parameters (you should be able to estimate from the plots) (20 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: Use `emcee` to optimize the model parameters and hyper-parameters, **using only every 10th sample in time**\n",
    "### (Don't go overboard with the number of walkers or steps) (20 pts)\n",
    "https://george.readthedocs.io/en/latest/tutorials/hyper/ may help "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: Plot your posterior model over the data after correcting for the offsets, showing the points you used to condition the GP in red, and the remaining data in black.  (20 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P2. Random Forests\n",
    "\n",
    "For this work, we'll use the datasets produced by [Dey et al. (2022)](https://ui.adsabs.harvard.edu/abs/2022MNRAS.515.5285D/abstract), who trained a deep capsule network on postage stamps of SDSS galaxies to predict photometric redshifts. \n",
    "\n",
    "We're not going to use a deep capsule network on postage stamps, but we can use tabular data. This won't be as performant, but it's still instructive to see how well we can do with a simple random forest. Dey et al. have done an excellent job making their data available - http://d-scholarship.pitt.edu/42023/ (all of it)\n",
    "\n",
    "You will need the [training set](http://d-scholarship.pitt.edu/42023/9/cat_train.csv) and the [test set](http://d-scholarship.pitt.edu/42023/8/cat_test.csv).\n",
    "\n",
    "I suggest reading through Sec. 2 of the paper to get some sense of what the data is. Importantly, the data includes columns for photometric redshift already. You can't use these to train your random forest (duh.). I've limited the number of columns you can use to a set defined below. If you use more than these (e.g. the GalaxyZoo parameters) you might get better performance at the cost of a smaller training sample because you've also got to filter missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = ['dered_petro_u', 'dered_petro_g', 'dered_petro_r', 'dered_petro_i', 'dered_petro_z',\\\n",
    "       'petroMagErr_u', 'petroMagErr_g', 'petroMagErr_r', 'petroMagErr_i', 'petroMagErr_z',\\\n",
    "       'v_disp', 'sersicN_r', 'petroR90_r']\n",
    "pred_cols  = ['bestObjID', 'z', 'zErr', 'zphot', 'dzphot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. Pre-process the data\n",
    "\n",
    "All ML work involves some amount of cleaning and pre-processing the data.\n",
    "Filter data that have `zphot_reliable` == `True`, and have redshifts and photo-zs >= 0. \n",
    "Next filter any entries in the `train_cols` that have any value that is > 5 $\\times$ the nomrally-scaled Median Absolute Deviation (as described in Sec 2.3) (`scipy.stats.median_abs_deviation` is your friend). \n",
    "Your pre-processed training data should have 357397 entries.\n",
    "Make a hexbin plot of `zphot` vs `z` for the training data (to avoid plotting that many points) but replicate Fig. 3 in Dey et al. \n",
    "(35 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. Train the forest(s)\n",
    "\n",
    "Using `n_estimators` (i.e. number of trees) in (5, 20, 50, 200, 500), train a random forest. You can use all the cores your CPU has with `n_jobs=-1`. Limit the maximum number of features at each branch with `sqrt`. Use the inverse variance of the redshifts as your sample weights. Plot the `oob_score` vs the number of trees. For each of the forests you trained, make a plot of the feature importances. (35 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. Test.\n",
    "\n",
    "Pick your best performing forest from Q7. Load the test data (remember to apply any cuts you did to the training data). Use your random forest to predict the photo-z. Replicate Fig. 3 and Fig. 4 with your photo-z prediction *and* the photo-z prediction from SDSS included in the file. (30 pts, 10 pts for prediction, 10 for the two figures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:yse]",
   "language": "python",
   "name": "conda-env-yse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
